\documentclass[11pt,openany,a4paper]{scrartcl}

\usepackage{indentfirst}
\usepackage{amsmath,amsthm,amssymb,amsfonts,amsopn}
\usepackage{mathtext}
\usepackage{enumitem}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[intlimits]{mathtools}
\usepackage[makeroom]{cancel}
\usepackage{titletoc}
\renewcommand{\bfdefault}{sbc}
\usepackage{ccfonts,eulervm,microtype}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{calc}

\tikzset{
    pil/.style={
           ->,
           thick,
           shorten <=2pt,
           shorten >=2pt},
    axis/.style={very thick, ->, >=stealth'},
}

\usepackage[portrait,a4paper,margin=2.5cm,headsep=5mm]{geometry}

\author{С. М. Ананьевский \thanks{Конспект подготовлен студентом Яскевичем С. В.}}
\title{Теория вероятностей и математическая статистика}

\theoremstyle{plain}
\newtheorem{theorem}{Теорема}[section]
\newtheorem{corollary}[theorem]{Следствие}
\newtheorem{proposition}[theorem]{Предложение}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{exercise}[theorem]{Упражнение}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Определение}
\newtheorem{remark}[theorem]{Замечание}
\newtheorem{example}[theorem]{Пример}
\newtheorem{examples}[theorem]{Примеры}
\newtheorem{num}[theorem]{}

\newcommand\mb{\mathbb}
\newcommand\real{\mb R}
\newcommand{\complex}{\mb C}
\newcommand\eqdef{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}
\newcommand\lparagraph[1]{\paragraph{#1}\mbox{}\\}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\uto}{\rightrightarrows}
\newcommand{\underto}[1]{\xrightarrow[#1]{}}
\newcommand{\dif}{\, \mathrm d}
\newcommand{\distr}{\mathfrak P_\xi}
\newcommand{\funcdistr}{F_\xi}
\DeclareMathOperator{\Ree}{Re}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Arg}{Arg}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\const}{const}
\DeclareMathOperator{\Ln}{Ln}
\DeclareMathOperator{\cov}{cov}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\section{Условные вероятности}

Пусть у нас есть вероятностное пространство $(\Omega, \mathfrak{F}, P)$ и два случайных 
события $A,B \in \mathfrak{F}$, причём будем считать, что $P(B) \neq 0$.

\begin{definition}
    \emph{Условной вероятностью события $A$ при условии события $B$} называется число
    $P(A/B) = \frac{P(A \cap B)}{P(B)}$ (иногда обозначается $P_B(A)$).
\end{definition}
\begin{example}
    Если у нас есть игральный кубик, то вероятность выпадения нечётной грани при 
    условии, что количество очков не превосходит $3$, равна
\end{example}

\begin{theorem}[Свойства условной вероятности]
    Условная вероятность является вероятностью.
\end{theorem}
\begin{proof}
    Проверим аксиомы вероятности:
    \begin{enumerate}
        \item $P_B(A) =  \frac{P(A \cap B)}{P(B)}$. Так как числитель и знаменатель 
        неотрицательны, то и дробь не отрицательна.
        \item $P_B(\Omega) = \frac{P(\Omega \cap B)}{P(B)} =  \frac{P(B)}{P(B)} = 1$
        \item Пусть $A_1, A_2 \ldots, \in \mathfrak{F}$;
        $A_i \cap A_j = \varnothing$ (при $i \neq j$). Тогда:
        $$
        P_B(\bigcup\limits_i A_i) =
        \frac{P(\bigcup\limits_i B\cap A_i)}{P(B)} =
        \frac{\sum\limits_i P(B \cap A_i)}{P(B)} =
        $$
        $$
        = \sum\limits_i \frac{P(B\cap A_i)}{P(B)} = \sum\limits_i P_B(A_i)
        $$
        
    \end{enumerate}
\end{proof}
\begin{corollary}
    Все свойства вероятности для условной вероятности выполнены.
\end{corollary}

\section{Формула полной вероятности. Формула Байеса}
\begin{definition}
    Пусть $A_1, A_2 \ldots, \in \mathfrak{F}$; $A_i \cap A_j = \varnothing$
    (при $i \neq j$), $\bigcup\limits_i A_i = \Omega$. Тогда $A_1, A_2 \ldots$ — полная
    система событий.
\end{definition}
\begin{theorem}[Формула полной вероятности]
    Пусть $A_1, A_2 \ldots$ — полная система событий и $\forall i$ $P(A_i) > 0$. Тогда
    вероятность любого случайного события $B \in \mathfrak{F}$ можно вычислить
    по формуле:
    $$
    P(B) = \sum_i P(B/A_i) \cdot P(A_i)
    $$
\end{theorem}
\begin{proof}
    $$
    P(B) = P(B \cap \Omega) = P(B \cap (\bigcup_i A_i)) = P(\bigcup_i (B \cap A_i)) =
    \sum_i P(B \cap A_i) = \sum_i\frac{P(B \cap A_i) \cdot P(A_i)}{P(A_i)} =
    $$
    $$
    = \sum_i P(B/A_i) \cdot P(A_i)
    $$
\end{proof}

\begin{theorem}[Байеса]
    Пусть $A_1, A_2 \ldots$ — полная система событий, $\forall i$ $P(A_i) > 0$, 
    $P(B) > 0$. Тогда $\forall k \geqslant 1$ $P(A_k/B) =
    \frac{P(B/A_k) \cdot P(A_k)}{\sum\limits_i P(B/A_i) \cdot P(A_i)}$
\end{theorem}
\begin{proof}
    Правая часть равна:
    $$
    \frac{P(B/A_k) \cdot P(A_k)}{P(B)} =
    \frac{P(B \cap A_k) \cdot P(A_k)}{P(A_k) \cdot P(B)} =
    \frac{P(B \cap A_k)}{P(B)} = P(A_k/B)
    $$
\end{proof}
\begin{examples}
    \begin{enumerate}
        \item Пусть с завода №1 поставлено 5 ящиков деталей, с завода №2 — 3 ящика,
        а с завода №3 — 2 ящика. Предположим также, что завод №1 допускает 2\% брака,
        завод №2 — 5\% брака, а завод №3 — 10\%. Какова вероятность выбрать хорошую
        деталь? Какова вероятность того, что деталь изготовлена на заводе №1 при
        условии, что она хорошая?
        
        Ящики считаем одинаковыми. Рассмотрим события $C_1, C_2, C_3$, где $C_i$
        означает выбрать ящик с завода №i, и событие $B$, означающее выбор хорошей 
        детали. Ясно, что $C1, C_2, C_3$ — полная система событий. Чтобы вычислить 
        вероятность события $B$, можно воспользоваться формулой полной вероятности:
        $$
        P(B) = \sum_{i = 1}^3 P(B/C_i) \cdot P(C_i) = 
        0.98 \cdot 0.5 + 0.95 \cdot 0.3 + 0.9 \cdot 0.2
        $$
        
        Чтобы ответить на второй вопрос, мы можем воспользоваться формулой Байеса:
        $$
        P(C_1/B) = \frac{0.98 \cdot 0.5}
        {0.98 \cdot 0.5 + 0.95 \cdot 0.3 + 0.9 \cdot 0.2}
        $$
        
        \item Представим, что у нас имеется ящик с шестью белыми и четырьмя чёрными 
        шариками. Сначала мы потеряли один шарик из этого ящика (какой — неизвестно),
        а затем из оставшихся мы вытащили два шарика. Какова вероятность вытащить
        два белых шарика? Какова вероятность того, что был потерян чёрный шар, при
        условии, что мы вытащили два белых шара?
        
        Введём два события, описывающие первый этап эксперимента: $C_б, C_ч$ — потеря
        белого и чёрного шаров соответственно. $C_б, C_ч$ — полная система событий.
        Пусть $B$ означает "вытащить два белых шарика".
        $$
        P(B) = P(B/C_б) \cdot P(C_б) + P(B/C_ч) \cdot P(C_ч) =
        \frac{C_5^2}{C_9^2} \cdot \frac{6}{10} + \frac{C_6^2}{C_9^2} \cdot \frac{4}{10}
        $$
        $$
        P(C_ч/B) = \frac{C_6^2 \cdot 4}{C_5^2 \cdot 6 + C_6^2 \cdot 4}
        $$
    \end{enumerate}
\end{examples}

\section{Независимые события. Пример Бернштейна}

Важно: нельзя путать понятия \emph{независимости} событий и \emph{несовместности}.

Пусть имеется эксперимент, описываемый с помощью вероятностного пространства
$(\Omega, \mathfrak{F}, P)$, и даны случайные события $A, B \in \mathfrak{F}$.

Независимость событий можно было бы рассматривать как выполнение равенств
$$
P(A/B) = P(A/\overline{B}) = P(A).
$$
Однако здесь нарушена симметрия - логично, что если событие $A$ независимо от $B$, то
и обратное тоже верно — $B$ независимо от $A$.

\begin{definition}
    $A$ и $B$ независимы, если $P(A\cap B) = P(A)P(B)$.
\end{definition}

Таким образом, если $P(B) > 0$, то независимость $A$ и $B$ равносильна
$P(A/B) = P(A)$.
\begin{proposition}[Свойства независимых событий]
    \begin{enumerate}
        \item $A$, $B$ независимы $\iff$ $A$, $\overline{B}$ независимы
        $\iff$ $\overline{A}$, $B$ независимы $\iff$ $\overline{A}$, $\overline{B}$ 
        независимы.
        \item $\forall A \in \mathfrak{F}$ $A$ и $\Omega$ независимы.
        \item $\forall A \in \mathfrak{F}$ $A$ и $\varnothing$ независимы.
    \end{enumerate}
\end{proposition}
\begin{proof}
    $$
    P(A) = P(A\cap \Omega) = P(A \cap (B \cup \overline{B})) =
    P((A \cap B) \cup (A \cap \overline{B}))
    $$
    $$
    P(A)(1 - P(B)) = P(A)P(B) = P(A \cap \overline{B})
    $$
\end{proof}
\begin{exercise}
    Пусть $A$ и $B$ независимы, $A$ и $C$ независимы. Верно ли, что $A$ и $B \cup C$
    независимы? Верно ли, что $A$ и $B \cap C$ независимы?
\end{exercise}

Можем ли мы определить понятие независимости для числа событий, большего $2$?
Для событий $A_1, A_2, \ldots, A_n$ мы можем выделить попарную независимость:
$$
\forall i\neq j\quad A_i, A_j \text{ независимы.}
$$
Или же независимость в совокупности (совместную): $A_1, \ldots, A_n$ независимы в
совокупности, если:
\begin{enumerate}
    \item $\forall i\neq j$ $A_i, A_j$ — независимы;
    \item $\forall i_1<i_2<i_3s$ $P(\bigcap_{j=1}^3 A_{ij}) = \prod_{j=1}^3P(A_{ij})$
    \item $P(\bigcap_{j=1}^n A_{ij}) = \prod_{j=1}^n P(A_{ij})$ и так далее.
\end{enumerate}
Это равносильно:
$$
\forall 2 \leqslant k \leqslant n \quad \forall i_1 < i_2 < \ldots < i_k\quad
P(\bigcap_{j=1}^k A_{ij}) = \prod_{j=1}^k P(A_{ij})
$$

\begin{example}[Берштейна]
    Рассмотрим эксперимент: будем подбрасывать тетраэдр с белой, синей, красной и 
    разноцветной (бело-сине-красной) гранями. Введём три события:
    $$Б = \{\text{внизу присутствует белый цвет}\}$$
    $$С = \{\text{внизу присутствует синий цвет}\}$$
    $$К = \{\text{внизу присутствует красный цвет}\}$$
    Проверим, что эти события попарно независимы. Верно ли, что
    $P(Б \cap С) = P(Б)P(С)$? Очевидно, что да. Значит, попарная независимость есть.
    Проверим теперь совместную независимость:
    $$
    P(Б\cap С\cap К) = \frac{1}{4} \neq \frac{1}{2} \cdot\frac{1}{2} \cdot\frac{1}{2} =
    P(Б)P(С)P(К)
    $$
    Это доказывает, что попарная независимость и совместная независимость 
    неравносильны.
\end{example}

\section{Независимые испытания Бернулли. Формулы Бернулли}

Пусть у нас есть вероятностное пространство $(\Omega, \mathfrak{F}, P)$.
Рассмотрим набор случайных событий $\mathfrak A = (A_1, \ldots, A_n)$,
причём $A_1, \ldots, A_n$ — полная системы событий. Определим \emph{испытание}
$A_1, \ldots, A_m$ как 
набор событий, являющийся полной системой событий.

\begin{definition}
    Испытания $A_1, \ldots, A_m$ будем называть \emph{независимыми}, если для любого
    набора $A_{1i_1}, A_{2i_2} \ldots, A_{mi_m}$ составляющие его события являются
    совместно независимыми.
\end{definition}

\begin{example}
    Представим, что мы одновременно подбрасываем монетку и кубик. Каким будет 
    вероятностное пространство? $\Omega = \{О1, О2, \ldots, О6, Р1, \ldots, Р6\}$
    Зададим испытании $A_1 = \{A_{11}, A_{12}\}$, где
    $A_{11} = \{\text{на монете О}\}$,
    $A_{12} = \{\text{на монете Р}\}$;
    $A_2 = \{A_{21},\ldots,A_{26}\}$, где $A_{2i}=\{\text{на кубике цифра } i\}$.
    $$
    P(A_{11}\cap P(A_{23}) = \frac{1}{12},\quad P(A_{11})=\frac{1}{2}\quad
    P(A_{23})=\frac{1}{2}
    $$
\end{example}

Испытаниями Бернулли называются набор из $n$ независимых испытаний с двумя
исходами в каждом из 
них, условно называемыми успехом и неудачей, и с постоянной вероятностью успеха во всех 
испытаниях. Будем обозначать такой набор $(У_1, У_2, Н_3, \ldots, У_n$, а вероятность
успеха $P(У_k)=p$.

Пусть
$A = \{\text{в } n \text{ испытаниях Бернулли успех произошёл ровно }
k \text{ раз}\}$. Какова вероятность $A$?
Заметим, что $A =(\underbrace{УУ\ldots У}_{k} \underbrace{НН\ldotsН}_{n-k}) \cup
(\underbrace{УУ\ldots УУ}_{k-1}НУНН\ldots Н) \cup \ldots$.
$$
P(УУ\ldots УНН\ldots Н) = P(У_1 \cap У_2 \cap \ldots \cap У_k \cap Н_{k+1} \cap
\ldots \cap Н_n) = p^k(1-p)^{n-k}
$$
Ясно, что вероятность любой цепочки, содержащей $k$ успехов и $n-k$ неудач,
равна $p^k(1-p)^{n-k}$

Получаем, что в нашем примере $P(A) = C_n^k p^k(1-p)^{n-k} = P_n(k)$ — эта формула
носит имя Бернулли.

\begin{corollary}
\begin{enumerate}
    \item $P_n(n)=p^n$
    \item $P_n(0) = (1 - p)^n$
    \item $P_n(\text{хотя бы один успех}) = 1 - P_n(0) = 1 - (1-p)^n$
\end{enumerate}
\end{corollary}

\begin{example}
    Пусть мы подбрасываем монету 10 раз. Какова вероятность того, что все десять раз
    выпал орёл? По формуле Бернулли:
    $$
    P_{10}(10) = (\frac{1}{2})^{10} = \frac{1}{1024}
    $$
    А вероятность того, что орёл выпал ровно пять раз, равна
    $$
    P_{10}(5) = C_{10}^5 (\frac{1}{2})^5 (\frac{1}{2})^5 = \frac{252}{1024}
    $$
    Возникает вопрос, каково наиболее вероятное число выпадений орла?
    $P_{10}(k) = C_{10}^k (\frak{1}{2})^{10}$, и ясно, что максимум достигается при
    $k=5$
\end{example}

Обобщим последний вопрос примера.
Пусть имеется $n$ испытаний Бернулли. Вероятность успеха в каждом испытании равна $p$.
Чему равно наиболее вероятное число появления успехов?

Рассмотрим неравенство: 
$$
P_n(k) < P_n(k+1)
$$
Его можно переписать:
$$
C_n^k p^k(1-p)^{n-k} < C_n^{+1-1} p^{k+1} (1-p)^{n-k-1}
$$
$$
\frac{n!}{k! (n-k)!} p^k (1-p)^{n-k} < \frac{n!}{(k+1)!(n-k-1)!} p^{k+1}(1-p)^{n-k-1}
$$
Полученное равносильно:
$$
(k+1)(1-p) < p(n-k)
$$
$$
k < (n+1)p - 1
$$

Если мы теперь посмотрим на обратное неравенство $P_n(k) > P_n(k+1)$, то увидим, что
оно равносильно $k > (n+1)p - 1$.

Рассмотрим два случая:
\begin{enumerate}
    \item $(n+1)p \notin \mb Z$. Обозначим за $k_n^\ast$ наиболее вероятное число
    успехов в $n$ испытаниях. Тогда $k_n^\ast=[(n+1)p]$
    \item $(n+1)p \in \mb Z$. В этому случае $k_n^{\ast_1} = (n+1)p - 1$ и
    $k_n^{\ast_2} = (n+1)p$ — наиболее вероятные числа успехов.
\end{enumerate}

\begin{example}
    Пусть $p = \frac{1}{2}$, $n = 10$. Тогда $k_{10}^\ast = 5$. Если же $n = 11$, то
    $k_{11}^{\ast_1} = 5$ и $k_{11}^{\ast_2} = 6$, так как $C_{11}^5 = C_{11}^6$.
\end{example}

\section{Предельные теоремы в схеме испытаний Бернулли}

Представим, что мы подбрасываем монету 10000 раз. Ясно, что наиболее вероятное число 
выпадений орла равна 5000. Чему же равна вероятность такого исхода?

$P_{10000}(5000) = C_{10000}^{5000}(\frac{1}{2})^{10000}$. Мы хотели бы оценить это 
число.

Рассмотрим функции:
$$
\varphi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},
\quad \Phi(x) = \int\limits_{-\infty}^x \varphi(t)\mathrm dt
$$

Ясно, что $\varphi(x)$ — чётная, а $\Phi(-x) = 1 -\Phi(x)$.

Пусть $n$ — число испытаний, $k$ — число успехов, $p$ — вероятность успеха, $q=1-p$.
Введём обозначение: $x_{n,k} = \frac{k - np}{\sqrt{npq}}$.

\begin{theorem}[Локальная теорема Муавра-Лапласа]
    Справедливо следующее соотношение:
    $$
    \frac{P_n(k)}{\frac{1}{\sqrt{npq}}\cdot \frac{1}{\sqrt{2\pi}}\cdot
    e^{-\frac{x_{n,k}^2}{2}}} \underto{n \to \infty} 1\quad \text{равномерно по всем } k:\quad
    |x_{n,k}|\leqslant Cn^{\frac{1}{6}-\varepsilon}\quad \forall C>0,\, \varepsilon > 0
    $$
    
    Таким образом, $P_n(k) \approx \frac{1}{\sqrt{npq}}\varphi(x_{n,k})$.
\end{theorem}
\begin{lemma}[Формула Стирлинга]
    $$
    n! = n^n e^{-n} \sqrt{2\pi n}\cdot(1 + o(1))\, (n \to \infty)
    $$
\end{lemma}
\begin{lemma}\label{laplacetheoremlemma2}
    $$
    \ln(1+x) = x - \frac{x^2}{2} + \theta x^3,\text{, где } |\theta| \leqslant 3
    \quad\forall |x| < \frac{1}{2}
    $$
\end{lemma}
\begin{proof}[Доказательство теоремы]
    По формуле Бернулли:
    $$
    P_n(k) = \frac{n!}{k!(n-k)!}p^k q^{n-k} =
    \frac{n^n e^{-n}\sqrt{2\pi n} (1 + o(1))}{k^k e^{-k} \sqrt{2\pi k} (1 + o(1))
    (n-k)^{n-k} e^{-n+k}\sqrt{2\pi (n-k)}(1 + o(1))}
    $$
    Но это верно при $b \to \infty$, $k \to \infty$, $n-k \to \infty$.
    $$
    k = np + x_{n,k}\sqrt{npq} \underto{n \to \infty} \infty
    $$
    $$
    n-k = nq - x_{n,k}\sqrt{npq} \underto{n \to \infty} \infty
    $$
    Продолжая вычисления:
    $$
    P_n(k) = (\frac{k}{np})^{-k-\frac{1}{2}}(\frac{n-k}{nq})^{-n+k-\frac{1}{2}}
    \frac{1}{\sqrt{npq}}\frac{1}{\sqrt{2\pi}}(1 + o(1))
    $$
    Пусть $\sqrt{npq}P_n(k) = T_{n,k}$. Тогда:
    $$
    T_{n,k} = (\frac{k}{np})^{-k-\frac{1}{2}}(\frac{n-k}{nq})^{-n+k-\frac{1}{2}}
    \frac{1}{\sqrt{2\pi}}(1 + o(1))
    $$
    $$
    \ln T_{n,k} = (-k - \frac{1}{2})\ln \frac{k}{np} + (-n+k-\frac{1}{2})
    \ln \frac{n-k}{nq} = \ln \frac{1}{\sqrt{2\pi}} + o(1)
    $$
    С учётом
    $$
    \frac{k}{np} = 1 + x\sqrt{\frac{q}{np}}
    $$
    $$
    \frac{n-k}{nq} = 1 - x\sqrt{\frac{p}{nq}}
    $$
    Применив лемму \ref{laplacetheoremlemma2}, получим:
    $$
    \ln T_{n,k} = (-np - x\sqrt{npq} - \frac{1}{2})(x\sqrt{\frac{q}{np}} -
    \frac{x^2}{2}\frac{q}{np} + \theta_1 \frac{x^3 q\sqrt{q}}{np\sqrt{np}}) +
    $$
    $$
    + (-np + x\sqrt{npq} - \frac{1}{2})(-x\sqrt{\frac{p}{nq}} -
    \frac{x^2p}{2nq} + \theta_2x^3\frac{p}{nq}\frac{\sqrt{p}}{\sqrt{nq}}) +
    \ln \frac{1}{\sqrt{2\pi}} + o(1) =
    $$
    $$
    = -x\sqrt{npq} + \frac{x^2}{2}q + o(1) - x^2q + x\sqrt{npq} + \frac{x^2}{2}p
    - x^2p + \ln \frac{1}{\sqrt{2\pi}} =
    $$
    $$
    =-\frac{x^2}{2} + \ln \frac{1}{\sqrt{2\pi}} + o(1) 
    $$
    Тогда само $T_{n,k}$ равно:
    $$
    T_{n,k} = e^{-\frac{x^2}{2}} \frac{1}{\sqrt{2\pi}} e^{o(1)}
    $$
    Отсюда:
    $$
    P_n(k) = \frac{1}{\sqrt{npq}}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2_{n,k}}{2}}
    e^{o(1)}
    $$
    Теорема доказана.
\end{proof}

Попробуем ответить на вопрос: как найти $P_n(a < \text{число успехов} < b)$
при $a<b$ и больших $n$? Применение локальной теоремы Муавра-Лапласа может давать
слишком высокие погрешности, поэтому необходимо использовать иное решение.
\begin{theorem}[Интегральная теорема Муавра-Лапласа]
    Пусть $p$ — вероятность успеха, $0 < p < 1$. Тогда:
    $$
    \sup_{a<b} \Bigg| P_n(a < \text{число успехов} < b) -
    \int\limits_{\frac{a - np}{\sqrt{npq}}}^{\frac{b-np}{\sqrt{npq}}}
    \varphi(t) \dif t \Bigg| \underto{n \to \infty} 0
    $$
\end{theorem}

Мы докажем эту теорему позже — как частный случай более общей теоремы.

\begin{corollary}
    $$
    \int\limits_{-\infty}^{+\infty}
    \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}\dif t = 1
    $$
\end{corollary}
\begin{proof}
    Примем $a = -\infty$, $b = +\infty$.
\end{proof}

Если мы возьмём функцию $\Phi(x) = \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^x
e^{-\frac{t^2}{2}}\dif t$,
то теорему можно сформулировать так:
$$
P_n(a < \text{число успехов} < b) \approx
\Phi\bigg(\frac{b-np}{\sqrt{npq}}\bigg) -
\Phi\bigg(\frac{a - np}{\sqrt{npq}}\bigg)
$$

\begin{example}
    Подсчитаем вероятность того, что при подбрасывании монетки 10000 раз <<орёл>>
    выпадет 5000 раз. Для этого применим локальную теорему Муавра-Лапласа.
    $$
    P_{10000}(5000) \approx
    \frac{1}{\sqrt{10000 \cdot \frac{1}{2} \cdot \frac{1}{2}}}\varphi(0) =
    \frac{1}{50} \cdot 0.39894
    $$
    Если мы захотим подсчитать вероятность того, что <<орёл>> выпадет как минимум 
    4900 раз и как максимум 5100, то необходимо будет применить интегральную
    теорему:
    $$
    P_{10000}(4900 < \text{число успехов} < 5100) \approx \ldots
    $$
\end{example}

\begin{theorem}[Пуассона]
    Будем рассматривать схему серий испытаний Бернулли. Допустим, первая серия 
    состоит из одного испытания такого, что $p_1 = P_1(У)$. Вторая серия
    состоит из двух испытаний и $p_2 = P_1(У)$.
    $n$-я серия испытаний состоит из $n$ испытаний и $p_n = P_1(У)$.
    (Здесь $P_1(У)$ — вероятность успеха в одном испытании для каждой серии 
    соответственно.)
    Пусть также $np_n = \lambda > 0$. Тогда:
    $$
    P_n(k) \underto{n \to \infty} \frac{\lambda^k}{k!}e^{-\lambda}
    $$
\end{theorem}
\begin{proof}
    $$
    P_n(k) = C_n^k p_n^k(1 - p_n)^{n-k} = \frac{n!}{k!(n-k)!}p_n^k(1-p_n)^{n-k} =
    $$
    $$
    = \frac{1}{k!} \frac{n(n-1)\ldots(n-k+1)}{n^k} n^k
    p_n^k(1-p_n)^n(1-p_n)^{-k} =
    $$
    $$
    = \frac{\lambda^k}{k!}
    \underbrace{(1 - \frac{1}{n})\ldots(1 - \frac{k-1}{n})}_{\to 1} \cdot
    \underbrace{(1 - \frac{\lambda}{n})^n}_{\to e^{-\lambda}}\cdot
    \underbrace{(1 - \frac{\lambda}{n})^{-k}}_{\to 1}    
    $$
\end{proof}

\begin{theorem}[Закон больших чисел Бернулли]
    $$
    \forall \varepsilon > 0\quad P_n\bigg(\bigg|\frac{k_n}{n} -
    p\bigg| > \varepsilon\bigg)
    \underto{n \to \infty} 0
    $$
    Здесь $\frac{k_n}{n}$ называется частотой успеха.
\end{theorem}
\begin{proof}
    Рассмотрим вероятность:
    $$
    P_n\big(\big|\frac{k_n}{n} -
    p\big| > \varepsilon\big) = 1 - P_n(\big|\frac{k_n}{n} -
    p\big| \leqslant \varepsilon\big) =
    \frac{1}{\sqrt{2\pi}}
    \int\limits_{-\infty}^{+\infty}e^{-\frac{t^2}{2}} \dif t -
    P_n(np - n\varepsilon \leqslant k_n \leqslant n\varepsilon + np) =
    $$
    = $$
    \frac{1}{\sqrt{2\pi}}
    \int\limits_{|t|\leqslant \varepsilon \sqrt{\frac{n}{pq}}}
    e^{-\frac{t^2}{2}} \dif t + \frac{1}{\sqrt{2\pi}}
    \int\limits_{|t| > \varepsilon \sqrt{\frac{n}{pq}}}
    e^{-\frac{t^2}{2}} \dif t -
    P_n(np - n\varepsilon \leqslant k_n \leqslant n\varepsilon + np)
    $$
    Разность первого и третьего слагаемого стремится к нулю, второе слагаемое тоже
    стремится к нулю. Теорема доказана.
\end{proof}

\section{Случайная величина. Распределение случайных величин}

Будем рассматривать вероятностное пространство $(\Omega, \mathfrak{F}, P)$.

\begin{definition}
    Функция $\xi: \Omega \to \real$ такая, что
    $\forall B \in \mathfrak B$
    $\xi^{-1}(B) \in \mathfrak F$, называется \emph{случайной величиной}.
    (Здесь $\mathfrak B$ обозначает борелевскую сигма-алгебру.)
\end{definition}
\begin{examples}
    \begin{enumerate}
        \item Пусть мы подбрасываем игральный кубик.
        $\Omega = \{\omega_1,\ldots,\omega_6\}$. Пусть $\xi_1(\omega_i) = i$,
        $\xi_2(\omega_i) =
        \begin{cases}
            0,\quad i \neq 6\\
            1,\quad i = 6\\  
        \end{cases}
        $. $\xi_1$, $\xi_2$ — случайные величины.
        \item Пусть $\Omega = \{x \big| x \in [0, 1]\}$,
        $\mathfrak F = \{[0, \frac{1}{2}],
        (\frac{1}{2}, 1], [0, 1], \varnothing\}$,
        $\eta(x) = x$. $\eta$ не будет случайной величиной, так как
        $\eta^{-1}([0, \frac{1}{3}]) = [0, \frac{1}{3}] \notin \mathfrak F$.
    \end{enumerate}
\end{examples}

Рассмотрим функцию $\mathfrak P_\xi: \mathfrak B \to \real$ такую, что
$\forall B \in \mathfrak B$ $\mathfrak P_\xi(B) = P(\xi^{-1}(B))$.

\begin{definition}
    $\mathfrak P_\xi$ называется \emph{распределением случайной величины $\xi$}.
\end{definition}
\begin{theorem}
    $\mathfrak P_\xi$ является вероятностью.
\end{theorem}
\begin{proof}
    Проверим аксиомы вероятности.
    \begin{enumerate}
        \item $\distr(B) = P(\xi^{-1}(B)) \geqslant 0$
        \item $\distr (\real) =P(\xi^{-1}(\real)) = P(\Omega) = 1$
        \item Пусть $B_1, B_2, \ldots \in B$, $B_i \cap B_j = \varnothing$
        ($i \neq j$).
        $$
        \distr (\bigcup\limits_{i=1}^\infty B_i) =
        P(\xi^{-1}(\bigcup\limits_{i=1}^\infty B_i) =
        $$
        $$
        = P(\bigcup\limits_{i=1}^\infty \underbrace{\xi^{-1}
        (B_i)}_{\text{попарно несовместны}}) =
        \sum\limits_i P(\xi^{-1}(B_i)) = \sum\limits_i \distr (B_i)
        $$
    \end{enumerate}
\end{proof}

Теперь мы можем перейти к использованию вероятностного пространства
$(\real, \mathfrak B, \distr)$.

\begin{definition}
    Функция $\funcdistr: \real \to \real$ такая, что
    $\forall x\in \real$ $\funcdistr(x) = \distr((-\infty, x)) = 
    P(\xi^{-1}((-\infty, x))) = P(\omega:\, \xi(\omega) < x) = P(\xi < x)$,
    называется \emph{функцией распределения случайной величины $\xi$}.
\end{definition}

Рассмотрим значение $\funcdistr(b) = \distr((-\infty, b)) =
\distr((-\infty, a) \cup [a, b)) = \distr((-\infty,a)) + \distr([a, b)) =
\funcdistr(a) + \distr([a, b)))$. Получается, что
$\distr([a, b)) = \funcdistr(b) - \funcdistr(a)$, то есть,
существует взаимно-однозначное соответствие между $\distr$ и $\funcdistr$.

\begin{proposition}[Свойства функции распределения]
    \begin{enumerate}
        \item $\forall x$ $0\leqslant \funcdistr(x) \leqslant 1$;
        \item $\funcdistr$ неубывает;
        \item\label{funcdistr_cont} $\funcdistr$ непрерывна слева во всех точках.
        \item\label{funcdistr_lim} $\funcdistr(x) \underto{x \to -\infty} 0$,
        $\funcdistr(x) \underto{x \to \infty} 1$
    \end{enumerate}
\end{proposition}
\begin{proof}
    Докажем свойство \ref{funcdistr_cont}.
    Пусть $x_1 < x_2 < \ldots$ и $x_n \underto{n \to \infty} x$.
    Мы хотим доказать, что $\funcdistr(x_n) \underto{n \to \infty} \funcdistr(x)$.
    $(-\infty, x_n) \subset (-\infty, x_{n+1})$ и $\bigcup\limits_n(-\infty, x_n)
    = (-\infty, x)$. Отсюда, по свойству вероятности,
    $\underbrace{\distr((-\infty, x_n))}_{\funcdistr(x_n)}
    \underto{n \to \infty} \underbrace{\distr((-\infty, x))}_{\funcdistr(x)}$.
    Всё доказано.
    
    Докажем свойство \ref{funcdistr_lim}.
    Пусть $\forall n \geqslant 1$ $[-n, n) \subset [-(n+1), n+1)$ и
    $\bigcup\limits_{n \geqslant 1}[-n, n) = \real$. Тогда:
    $$
    \underbrace{\distr([-n, n))}_{\funcdistr(n) - \funcdistr(-n)}
    \underto{n \to \infty} \distr(\real) = 1
    $$
    Отсюда получаем, что $\lim\limits_{n \to \infty} \funcdistr(n) = 1$
    и $\lim\limits_{n \to \infty} \funcdistr(-n) = 0$. Свойство \ref{funcdistr_lim}
    доказано.
\end{proof}

Мы доказали, что функция распределения удовлетворяет указанным свойствам, однако
верно и обратное.

\begin{proposition}
    Пусть $G: \real \to \real$ — функция, удовлетворяющая свойствам функции
    распределения. Тогда $G$ является функцией распределения некоторой случайной 
    величины.
\end{proposition}
\begin{proof}
    Упражнение.
\end{proof}

\section{Различные типы распределений случайных величин. Случайные величины
с дискретным распределением}

\begin{definition}
    Случайная величина $\xi$ называется случайной величиной с дискретным 
    распределением, если существует не более чем счётное подмножество
    вещественных чисел $A$ такое, что $P(\xi \in A) = 1$. Другими словами, у этой
    случайной величины конечное или счётное число значений.
\end{definition}

Занумеруем все элементы множества $A$: $A = \{a_1, a_2,\ldots\}$, обозначим
$p_i = P(\xi = a_i)$. Правило, сопоставляющее каждому значению $a_i$ случайной
величины $\xi$ вероятность $p_i$, называется \emph{законом распределения}.
\begin{examples}
    \begin{enumerate}
        \item $\xi = c$, $p = 1$ — случайная величина с вырожденной в точке
        $c$ распределением. График функции распределения $\funcdistr(x) =
        P(\xi < x)$ представляет собой ступенчатую функцию, принимающую
        значение $0$ при $x \leqslant c$ и $1$ при $x > c$.
        \item $(\xi, p): (0, 1-p), (1, p)$ — случайная величина с
        распределением Бернулли. График — ступенчатая функция, принимающая значения
        $1-p$ и $p$ в точках $0$ и $1$ соответственно.
        \item $(\xi, p): (a_1, p_1), (a_2, p_2), \ldots, (a_n, p_n)$. График —
        аналогичная ступенчатая функция.
        \item $\xi: 0, 1, 2, \ldots, n$ и $p_k = P(\xi = k) = C_n^k p^k(1-p)^{n-k}$ 
        — случайная величина с биномиальным законом распределения и
        параметрами $n$ и $p$.
        \item $\xi: 0, 1, 2, \ldots$ и
        $P(\xi = k) = \frac{\lambda^k}{k!}e^{-\lambda}$ — случайная величина
        с распределением Пуассона с параметром $\lambda > 0$.
        \item $\xi: 0, 1, 2, \ldots$ и $P(\xi = k) = p^k(1-p)$ —
        случайная величина с геометрическим распределением с параметром
        $p$ ($0 \leqslant p \leqslant 1$).
    \end{enumerate}
\end{examples}
\begin{definition}
    Случайная величина $\xi$ называется случайной величиной с абсолютно 
    непрерывным распределением, если существует функция $p(x)$ такая, что
    для любого $y \in \real$ $F_\xi(y) = \int\limits_{-\infty}^y p(x) \dif x$.
    В этом случае $p(x) = p_\xi(x)$ называется \emph{плотностью распределения 
    случайной величины $\xi$}.
\end{definition}
\begin{proposition}[Свойства плотности распределения]
    \begin{enumerate}
        \item $p(x) \geqslant 0$.
        \item $\int\limits_{-\infty}^{+\infty} p(x) \dif x = 1$.
    \end{enumerate}
\end{proposition}
\begin{examples}
    \begin{enumerate}
        \item $\xi$ — случайная величина с равномерным распределением на
        отрезке $[a,b]$. (Обозначается $\xi \in U_{[a,b]}$).
        $$
        p_\xi (x) =
        \begin{cases}
            \frac{1}{b-a},\quad x \in [a,b] \\
            0, \quad x \notin [a,b] \\
        \end{cases}
        $$
        \item $\xi$ — случайная величина с экспоненциальным распределением с 
        параметром $\alpha$.
        $$
        p_\xi(x) =
        \begin{cases}
            0,\quad x < 0 \\
            \alpha e^{-\alpha x},\quad x \geqslant 0 \\
        \end{cases}
        $$
        \item (Важный пример!) $\xi$ — случайная величина с нормальным 
        распределением с параметрами $a \in \real$ и $\sigma^2 > 0$.
        $$
        p_\xi = \frac{1}{\sqrt{2\pi \sigma^2}}e^{- \frac{(x - a)^2}{2\sigma^2}}
        $$
        В случае, когда $a \neq 0$, $\sigma^2 = 1$, говорят о \emph{стандартном 
        нормальном распределении}.
        В этом случае $p_\xi = \varphi$ — функция Гаусса.
        \item $\xi$ — случайная величина с распределением Коши.
        $p_\xi(x) = \frac{1}{\pi (1 + x^2)}$.
    \end{enumerate}
\end{examples}

Заметим, что $P(a < \xi < b) = P(a \leqslant \xi < b) =
P(a \leqslant \xi \leqslant b) = F_\xi(b) - F_\xi(a)$. Почему строгость неравенства
не имеет значения? В силу непрерывности функции распределения
$P(a < \xi < b) = F_\xi(b) - F_\xi(a) \underto{b \to a} 0$.

\section{Случайные векторы. Распределение случайных векторов}

\begin{definition}
    Пусть $\xi_1, \ldots, \xi_d$ — случайные величины, определённые на одном
    вероятностном пространстве. Тогда $\xi = (\xi_1, \ldots, \xi_d)$ будет
    называться случайным $d$-мерным вектором, $\xi: \Omega \to \real^d$.
    Иначе говоря, $\forall B \in \mathfrak B^d$ $\xi^{-1}(B) \in \mathfrak F$.
\end{definition}

\begin{definition}
    Функция $F_{(\xi_1, \ldots, \xi_d)}: \real^d \to \real$ такая, что
    $\forall x_1, \ldots, x_d$ $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d) =
    P(\xi_1 < x_1, \ldots, \xi_d < x_d)$, называется функцией распределения
    случайного вектора.
\end{definition}

Рассмотрим случай $d = 2$, $F_{(\xi_1, \xi_2)} (x_1, x_2) =
P(\xi_1 < x_1, \xi_2 < x_2)$.

$\xi_1, \ldots, \xi_d$ — с дискретным распределением, если существует $A \subset 
\real^d$ такое, что $A$ не более чем счётно и
$P((\xi_1, \ldots, \xi_d) \in A) = 1$.

$\xi_1, \ldots, \xi_d$ — с абсолютно непрерывным распределением, если существует
$p(x_1, \ldots, x_d)$ такое, что $F_{(\xi_1, \ldots, \xi_d)}(y_1, \ldots, y_d) =
\int\limits_{-\infty}^{y_1} \ldots \int\limits_{-\infty}^{y_d}
p(x_1, \ldots, x_d) \dif x_1 \ldots \dif x_d$.

\begin{proposition}[Свойства функции распределения случайного вектора]
\mbox{}
    \begin{enumerate}
        \item  $0 \leqslant F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d)
        \leqslant 1$
        \item Пусть $x_1 \leqslant y_1, \ldots, x_d \leqslant y_d$.
        Тогда $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d) \leqslant
        F_{(\xi_1, \ldots, \xi_d)}(y_1, \ldots, y_d)$
        \item $F_{(\xi_1, \ldots, \xi_d)}$ непрерывна слева по каждой координате.
        \item $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d)
        \underto{x_k \to -\infty} 0$.
        \item $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d)
        \underto{x_k \to +\infty}
        F_{(\xi_1, \ldots, \xi_{k-1}, \xi_{k+1}, \ldots \xi_d)}
        (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots x_d)$.
        \item $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d)
        \underto{x_1 \to +\infty, \ldots, x_d \to +\infty} 1$
        \item Пусть $d = 2$. Тогда
        $P\big((\xi_1, \xi_2) \in [a_1, b_1) \times [a_2, b_2)\big) =
        F_{(\xi_1, \xi_2)}(b_1, b_2) - F_{(\xi_1, \xi_2)}(a_1, b_2) -
        F_{(\xi_1, \xi_2)}(b_1, a_2) + F_{(\xi_1, \xi_2)}(a_1, a_2)$. То есть, 
        вероятность попадания точки в произвольный параллелепипед
        однозначно выражается 
        через значения функции распределения в вершинах этого параллелепипеда.
    \end{enumerate}
\end{proposition}

\section{Независимость случайных величин}

Вспомним, что события $A, B \in \mathfrak F$ называются независимыми, если
$P(A \cap B) = P(A)P(B)$. Введём теперь понятие независимости для случайных
величин.

\begin{definition}\label{random_var_independence1}
    Пусть $\xi, \eta$ — случайные величины, определённые на одном вероятностном
    пространстве. Будем говорить, что $\xi$ и $\eta$ — независимые случайные 
    величины, если для любых вещественных $x, y$
    $F_{(\xi, \eta)}(x, y) = F_\xi(x)F_\eta(y)$.
\end{definition}
\begin{definition}\label{random_var_independence2}
    $\xi$ и $\eta$ — независимые случайные величины, если
    $\forall a_1 < b_1, a_2 < b_2$ $P\big((\xi, \eta) \in [a_1, b_1) \times
    [a_2, b_2)\big) = P(a_1 \leqslant \xi < b_1) \cdot P(a_2 \leqslant
    \eta < b_2)$.
\end{definition}

\begin{proposition}
    Определения \ref{random_var_independence1} и \ref{random_var_independence2} 
    эквивалентны.
\end{proposition}
\begin{proof}
\mbox{}
    \begin{enumerate}
        \item (\ref{random_var_independence1} $\Rightarrow$
        \ref{random_var_independence2})
        $$
        P\big((\xi, \eta) \in [a_1, b_1) \times
        [a_2, b_2)\big) = P(a_1 \leqslant \xi < b_1) \cdot P(a_2 \leqslant
        \eta < b_2)
        $$
        При $a_1, a_2 \to -\infty$ левая часть равенства стремится к
        $F_{(\xi, \eta)}(b_1, b_2)$, а правая часть — к
        $F_\xi(b_1)\cdot F_\eta(b_2)$.
        \item (\ref{random_var_independence2} $\Rightarrow$
        \ref{random_var_independence1})
        $$
        P\big((\xi, \eta) \in [a_1, b_1) \times
        [a_2, b_2)\big) = F_{(\xi, \eta)}(b_1, b_2) - F_{(\xi, \eta)}(a_1, b_2)
        - F_{(\xi, \eta)}(b_1, b_a) +
        $$
        $$
        + F_{(\xi, \eta)})(a_1, a_2) = F_\xi(b_1)F_\eta(b_2) - 
        F_\xi(a_1)F_\eta(b_2) - F_\xi(b_1)F_\eta(a_2) +
        $$
        $$
        +F_\xi(a_1)F_\eta(a_2) = \big(F_\xi(b_1) - F_\xi(a_1)\big)
        \big(F_\eta(b_2) - F_\eta(a_2)\big) = 
        $$
        $$
        P(a_1 \leqslant \xi < b_1) \cdot
        P(a_2 \leqslant \eta < b_2)
        $$
    \end{enumerate}
\end{proof}
\begin{definition}\label{random_var_independence3}
    Пусть для любых $B_1,\ldots, B_d \in \mathfrak B$ $P(\xi_1 \in B_1, \ldots,
    \xi_d \in B_d) = \prod\limits_{i=1}^d P(\xi_i \in B_i)$. Тогда $\xi_1, \ldots,
    \xi_d$ называются независимыми случайными величинами.
\end{definition}

Очевидно, что это определение также эквивалентно двум предыдущим.

\begin{example}
        Рассмотрим случайный вектор $\xi = (\xi_1, \xi_2)$ с плотностью 
        распределения
        $$
        p_\xi(x, y) =
        \begin{cases}
            1,\quad 0\leqslant x \leqslant 1,\, 0\leqslant y \leqslant 1 \\
            0\quad \text{иначе} \\
        \end{cases}
        $$
        Здесь:
        $$
        F_{\xi_1}(x) = P(\xi_1 < x) =
        \begin{cases}
            0,\quad x\leqslant 0 \\
            x,\quad 0 < x \leqslant 1 \\
            1,\quad x > 1 \\
        \end{cases}
        $$
        $$
        F_{\xi_2}(y) = P(\xi_2 < y) =
        \begin{cases}
            0,\quad y\leqslant 0 \\
            y,\quad 0 < y \leqslant 1 \\
            1,\quad y > 1 \\
        \end{cases}
        $$
        $$
        F_{(\xi_1, \xi_2)}(x, y) =
        \begin{cases}
            0,\quad x\leqslant 0 \text{ или } y \leqslant 0 \\
            xy,\quad 0 < x \leqslant 1 \text{ и } 0 < y \leqslant 1 \\
            x,\quad 0 < x \leqslant 1 \text{ и } y > 1 \\
            y,\quad 0 < y \leqslant 1 \text{ и } x > 1 \\
            1,\quad x > 1 \text{ и } y > 1 \\
        \end{cases}
        $$
        Видим, что $F_{(\xi_1, \xi_2)}(x, y) = F_{\xi_1}(x)F_{\xi_2}(y)$, значит,
        $\xi_1$ и $\xi_2$ независимы.
\end{example}

Заметим, что для того, чтобы доказать, что события не являются независимыми,
достаточно найти хотя бы одну точку, в которой определение нарушается.

Мы хотим найти более простой критерий независимости событий.

Рассмотрим для начала случай дискретного распределения, ограничимся двумерным 
случаем.

\begin{proposition}
    $\xi = (\xi_1, \xi_2)$ — случайный вектор с дискретным распределением
    тогда и только тогда, когда его компоненты — с дискретным распределением.
\end{proposition}
\begin{proof}
    Очевидно.
\end{proof}

\begin{theorem}
    Пусть $\xi = (\xi_1, \xi_2)$ — с дискретным распределением.
    Тогда независимость
    случайных величин $\xi_1$ и $\xi_2$ равносильна тому, что для любых 
    вещественных $a$ и $b$ выполняется $P(\xi_1 = a, \xi_2 = b) =
    P(\xi_1 = a)P(\xi_2 = b)$.
\end{theorem}
\begin{proof}
        Если $\xi_1$, $\xi_2$ независимы, то
        $\forall a,b \in \real$ и $\forall \varepsilon > 0$
        $P(a \leqslant \xi_1 \leqslant a + \varepsilon,
        b \leqslant \xi_2 < b - \varepsilon) =
        P(a \leqslant \xi_1 \leqslant a + \varepsilon)
        P(b \leqslant \xi_2 < b - \varepsilon)$ — по определению 
        \ref{random_var_independence2}. Устремим $\varepsilon$ к нулю.
        
        Обратно:
        $$
        F_{(\xi_1, \xi_2)}(y_1, y_2) =
        \sum\limits_{a_i < y_1, b_j < y_2} P(\xi_1 = a_i, \xi_2 = b_j) =
        \sum\limits_{a_i < y_1, b_j < y_2} P(\xi_1 = a_i)P(\xi_2 = b_j) =
        $$
        $$
        = \sum\limits_{b_j < y_2}\Bigg(\sum\limits_{a_i < y_1}
        P(\xi_1 = a_i)P(\xi_2 = b_j)\Bigg) =
        \sum\limits_{a_i < y_1} P(\xi_1 = a_i) \cdot
        \sum\limits_{b_j < y_2} P(\xi_2 = b_j) =
        F_{\xi_1}(y_1) F_{\xi_2}(y_2)
        $$
\end{proof}

Теперь рассмотрим случай непрерывного распределения.

\begin{proposition}
    Если $\xi = (\xi_1, \ldots, \xi_d)$ — с абсолютным непрерывным распределением,
    то $\forall k = 1, 2,\ldots, d$ $\xi_k$ с абсолютно непрерывным 
    распределением.
\end{proposition}

Заметим, что обратное, вообще говоря, неверно.

Таким образом, $\xi$ — случайный вектор с абсолютно непрерывным 
распределением тогда и только тогда, когда для любых $y_1, \ldots, y_d$
$$
F_\xi(y_1, \ldots, y_d) = \int\limits_{-\infty}^{y_1}\ldots
\int\limits_{-\infty}^{y_d} p_\xi(x_1, \ldots, x_d) \dif x_1\ldots \dif x_d
$$
При $y_2, \ldots, y_d \to \infty$ левая часть стремится к $F_{\xi_1}(y_1)$,
а в правой будет
$$
\int\limits_{-\infty}^{y_1}\Bigg(\int\limits_{-\infty}^{+\infty}\ldots
\int\limits_{-\infty}^{+\infty} p_\xi(x_1, \ldots, x_d)
\dif x_2\ldots\dif x_d\Bigg)\dif x_1
$$
Что по определению равно $p_{\xi_1}(x_1)$ — плотности распределения $\xi_1$.

Помимо этого, $\xi = (\xi_1, \ldots, \xi_d)$ —
с абсолютно непрерывным распределением
$\iff$ для любых $a_1 < b_1, \ldots, a_d < b_d$ выполнено
$$
P(a_1 \leqslant \xi_1 < b_1, \ldots, a_d \leqslant \xi_d < b_d =
\int\limits_{a_1}^{b_1}\ldots\int\limits_{a_d}^{b_d} p_\xi(x_1, \ldots, x_d)
\dif x_1\ldots \dif x_d
$$
Что также равносильно
$$
\forall B \in \mathfrak B^d\quad P(\xi \in B) = \int\limits_B p_\xi(x_1, \ldots, x_d)
\dif x_1\ldots \dif x_d
$$

Как нам проверить, будут ли случайные величины, составляющие случайный вектор
с абсолютно непрерывным распределением, независимыми?

\begin{theorem}
    Пусть $\xi = (\xi_1, \xi_2)$ — с абсолютно непрерывным распределением и 
    плотностью $p_\xi(x, y)$. Тогда независимость $\xi_1, \xi_2$ равносильна
    тому, что $p_\xi(x, y) = p_{\xi_1}(x)p_{\xi_2}(y)$.
\end{theorem}
\begin{proof}
    Проверим необходимость. Рассмотрим функцию распределения в произвольной точке:
    $F_{(\xi_1, \xi_2)}(x, y) = F_{\xi_1}(x)F_{\xi_2}(y)$. Дважды 
    продифференцировав это равенство, получим
    $$
    \frac{\dif^2 F_{(\xi_1, \xi_2)}(x, y)}{\dif x \dif y} = p_\xi(x, y)
    $$
    Левая часть равна $\frac{\dif^2 F_{\xi_1}(x)F_{\xi_2}(y)}{\dif x \dif y} =
    \frac{\dif F_{\xi_1}(x)}{\dif x}\frac{\dif F_{\xi_2}(y)}{\dif y} =
    p_{\xi_1}(x)p_{\xi_2}(y)$.
    
    Обратно. Для любых $x, y$
    $$
    \int\limits_{-\infty}^x\int\limits_{-\infty}^y p_\xi(s, t) \dif s \dif t =
    \int\limits_{-\infty}^x p_{\xi_1}(s) \dif s
    \int\limits_{-\infty}^y p_{\xi_2}(t) \dif t =
    F_{\xi_1}(x)F_{\xi_2}(y)
    $$
\end{proof}

\begin{theorem}\label{functions_of_independent_variables}
    Пусть $\xi, \eta$ — независимые случайные величины, $f, g$ — измеримые 
    функции. Тогда $f(\xi)$ и $g(\eta)$ — тоже независимые случайные величины.
\end{theorem}
\begin{proof}
    Рассмотрим $B_1, B_2 \in \mathfrak B$.
    $P(f(\xi) \in B_1, g(\eta) \in B_2) = P(\xi \in f^{-1}(B_1), \eta g^{-1}(B_2)$
    Но так как функции измеримы, то прообразы измеримых подмножеств будут также 
    измеримыми. По определению \ref{random_var_independence3} это равно
    $P(\xi \in f^{-1}(B_1)) P(\eta \in g^{-1}(B_2)) = P(f(\xi) \in B_1,
    g(\eta) \in B_2)$
\end{proof}

\section{Свёртка распределения}

Пусть имеется две независимые случайные величины $\xi, \eta$. Мы хотим узнать
распределение $\xi + \eta$, зная функции распределения $F_\xi$ и $F_\eta$.

Рассмотрим функцию распределения $\xi + \eta$ в произвольной точке $x$:
$$
F_{\xi + \eta}(x) = P(\xi + \eta < x) = P((\xi, \eta) \in A_x) = 
\iint\limits_{A_x} \dif F_{(\xi, \eta)}(s, t) =
\iint\limits_{A_x} \dif(F_\xi(s)F_\eta(t)) =
$$
$$
= \iint\limits_{A_x} \dif F_\xi(s)\dif F_\eta(t) =
\int\limits_{-\infty}^{+\infty}
\Bigg(\int\limits_{-\infty}^{x-s} \dif F_\eta(t)\Bigg) \dif F_\xi(s)
$$
Таким образом, доказана следующая теорема.
\begin{theorem}[О свёртке распределения]
    $$
    F_{\xi + \eta}(x) = \int\limits_{-\infty}^{+\infty} F_\eta(x-s)
    \dif F_\xi(s) =
    \int\limits_{-\infty}^{+\infty} F_\xi(x-t) \dif F_\eta(t)
    $$
\end{theorem}
\begin{corollary}
    Если $\xi, \eta$ — независимые случайные величины и $\xi$ — с абсолютно
    непрерывным распределением, то $\xi + \eta$ — тоже с абсолютно непрерывным
    распределением и, кроме того, $p_{\xi + \eta}(x) =
    \int\limits_{-\infty}^{+\infty} p_\xi(x - t) \dif F_\eta(t)$. Если же
    обе случайные величины $\xi, \eta$ с абсолютно непрерывным распределением,
    то $p_{\xi + \eta}(x) = \int\limits_{-\infty}^{+\infty}
    p_\xi(x-t)p_\eta(t)\dif t$.
\end{corollary}

\section{Числовые характеристики случайных величин. Математическое ожидание}

Как обычно, будем рассматривать вероятностное пространство
$(\Omega, \mathfrak{F}, P)$, случайная величина $\xi: \Omega \to \real$ с функцией
распределения $F_\xi(x( = P(\xi < x)$ и распределением $\mathfrak P_\xi(x) =
P(\xi^{-1}(A)) = P(\xi \in A)$.

\begin{definition}
    Математическим ожиданием случайной величины $\xi$ называется число
    $E\xi = \int\limits_\Omega \xi(\omega) \dif P(\omega)$, если таковое
    существует.
\end{definition}

Интеграл можно записать следующим образом:
$\int\limits_\Omega \xi^+ (\omega) \dif P(\omega) + \int\limits_\Omega
\xi^-(\omega)\dif P(\omega)$. Здесь:
$$
\xi^+ =
\begin{cases}
    \xi,\quad \xi \geqslant 0 \\
    0,\quad \xi < 0 \\
\end{cases}
\quad
\xi^- =
\begin{cases}
    0,\quad \xi \geqslant 0 \\
    \xi,\quad \xi < 0 \\
\end{cases}
$$

Тогда $|\xi| = \xi^+ - \xi^-$. Ясно, что $E\xi$ и $E|\xi|$ существуют 
одновременно.

\begin{remark}
    В литературе математическое ожидание также может обозначаться как $M\xi$.
\end{remark}

Для того, чтобы понять смысл этой величины, рассмотрим следующий пример.

\begin{example}
    Пусть $\xi$ — случайная величина с дискретным распределением и законом 
    распределения $(\xi, p): (a_1, p_1), (a_2, p_2), \ldots, (a_n, p_n)$.
    Для такой величины:
    $$
    E\xi = \int\limits_\Omega \xi(\omega) \dif P
    $$
    Рассмотрим события $A_k = (\xi = a_k)$, причём $A_k \cap A_l = \varnothing$ 
    при $k \neq l$ и $\bigcup\limits_k A_k = \Omega$. Тогда:
    $$
    E\xi = \sum\limits_k \int\limits_{A_k} \xi(\omega) \dif P =
    \sum\limits_k a_k \underbrace{\int\limits_{A_k} \dif P}_{P(\xi = a_k)} = \sum\limits_k a_kp_k
    $$
    То есть, математическое ожидание является средневзвешенным значением случайной
    величины.
    В частности, если $p_1 = p_2 = \ldots = p_n = \frac{1}{n}$, то
    $E\xi = \frac{a_1 + \ldots + a_n}{n}$.
\end{example}

\begin{proposition}[Свойства математического ожидания]
Пусть $\xi$, $\eta$ — случайные величины. Верно:
    \begin{enumerate}
        \item $P(\xi = c) = 1 \implies E\xi = c$
        \item $E(a\xi + n\eta) = aE\xi + bE\eta$, где $a,b \in \real$
        \item $P(\xi \geqslant 0) = 1 \implies E\xi \geqslant 0$
        \item $P(\xi \geqslant \eta) = 1 \implies E\xi \geqslant E\eta$
        \item $|E\xi| \leqslant E|\xi|$
    \end{enumerate}
\end{proposition}
\begin{proof}
\mbox{}
    \begin{enumerate}
        \item $E\xi = \int\limits_\Omega\xi(\omega)\dif P =
        c\int\limits_\Omega \dif P = c\cdot P(\Omega) = c$
        \item Очевидно из линейности интеграла.
        \item Очевидно.
        \item $P(\xi \geqslant \eta) = P(\xi - \eta \geqslant 0) = 1 \implies
        E(\xi - \eta) = E\xi - E\eta \geqslant 0$
        \item Так как $|\xi| \geqslant \xi$ и $|\xi| \geqslant - \xi$, то
        $E|\xi| \geqslant\xi$ и $E|\xi| \geqslant - E\xi$.
    \end{enumerate}
\end{proof}

Следующая теорема даёт правило вычисления математического ожидания случайных
величин.

\begin{theorem}
    Пусть $\xi_1, \xi_2, \ldots, \xi_n$ — случайные величины, определённые на
    одном вероятностном пространстве. Обозначим через $\xi$ случайные вектор
    $\xi = (\xi_1, \ldots, \xi_n)$. Рассмотрим также измеримую функцию
    $f: \real^n \to \real$. Тогда:
    $$
    Ef(\xi) = \int\limits_{\real^n} f(x_1, \ldots, x_n)
    \mathfrak P_\xi (\dif x_1, \ldots, \dif x_n) =
    \int\limits_{\real^n} f(x_1, \ldots, x_n) \dif F_\xi(x_1, \ldots, x_n)
    $$
\end{theorem}
\begin{corollary}
    Если $\xi$ — случайная величина с функцией распределения $F_\xi$,
    $f: \real^n \to \real$ — измеримая функция, то
    $$
    Ef(\xi) = \int\limits_\real f(x) \dif F_\xi(x)
    $$
    $$
    E\xi = \int\limits_\real x \dif F_\xi(x)
    $$
    Если известна плотность $p_\xi$, то $E\xi = \int\limits_\real xp_\xi(x)\dif x$
\end{corollary}

\begin{proof}[Доказательство теоремы]
    Рассмотрим два случая.
    \begin{enumerate}
        \item Функция $f$ принимает не более чем счётное число значений. Обозначим
        через $f_1, f_2, \ldots$ все значения функции $f$. Введём множества:
        
        $B_k = f^{-1}(f_k) \subset \real^n$, при этом $B_k \cap B_l = \varnothing$
        ($k \neq l$), $\bigcup\limits_k B_k = \real^n$;
        
        $A_k = \xi^{-1}(B_k) \in \mathfrak F$, $A_k \cap A_l = \varnothing$
        ($k \neq l$), $\bigcup\limits_k A_k = \Omega$.
        
        Рассмотрим математическое ожидание $Ef(\xi)$. По определению:
        $$
        Ef(\xi) = \int\limits_\Omega f(\xi) \dif P =
        \sum\limits_k \int\limits_{A_k} f(\xi) \dif P =
        \sum\limits_k \int\limits_{A_k} f_k \dif P =
        \sum\limits_k f_k \int\limits_{A_k} \dif P = \sum\limits_k f_k P(A_k) =
        $$
        $$
        = \sum\limits_k f_k \mathfrak P_\xi (B_k) =
        \sum\limits_k f_k \int\limits_{B_k}
        \mathfrak P_\xi (\dif x_1, \ldots, \dif x_n) =
        \sum\limits_k \int\limits_{B_k} f_k P_\xi (\dif x_1, \ldots, \dif x_n) =
        $$
        $$
        = \sum\limits_k \int\limits_{B_k} f(x_1, \ldots, x_n)
        \mathfrak P_\xi (\dif x_1, \ldots, \dif x_n) =
        \int\limits_{\real^n} f(x_1, \ldots, x_n)
        \mathfrak P_\xi (\dif x_1, \ldots, \dif x_n)
        $$
        \item Функция $f$ — произвольная.
        Возьмём $m \geqslant 1$. Определим $f^m$ следующим образом:
        $$
        f^m(x_1, \ldots, x_n) = \frac{k}{m} \text{, если }
        \frac{k}{m} \leqslant f(x_1, \ldots, x_n) \leqslant \frac{k+1}{m}
        $$
        То есть, мы будем приближать функцию $f$ ступенчатой функцией:
        $$
        \big|f(x_1, \ldots, x_n) - f^m(x_1, \ldots, x_n)\big| < \frac{1}{m}
        $$
        Отсюда:
        $$
        \big|Ef(\xi) - Ef^m(\xi)\big| = \big|E(f(\xi) - f^m(\xi))\big| \leqslant
        E\big|f(\xi) - f^m(\xi)\big| \leqslant \frac{1}{m}
        $$
        $$
        \big|\int\limits_{\real^n} f(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n) -
        \int\limits_{\real^n} f^m(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\big| \leqslant \frac{1}{m}
        $$
        Рассмотрим модуль:
        $$
        \big| Ef(\xi) - \int\limits_{\real^n} f(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\big|
        $$
        Прибавим и вычтем в выражении под знаком модуля
        $$
        \int\limits_{\real^n} f^m(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)
        $$
        Получим:
        $$
        \big| Ef(\xi) - \int\limits_{\real^n} f(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\big| \leqslant
        $$
        $$
        \leqslant\big| Ef(\xi) - \int\limits_{\real^n} f^m(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\big| +
        $$
        $$
        +\big|f^m(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n) -
        f(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\big| \leqslant
        \frac{1}{m} + \frac{1}{m} = \frac{2}{m}
        $$
        Поскольку результат не зависит от $m$, то исходная разность равна нулю
        и теорема доказана.
    \end{enumerate}
\end{proof}

\begin{theorem}[Следствие для произведения независимых случайных величин]
    Пусть $\xi, \eta$ — независимые случайные величины и, кроме того,
    существуют $E\xi$ и $E\eta$. Тогда $E(\xi\eta) = E\xi \cdot E\eta$.
\end{theorem}
\begin{proof}
    Рассмотрим случайные вектор $(\xi, \eta)$.
    $$
    F_{(\xi, \eta)}(x, y) = F_\xi(x)F_\eta(y)
    $$
    $$
    \mathfrak P_{(\xi, \eta)} = \mathfrak P_\xi \times \mathfrak P_\eta
    $$
    Возьмём функцию $f: \real^2 \to \real$: $f(x, y) = xy$.
    $$
    Ef(\xi, \eta) = E(\xi\eta) = \iint\limits_{\real^2} x_1x_2
    \mathfrak P_{(\xi, \eta)}(\dif x_1, \dif x_2) =
    \iint\limits_{\real^2} x_1x_2 \mathfrak P_\xi(\dif x_1)
    \mathfrak P_\eta(\dif x_2) =
    $$
    $$
    = \int\limits_{\real} x_1\mathfrak P_\xi(\dif x_1) \cdot
    \int\limits_{\real} x_2\mathfrak P_\eta(\dif x_2) =
    E\xi \cdot E\eta
    $$
\end{proof}

\section{Дисперсия}

\begin{definition}
    Дисперсией случайной величины $\xi$ называется число
    $D\xi = E(\xi - E\xi)^2$, если оно существует.
\end{definition}

Дисперсия представляет собой среднеквадратичное отклонение случайной величины
от своего среднего значения.

\begin{proposition}[Свойства дисперсии]
    \mbox{}
    \begin{enumerate}
        \item $D\xi \geqslant 0$. $D\xi = 0 \iff  \exists c:$ $P(\xi = c) = 1$
        \item Если $a, b \in \real$, то $D(a\xi + b) = a^2D\xi$
        \item Если $\xi, \eta$ — случайные величины, то $D(\xi + \eta) =
        D\xi + D\eta + 2E((\xi - E\xi)(\eta - E\eta))$.
        \item Если $\xi, \eta$ — независимые случайные величины, то
        $D(\xi+ \eta) = D\xi + D\eta$. Обратное, вообще говоря, неверно.
        \item $D\xi = E\xi^2 - (E\xi)^2$
    \end{enumerate}
\end{proposition}
\begin{definition}
    Величина $E((\xi - E\xi)(\eta - E\eta))$ называется \emph{ковариацией}
    случайных величин $\xi$ и $\eta$. Обозначение: $\cov(\xi, \eta)$.
\end{definition}

\begin{proof}[Доказательство предложения]
\mbox{}
    \begin{enumerate}
        \item Неотрицательность следует из свойств математического ожидания.
        Далее, $D\xi = \int\limits_\Omega(\xi(\omega) - E\xi)^2\dif P(\omega) =
        0$. Это означает, что $P(\xi - E\xi = 0) = 1$, возьмём $c = E\xi$.
        \item $D(a\xi + b) = E(a\xi + b - E(a\xi + b))^2 =
        E(a\xi + b - aE\xi - b)^2 = a^2E(\xi - E\xi)^2 = a^2D\xi$.
        \item $D(\xi + \eta) = E(\xi + \eta - E\xi - E\eta)^2 = E(\xi - E\xi)^2 +
        E(\eta - E\eta)^2 + 2E((\xi - E\xi)(\eta - E\eta))$
        \item По теореме \ref{functions_of_independent_variables} $\xi - E\xi$ и
        $\eta - E\eta$ будут независимыми, отсюда
        $E((\xi - E\xi)(\eta - E\eta)) = E(\xi - E\xi)\cdot E(\eta - E\eta) = 0$.
        Подставим полученный результат в предыдущее свойство.
        \item $D\xi = E(\xi - E\xi) \ldots$
    \end{enumerate}
\end{proof}

\begin{examples}
    \begin{enumerate}
        \item $(\xi, p): (c, 1)$ (вырожденное распределение). Тогда $E\xi = c$,
        $D\xi = 0$.
        \item Если $(\xi, p): (0, 1-p), (1-p, p)$, то $E\xi = p$, $D\xi = p(1-p)$.
        \item $\xi \in B(n, p)$ — случайная величина с биномиальным законом
        распределения, т. е. $p(\xi = k) = C_n^kp^k(q-p)^{n-k}$. Для этой величины
        $E\xi = \sum\limits_{k=0}^n k\cdot C_n^kp^k(1-p)^{n-k} =
        \sum\limits_{k=1}^n k\cdot C_n^kp^k(1-p)^{n-k} =
        \sum\limits_{k=1}^n \frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k} =
        np\sum\limits_{k=1}^n \frac{(n-1)!}{(k-1)!(n-1-(k-1))!}
        p^{k-1}(1-p)^{n-1-(k-1)}$. Взяв $l = k-1$, получим
        $E\xi = np\sum\limits_{l=0}^{n-1} C_n^l p^l(1-p)^{n-1 - l} = np$.
        $D\xi = np(1-p)$.
        \item Рассмотрим $\xi \in \Pi(\lambda)$ — случайную величину с 
        пуассоновским законом распределения: $P(\xi = k) =
        \frac{\lambda^k}{k!}e^{-\lambda}$.
        $E\xi = \sum\limits_{k=0}^\infty k\frac{\lambda^k}{k!}e^{-\lambda} =
        \sum\limits_{k=1}^\infty \frac{\lambda^k}{(k-1)!}e^{-\lambda} =
        \lambda e^{-\lambda}\sum\limits_{k=1}^\infty
        \frac{\lambda^{k-1}}{(k-1)!} =
        \lambda$. Для вычисления дисперсии вычислим сначала
        $E\xi^2 = \sum\limits_{k=0}^\infty k^2\frac{\lambda^k}{k!}e^{-\lambda} =
        \sum\limits_{k=1}^\infty k\frac{\lambda^k}{(k-1)!}e^{-\lambda} =
        \sum\limits_{k=1}^\infty (k-1)\frac{\lambda^k}{(k-1)!}e^{-\lambda} +
        \sum\limits_{k=1}^\infty\frac{\lambda^k}{(k-1)!}e^{-\lambda} =
        \sum\limits_{k=2}^\infty \frac{\lambda^k}{(k-2)!}e^{-\lambda} +
        \lambda e^{-\lambda}
        \sum\limits_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!} =
        \lambda^2 e^{-\lambda}
        \sum\limits_{k=2}^\infty\frac{\lambda^{k-2}}{(k-2)!}$
        
        $\cdots$
        \item Для $\xi \in U_{[a,b]}$ математическое ожидание $E\xi =
        \int\limits_a^b x\frac{1}{b-a}\dif x = \frac{a + b}{2}$. $E\xi^2 =
        \int\limits_a^b x^2\frac{1}{b-a}\dif x = \frac{b^3 - a^3}{3(b-a)}$.
        Тогда $D\xi = E\xi^2 - (E\xi)^2 = \frac{(b-a)^2}{12}$.
        \item $\xi \in N(a, \sigma^2)$ — случайная величина с нормальным
        законом распределения ($p_\xi(x) =
        \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-a)^2}{2\sigma^2}}$).
        $$
        E\xi = \int\limits_{-\infty}^{+\infty}x\frac{1}{\sqrt{2\pi\sigma^2}}
        e^{-\frac{(x-a)^2}{2\sigma^2}}\dif x =
        \int\limits_{-\infty}^{+\infty}
        (\sigma y - a)\frac{1}{\sqrt{2\pi\sigma^2}}
        e^{-\frac{y^2}{2}}\sigma\dif y =
        $$
        $$
        =\underbrace{\sigma \int\limits_{-\infty}^{+\infty}
        y\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}} \dif y}_0 +
        a\int\limits_{-\infty}^{+\infty}
        \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}} \dif y = a
        $$
        $$
        D\xi = \int\limits_{-\infty}^{+\infty}
        (x-a)^2\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-a)^2}{2\sigma^2}} \dif x
        = \int\limits_{-\infty}^{+\infty}
        \sigma^2 y^2 \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{y^2}{2}}\sigma \dif y
        = \sigma^2 \int\limits_{-\infty}^{+\infty}
        y^2 \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}\dif y =
        $$
        $$
        = \sigma^2 \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{+\infty}
        y\cdot ye^{-\frac{y^2}{2}}\dif y = \sigma^2 \frac{1}{\sqrt{2\pi}}
        \bigg[ -ye^{-\frac{y^2}{2}}\bigg|_{-\infty}^{+\infty} +
        \int\limits_{-\infty}^{+\infty}e^{-\frac{y^2}{2}}\dif y\bigg] = \sigma^2
        $$
        \item $\xi$ — случайная величина с распределением Коши
        ($p_\xi(x) = \frac{1}{\pi(1 + x^2)}$).
        $\int\limits_{-\infty}^{+\infty} \frac{x}{\pi(1 + x^2)}\dif x$ — этот 
        интеграл расходится, значит, математического ожидания не существует.
    \end{enumerate}
\end{examples}

\section{Моменты случайных величин}

\begin{definition}
    Пусть $k$ — целое неотрицательное число, $\xi$ — случайная величина.
    Число $E\xi^k$, если оно существует, называется $k$-м моментом $\xi$.
    Число $E|\xi|^k$ — абсолютный $k$-й момент $\xi$.
    Число $E(\xi - E\xi)^k$ — центральный $k$-й момент $\xi$.
    Число $E|\xi - E\xi|^2$ — абсолютный центральный $k$-й момент $\xi$.
\end{definition}

Заметим, что все моменты величины $\xi$ существуют или отсутствуют одновременно.

\begin{proposition}[Свойства моментов]
\mbox{}
    \begin{enumerate}
        \item Пусть $n > m$ и существует $E\xi^n$. Тогда существует и $E\xi^m$.
        \item (Неравенство Гёльдера) Если $p, q > 0$ и
        $\frac{1}{p} + \frac{1}{q} = 1$, то $E|\xi\eta| \leqslant
        (E|\xi|^p)^\frac{1}{p} \cdot (E|\eta|^q)^\frac{1}{q}$.
        \item (Неравенство Коши-Буняковского-Шварца)
        $E|\xi\eta| \leqslant \sqrt{E\xi^2} \cdot \sqrt{E\eta^2}$.
        \item (Неравенство Ляпунова) Пусть $0 < p < q$. Тогда
        $(E|\xi|^p)^\frac{1}{p} \leqslant (E|\xi|^q)^\frac{1}{q}$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}
        \item Ясно, что $|x|^m \leqslant |x|^n + 1$ для любого $x$. То есть,
        $|\xi|^m \leqslant |\xi|^n + 1$ и
        $E|\xi|^m \leqslant E|\xi|^n + 1 < \infty$.
        \item Очевидно из неравенства Гёльдера для интегралов.
        \item Это свойство — частный случай предыдущего при $p = q = \frac{1}{2}$.
        \item Данное неравенство равносильно
        $E|\xi|^p \leqslant (E|\xi|^q)^\frac{p}{q}$. С учётом
        $\frac{p}{q} + \frac{q - p}{q} = 1$, применим неравенство Гёльдера:
        $$
        E(\xi|^p \cdot 1|) \leqslant (E|\xi|^q)^\frac{p}{q}\cdot
        (E (1^\frac{q}{q - p}))^\frac{q - p}{q} = (E|\xi|^q)^\frac{p}{q}
        $$
    \end{enumerate}
\end{proof}

\begin{definition}
    Пусть $m,n > 0$. Смешанным моментом величин $\xi$, $\eta$ порядка $m$ плюс $n$ 
    называется число $E\xi^m\eta^n$, если оно существует. Смешанным центральным 
    моментом $\xi$, $\eta$ порядка $m$ плюс $n$ называется число
    $E(\xi - E\xi)^m(\eta - E\eta)^n$.
\end{definition}
\begin{remark}
    Таким образом, $\cov (\xi, \eta)$ является смешанным центральным моментом
    величин порядка $1$ плюс $1$.
\end{remark}

\end{document}
