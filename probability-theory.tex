\documentclass[11pt,openany,a4paper]{scrartcl}

\usepackage{indentfirst}
\usepackage{amsmath,amsthm,amssymb,amsfonts,amsopn}
\usepackage{mathtext}
\usepackage{enumitem}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[intlimits]{mathtools}
\usepackage[makeroom]{cancel}
\usepackage[colorlinks=false,pagebackref=true]{hyperref}
\usepackage{titletoc}
\renewcommand{\bfdefault}{sbc}
\renewcommand\thesubsection{\arabic{subsection}}
\usepackage{ccfonts,eulervm,microtype}
\usepackage{enumitem}
\usepackage{extarrows}
\usepackage[portrait,a4paper,margin=2.5cm,headsep=5mm]{geometry}

\usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,calc}

\author{С. М. Ананьевский \thanks{Конспект подготовлен студентом Яскевичем С. В.}}
\title{Теория вероятностей и математическая статистика}

\theoremstyle{plain}
\newtheorem{theorem}{Теорема}[subsection]
\newtheorem{corollary}[theorem]{Следствие}
\newtheorem{proposition}[theorem]{Предложение}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{exercise}[theorem]{Упражнение}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Определение}
\newtheorem{remark}[theorem]{Замечание}
\newtheorem{example}[theorem]{Пример}
\newtheorem{examples}[theorem]{Примеры}
\newtheorem{num}[theorem]{}

\newcommand\mb{\mathbb}
\newcommand\real{\mb R}
\newcommand{\complex}{\mb C}
\newcommand\eqdef{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}
\newcommand{\undereq}[1]{\xlongequal{#1}}
\newcommand\lparagraph[1]{\paragraph{#1}\mbox{}\\}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\uto}{\rightrightarrows}
\newcommand{\underto}[1]{\xrightarrow[#1]{}}
\newcommand{\overto}[1]{\xrightarrow{#1}}
\newcommand{\dif}{\, \mathrm d}
\newcommand{\distr}{\mathfrak P_\xi}
\newcommand{\funcdistr}{F_\xi}
\newcommand{\ol}{\overline}

\DeclareMathOperator{\Ree}{Re}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Arg}{Arg}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\const}{const}
\DeclareMathOperator{\Ln}{Ln}
\DeclareMathOperator{\cov}{cov}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\subsection{Случайные события. Вероятность}

Рассмотрим эксперимент с подбрасыванием монеты. Мы не можем угадать следующий исход, но легко можем
перечислить все исходы: это выпадение <<орла>> ($O$) или <<решки>> ($Р$). В случае с подбрасыванием
кубика это будут числа от $1$ до $6$; для двух кубиков это упорядоченные пары таких чисел. Во
всех этих случаях мы имеем дело с конечным числом исходов.
Мы теперь можем рассмотреть эксперимент с подбрасыванием монеты, который прекращается как только
выпадает <<орёл>>. Множество исходов будет иметь вид: $\{О, РО, РРО, \ldots, Р\ldotsРО, \ldots\}$.
Ясно, что это множество счётно. Примером несчётного множества исходов служит выбор случайной точки
на отрезке $[0, 1]$.

Условимся обозначать $\Omega$ множество всех исходов, а $\omega$ — исход эксперимента, или
\emph{элементарное событие}.

Пусть на $\Omega$ определена система подмножеств $\mathfrak F$, являющаяся сигма-алгеброй. Элемент
$A \in \mathfrak F$ назовём \emph{случайным событием}. Будем говорить, что случайное событие
произошло, если $\omega \in A$.

\begin{remark}
    Обычно в случае конечного или счётного числа исходов в качестве $\mathfrak F$ выбирается 
    множество всех подмножеств $\Omega$.
\end{remark}

\begin{definition}
    Пусть $P$ — функция, заданная на $\mathfrak F$ и действующая в $\real$, а также выполнены 
    условия:
    \begin{enumerate}
        \item $\forall A \in \mathfrak F \quad P(A) \geqslant 0$.
        \item $P(\Omega) = 1$.
        \item Для $A_1, A_2, A_3, \ldots \in \mathfrak F$ — не более чем счётного набора
        событий, причём попарно непересекающихся, выполнено
        $P\Big(\bigcup\limits_i A_i\Big) = \sum\limits_i P(A_i)$.
    \end{enumerate}
    Тогда $P$ называется \emph{вероятностью}, а указанные условия — \emph{аксиомами вероятности}.
\end{definition}

\begin{remark}
    Вероятность является счётно-аддитивной мерой на $\mathfrak F$.
\end{remark}

\begin{example}
    Вспомним подбрасывание монетки: $\Omega = \{О, Р\}$,
    $\mathfrak F = \{\{О\}, \{Р\}, \{О, Р\}, \varnothing\}$. Мы можем сопоставить каждому элементу
    $\mathfrak F$ число разными способами:
    $$
    \begin{cases}
        \{О\} \mapsto \frac{1}{2} \\
        \{Р\} \mapsto \frac{1}{2} \\
        \{О, Р\} \mapsto 1        \\
        \varnothing \mapsto 0     \\
    \end{cases}
    \text{ или }
    \begin{cases}
        \{О\} \mapsto \frac{1}{3} \\
        \{Р\} \mapsto \frac{2}{3} \\
        \{О, Р\} \mapsto 1        \\
        \varnothing \mapsto 0     \\
    \end{cases}
    $$
    Разумеется, то какая вероятность будет правильной, зависит от монетки.
\end{example}

\begin{definition}
    Пусть $A$ — случайное событие. Событие $\ol A = \Omega \backslash A$ — называется
    \emph{обратным} к $A$.
\end{definition}
\begin{definition}
    Если $A, B \in \mathfrak F$ — случайные события и $A \cap B = \varnothing$, то $A, B$
    называются \emph{несовместными}. Это означает, что они не могут произойти одновременно.
\end{definition}
\begin{definition}
    $\Omega$ называется \emph{достоверным событием}, а
    $\ol \Omega = \varnothing$ — \emph{невозможным событием}.
\end{definition}

\begin{definition}
    Тройка $(\Omega, \mathfrak F, P)$ называется \emph{вероятностным пространством}.
\end{definition}

Вероятностное пространство представляет собой математическую модель, с помощью которой мы
описываем эксперимент.

\begin{definition}
    Пусть имеется не более чем счётный набор попарно непересекающихся событий
    $A_1, A_2, A_3, \ldots \in \mathfrak F$, $A_i \cap A_j = \varnothing$ ($i \neq j$),
    $\bigcup\limits_i A_i = \Omega$. Тогда этот набор называется \emph{полной системой событий}.
    То есть, полная система событий образует разбиение $\Omega$.
\end{definition}
\begin{definition}
    Если $A, B \in \mathfrak F$, то $A \cup B$ называется \emph{суммой событий}, а
    $A \cap B = AB$ — \emph{произведением событий}.
\end{definition}

\subsection{Свойства вероятности}

\begin{proposition}
    Любая вероятность $P: \mathfrak F \to \real$ удовлетворяет следующим свойствам:
    \begin{enumerate}
        \item $\forall A \in \mathfrak F\quad P(\ol A) = 1 - P(A)$.
        \item $\forall A \in F \quad 0 \leqslant P(A) \leqslant 1$.
        \item $P(\varnothing) = 0$.
        \item Если $A, B \in \mathfrak F$, то $P(A \cup B) = P(A) + P(B) - P(AB)$.
        \item Для конечного набора $A_1, A_2, \ldots, A_m \in \mathfrak F$ верно:
        $$
        P\bigg(\bigcup\limits_{i=1}^m A_i\bigg) = \sum\limits_{i=1}^m P(A_i) -
        \sum\limits_{i \neq j} P(A_i \cap A_j) +
        \sum\limits_{i \neq j \neq k} P(A_i \cap A_j \cap A_k) - \ldots +
        (-1)^{m+1} P\bigg(\bigcap\limits_{i=1}^m A_i\bigg)
        $$
        \item Если $A, B \in \mathfrak F$ и $A \subset B$, то $P(A) \leqslant P(B)$.
        \item Если $A_1, A_2, A_3 \ldots \in \mathfrak F$, то
        $P\Big(\bigcup\limits_i A_i\Big) \leqslant \sum\limits_i P(A_i)$.
        \item Если $A_1, A_2, A_3, \ldots \in \mathfrak F$ — не более чем счётный набор событий,
        причём $A_1 \subset A_2 \subset A_3 \subset \ldots$, и $\bigcup\limits_i A_i = A$,
        то $P(A) = \lim\limits_{n \to \infty} P(A_n)$.
        \item Если $B_1, B_2, B_3, \ldots \in \mathfrak F$ — не более чем счётный набор событий,
        причём $B_1 \supset B_2 \supset B_3 \supset \ldots$, и $\bigcap\limits_i B_i = B$,
        то $P(B) = \lim\limits_{n \to \infty} P(B_n)$.
    \end{enumerate}
    
    Последние два свойства утверждают о непрерывности вероятности.
\end{proposition}
\begin{proof}
\mbox{}
    \begin{enumerate}
        \item Заметим, что $A$ и $\ol A$ несовместны и $A \cup \ol A = \Omega$. По второй аксиоме
        вероятности $P(\Omega) = 1 = P(A \cup \ol A)$, откуда по третьей аксиоме
        $1 = P(A) + P(\ol A)$.
        \item $P(\ol A) = 1 - P(A) \geqslant 0$ — из первой аксиомы.
        \item Поскольку $\ol \Omega = \varnothing$, то $P(\varnothing) = 1 - P(\Omega) = 1-1 = 0$.
        \item Рассмотрим $A\cup B = (A \cap \ol B) \cup (\ol A \cap B) \cup (A \cap B)$. Легко
        видеть, что слагаемые попарно несовместны. Отсюда:
        $$
        P(A\cup B) = P(A\cap \ol B) + P(\ol A \cap B) + P(A\cap B) + P(A\cap B) - P(A\cap B)
        $$
        Здесь мы прибавили и вычли $P(A\cap B)$. Так как $A \cap \ol B$ и $A \cap B$ несовместны,
        то по третьей аксиоме будет $P(A\cup B) = P(A) + P(B) - P(A\cap B)$.
        \item \emph{Упражнение}.
        \item Разложим $B$ как $B = A \cup (B \cap \ol A)$.
        Заметим, что $A$ и $B \cap \ol A$ несовместны. Тогда
        $P(B) = P(A) + P(B\cap \ol A) \geqslant P(A)$ (так как $P(B\cap \ol A) \geqslant 0$).
        \item $\bigcup\limits_i A_i = A_1 \cup (\ol A_1 \cap A_2) \cup
        (\ol A_1 \cap \ol A_2 \cap A_3) \cup \ldots$ — сумма попарно несовместных событий.
        Отсюда:
        $$
        P\bigg(\bigcup\limits_i A_i\bigg) =
        P(A_1) + P(\underbrace{\ol A_1 \cap A_2}_{\subset A_2}) +
        P(\underbrace{\ol A_1 \cap \ol A_2 \cap A_3}_{\subset A_3}) + \ldots
        $$
        По предыдущему свойству получаем $P\Big(\bigcup\limits_i A_i\Big) \leqslant
        P(A_1) + P(A_2) + P(A_3) + \ldots$
        \item Рассмотрим $A = \bigcup\limits_{i=1}^n A_i \cup \bigcup\limits_{i > n} A_i =
        A_n \cup \bigcup\limits_{i > n}A_i =
        A_n \cup (\ol A_n \cap A_{n+1}) \cup (\ol A_{n+1} \cap A_{n+1}) \cup \ldots$
        Отсюда $P(A) = P(A_n) + \sum\limits_{k=n}^\infty P(\ol A_k \cap A_{k+1})$. Так как
        $P(A) \leqslant 1$, то можно утверждать, что ряд сходится и
        $\sum\limits_{k=n}^\infty P(\ol A_k \cap A_{k+1}) \underto{n \to \infty} 0$.
        \item Рассмотрим $\ol B_1 \subset \ol B_2 \subset \ol B_3 \subset \ldots$ Заметим, что
        $\ol B = \bigcup\limits_i \ol B_i$. По предыдущему свойству
        $P(\ol B) = \lim\limits_{n \to \infty} P(\ol B_n)$ и
        $1 - P(B) = \lim\limits_{n \to \infty} (1 - P(B_n)) =
        1 - \lim\limits_{n \to \infty} P(B_n)$.
    \end{enumerate}
\end{proof}

\subsection{Классическое определение вероятности. Схема равновозможных исходов}

Рассмотрим вероятностное пространство некоторого эксперимента: $(\Omega, \mathfrak F, P)$.
Пусть $\Omega = \{\omega_1, \ldots, \omega_n\}$. Введём следующее предположение:
$P(\{\omega_1\}) = \ldots = P(\{\omega_n\})$. Такую ситуацию будем называть
\emph{схемой равновозможных исходов}. Тогда:
$$
1 = P(\Omega) = P\bigg(\bigcup\limits_{i=1}^n \{\omega_i\}\bigg) =
\sum\limits_{i=1}^n P(\{\omega_i\}) = n \cdot P(\{\omega_j\})\quad \forall j=1\ldots n
$$
Мы воспользовались тем, что элементарные события, конечно, попарно несовместны,
и получили, что $P(\{\omega_j\}) = \frac{1}{n}$.

Рассмотрим теперь событие $A \in \mathfrak F$, $A = \{\omega_{i_1}, \ldots, \omega_{i_k}\}$,
$k \leqslant n$. Тогда:
$$
P(A) = \sum\limits_{j=1}^k P(\{\omega_{i_j}\}) = \frac{k}{n}
$$

Таким образом мы получили формулу классического определения вероятности:
$$
P(A) = \frac{\text{число исходов, благоприятствующих событию } A}
{\text{число всех возможных исходов}}
$$

\begin{example}
    Представим, что мы одновременно подбрасываем два кубика, которые будем считать честными:
    $\Omega = \{ (i, j)\,|\, 1 \leqslant i,j \leqslant 6\}$. Рассмотрим событие $A$:
    $$
    A = \{\text{сумма выпавших очков больше или равна }10\}
    $$
    $$
    P(A) = \frac{6}{36} = \frac{1}{6}
    $$
\end{example}
\begin{remark}
    События $\{\text{сумма очков равна } 2\}$ и $\{\text{сумма очков равна } 3\}$ не равновероятны.
\end{remark}

\subsection{Условные вероятности}

Пусть у нас есть вероятностное пространство $(\Omega, \mathfrak{F}, P)$ и два случайных 
события $A,B \in \mathfrak{F}$, причём будем считать, что $P(B) \neq 0$.

\begin{definition}
    \emph{Условной вероятностью события $A$ при условии события $B$} называется число
    $P(A/B) = \frac{P(A \cap B)}{P(B)}$ (иногда обозначается $P_B(A)$).
\end{definition}
\begin{example}
    Если у нас есть игральный кубик, то вероятность выпадения нечётной грани при 
    условии, что количество очков не превосходит $3$, равна $\frac{2}{3}$.
\end{example}

\begin{theorem}[Свойства условной вероятности]
    Условная вероятность является вероятностью.
\end{theorem}
\begin{proof}
    Проверим аксиомы вероятности:
    \begin{enumerate}
        \item $P_B(A) =  \frac{P(A \cap B)}{P(B)}$. Так как числитель и знаменатель 
        неотрицательны, то и дробь неотрицательна.
        \item $P_B(\Omega) = \frac{P(\Omega \cap B)}{P(B)} =  \frac{P(B)}{P(B)} = 1$
        \item Пусть $A_1, A_2 \ldots, \in \mathfrak{F}$;
        $A_i \cap A_j = \varnothing$ (при $i \neq j$). Тогда:
        $$
        P_B\bigg(\bigcup\limits_i A_i\bigg) =
        \frac{P\Big(\bigcup\limits_i B\cap A_i\Big)}{P(B)} =
        \frac{\sum\limits_i P(B \cap A_i)}{P(B)} =
        $$
        $$
        = \sum\limits_i \frac{P(B\cap A_i)}{P(B)} = \sum\limits_i P_B(A_i)
        $$
    \end{enumerate}
\end{proof}
\begin{corollary}
    Все свойства вероятности для условной вероятности выполнены.
\end{corollary}

\subsection{Формула полной вероятности. Формула Байеса}
\begin{theorem}[Формула полной вероятности]
    Пусть $A_1, A_2 \ldots$ — полная система событий и $\forall i$ $P(A_i) > 0$. Тогда
    вероятность любого случайного события $B \in \mathfrak{F}$ можно вычислить
    по формуле:
    $$
    P(B) = \sum_i P(B/A_i) \cdot P(A_i)
    $$
\end{theorem}
\begin{proof}
    $$
    P(B) = P(B \cap \Omega) = P\Bigg(B \cap \bigg(\bigcup_i A_i\bigg)\Bigg) =
    P\bigg(\bigcup_i (B \cap A_i)\bigg) =
    \sum_i P(B \cap A_i) = \sum_i\frac{P(B \cap A_i) \cdot P(A_i)}{P(A_i)} =
    $$
    $$
    = \sum_i P(B/A_i) \cdot P(A_i)
    $$
\end{proof}

\begin{theorem}[Формула Байеса]
    Пусть $A_1, A_2 \ldots$ — полная система событий, $\forall i$ $P(A_i) > 0$, 
    $P(B) > 0$. Тогда $\forall k \geqslant 1$ $P(A_k/B) =
    \frac{P(B/A_k) \cdot P(A_k)}{\sum\limits_i P(B/A_i) \cdot P(A_i)}$
\end{theorem}
\begin{proof}
    Правая часть равна:
    $$
    \frac{P(B/A_k) \cdot P(A_k)}{P(B)} =
    \frac{P(B \cap A_k) \cdot P(A_k)}{P(A_k) \cdot P(B)} =
    \frac{P(B \cap A_k)}{P(B)} = P(A_k/B)
    $$
\end{proof}
\begin{examples}
    \begin{enumerate}
        \item Пусть с завода №1 поставлено 5 ящиков деталей, с завода №2 — 3 ящика,
        а с завода №3 — 2 ящика. Предположим также, что завод №1 допускает 2\% брака,
        завод №2 — 5\% брака, а завод №3 — 10\%. Какова вероятность выбрать хорошую
        деталь? Какова вероятность того, что деталь изготовлена на заводе №1 при
        условии, что она хорошая?
        
        Ящики считаем одинаковыми. Рассмотрим события $C_1, C_2, C_3$, где $C_i$
        означает выбрать ящик с завода №i, и событие $B$, означающее выбор хорошей 
        детали. Ясно, что $C1, C_2, C_3$ — полная система событий. Чтобы вычислить 
        вероятность события $B$, можно воспользоваться формулой полной вероятности:
        $$
        P(B) = \sum_{i = 1}^3 P(B/C_i) \cdot P(C_i) = 
        0.98 \cdot 0.5 + 0.95 \cdot 0.3 + 0.9 \cdot 0.2 = 0.955
        $$
        
        Чтобы ответить на второй вопрос, мы можем воспользоваться формулой Байеса:
        $$
        P(C_1/B) = \frac{0.98 \cdot 0.5}
        {0.98 \cdot 0.5 + 0.95 \cdot 0.3 + 0.9 \cdot 0.2} \approx 0.51
        $$
        
        \item Представим, что у нас имеется ящик с шестью белыми и четырьмя чёрными 
        шариками. Сначала мы потеряли один шарик из этого ящика (какой — неизвестно),
        а затем из оставшихся мы вытащили два шарика. Какова вероятность вытащить
        два белых шарика? Какова вероятность того, что был потерян чёрный шар, при
        условии, что мы вытащили два белых шара?
        
        Введём два события, описывающие первый этап эксперимента: $C_{б}, C_{ч}$ — потеря
        белого и чёрного шаров соответственно. $C_{б}, C_{ч}$ — полная система событий.
        Пусть $B$ означает <<вытащить два белых шарика>>.
        $$
        P(B) = P(B/C_{б}) \cdot P(C_{б}) + P(B/C_{ч}) \cdot P(C_{ч}) =
        \frac{C_5^2}{C_9^2} \cdot \frac{6}{10} + \frac{C_6^2}{C_9^2} \cdot \frac{4}{10}
        $$
        $$
        P(C_{ч}/B) = \frac{C_6^2 \cdot 4}{C_5^2 \cdot 6 + C_6^2 \cdot 4}
        $$
    \end{enumerate}
\end{examples}

\subsection{Независимые события. Пример Бернштейна}

Важно: нельзя путать понятия \emph{независимости} событий и \emph{несовместности}.

Пусть имеется эксперимент, описываемый с помощью вероятностного пространства
$(\Omega, \mathfrak{F}, P)$, и даны случайные события $A, B \in \mathfrak{F}$.

Независимость событий можно было бы рассматривать как выполнение равенств
$$
P(A/B) = P(A/\overline{B}) = P(A).
$$
Однако здесь нарушена симметрия - логично, что если событие $A$ независимо от $B$, то
и обратное тоже верно — $B$ независимо от $A$.

\begin{definition}
    $A$ и $B$ независимы, если $P(A\cap B) = P(A)P(B)$.
\end{definition}

Таким образом, если $P(B) > 0$, то независимость $A$ и $B$ равносильна
$P(A/B) = P(A)$.
\begin{proposition}[Свойства независимых событий]
    \begin{enumerate}
        \item $A$, $B$ независимы $\iff$ $A$, $\overline{B}$ независимы
        $\iff$ $\overline{A}$, $B$ независимы $\iff$ $\overline{A}$, $\overline{B}$ 
        независимы.
        \item $\forall A \in \mathfrak{F}$ $A$ и $\Omega$ независимы.
        \item $\forall A \in \mathfrak{F}$ $A$ и $\varnothing$ независимы.
    \end{enumerate}
\end{proposition}
\begin{exercise}
    Пусть $A$ и $B$ независимы, $A$ и $C$ независимы. Верно ли, что $A$ и $B \cup C$
    независимы? Верно ли, что $A$ и $B \cap C$ независимы?
\end{exercise}

Можем ли мы определить понятие независимости для числа событий, большего $2$?
Для событий $A_1, A_2, \ldots, A_n$ мы можем выделить попарную независимость:
$$
\forall i\neq j\quad A_i, A_j \text{ независимы.}
$$
Или же независимость в совокупности (совместную): $A_1, \ldots, A_n$ независимы в
совокупности, если:
\begin{enumerate}
    \item $\forall i\neq j$ $A_i, A_j$ — независимы;
    \item $\forall i_1<i_2<i_3$ $P\Big(\bigcap\limits_{j=1}^3 A_{i_j}\Big) =
    \prod\limits_{j=1}^3P(A_{i_j})$, и так далее;
    \item $P\Big(\bigcap\limits_{j=1}^n A_{i_j}\Big) = \prod\limits_{j=1}^n P(A_{i_j})$.
\end{enumerate}
Это равносильно:
$$
\forall\, 2 \leqslant k \leqslant n \quad \forall i_1 < i_2 < \ldots < i_k\quad
P\bigg(\bigcap_{j=1}^k A_{i_j}\bigg) = \prod_{j=1}^k P(A_{i_j})
$$

\begin{example}[Берштейна]
    Рассмотрим эксперимент: будем подбрасывать тетраэдр с белой, синей, красной и 
    разноцветной (бело-сине-красной) гранями. Введём три события:
    $$Б = \{\text{внизу присутствует белый цвет}\}$$
    $$С = \{\text{внизу присутствует синий цвет}\}$$
    $$К = \{\text{внизу присутствует красный цвет}\}$$
    Проверим, что эти события попарно независимы. Верно ли, что
    $P(Б \cap С) = P(Б)P(С)$? Очевидно, что да. Значит, попарная независимость есть.
    Проверим теперь совместную независимость:
    $$
    P(Б\cap С\cap К) = \frac{1}{4} \neq \frac{1}{2} \cdot\frac{1}{2} \cdot\frac{1}{2} =
    P(Б)P(С)P(К)
    $$
    Это доказывает, что попарная независимость и совместная независимость 
    неравносильны.
\end{example}

\subsection{Независимые испытания Бернулли. Формулы Бернулли}

Пусть у нас есть вероятностное пространство $(\Omega, \mathfrak{F}, P)$.
Определим \emph{испытание}
$A_1, \ldots, A_m$ как 
набор событий, являющийся полной системой событий.

\begin{example}
    Представим, что мы одновременно подбрасываем монетку и кубик. Каким будет 
    вероятностное пространство? $\Omega = \{О1, О2, \ldots, О6, Р1, \ldots, Р6\}$
    Зададим испытания $A_1 = \{A_{11}, A_{12}\}$, где
    $A_{11} = \{\text{на монете О}\}$,
    $A_{12} = \{\text{на монете Р}\}$;
    $A_2 = \{A_{21},\ldots,A_{26}\}$, где $A_{2i}=\{\text{на кубике цифра } i\}$.
    $$
    P(A_{11})\cap P(A_{23}) = \frac{1}{12},\quad P(A_{11})=\frac{1}{2},\quad
    P(A_{23})=\frac{1}{2}
    $$
\end{example}

\begin{definition}
    Испытания $A_1, \ldots, A_m$ будем называть \emph{независимыми}, если для любого
    набора $A_{1i_1}, A_{2i_2} \ldots, A_{mi_m}$ составляющие его события являются
    совместно независимыми.
\end{definition}

Испытаниями Бернулли называются набор из $n$ независимых испытаний с двумя
исходами в каждом из 
них, условно называемыми успехом и неудачей, и с постоянной вероятностью успеха во всех 
испытаниях. Будем обозначать такой набор $(У_1, У_2, Н_3, \ldots, У_n)$, а вероятность
успеха $P(У_k)=p$.

Пусть
$A = \{\text{в } n \text{ испытаниях Бернулли успех произошёл ровно }
k \text{ раз}\}$. Какова вероятность $A$?
Заметим, что $A =(\underbrace{УУ\ldots У}_{k} \underbrace{НН\ldotsН}_{n-k}) \cup
(\underbrace{УУ\ldots УУ}_{k-1}НУНН\ldots Н) \cup \ldots$.
$$
P(УУ\ldots УНН\ldots Н) = P(У_1 \cap У_2 \cap \ldots \cap У_k \cap Н_{k+1} \cap
\ldots \cap Н_n) = p^k(1-p)^{n-k}
$$
Ясно, что вероятность любой цепочки, содержащей $k$ успехов и $n-k$ неудач,
равна $p^k(1-p)^{n-k}$

Получаем, что в нашем примере $P(A) = C_n^k p^k(1-p)^{n-k} = P_n(k)$ — эта формула
носит имя Бернулли.

\begin{corollary}
\mbox{}
\begin{enumerate}
    \item $P_n(n)=p^n$
    \item $P_n(0) = (1 - p)^n$
    \item $P_n(\text{хотя бы один успех}) = 1 - P_n(0) = 1 - (1-p)^n$
\end{enumerate}
\end{corollary}

\begin{example}
    Пусть мы подбрасываем монету 10 раз. Какова вероятность того, что все десять раз
    выпал орёл? По формуле Бернулли:
    $$
    P_{10}(10) = \bigg(\frac{1}{2}\bigg)^{10} = \frac{1}{1024}
    $$
    А вероятность того, что орёл выпал ровно пять раз, равна
    $$
    P_{10}(5) = C_{10}^5 \bigg(\frac{1}{2}\bigg)^5 \bigg(\frac{1}{2}\bigg)^5 = \frac{252}{1024}
    $$
    Возникает вопрос, каково наиболее вероятное число выпадений орла?
    $P_{10}(k) = C_{10}^k \big(\frac{1}{2}\big)^{10}$, и ясно, что максимум достигается при
    $k=5$
\end{example}

Обобщим последний вопрос примера.
Пусть имеется $n$ испытаний Бернулли. Вероятность успеха в каждом испытании равна $p$.
Чему равно наиболее вероятное число появления успехов?

Рассмотрим неравенство: 
$$
P_n(k) < P_n(k+1)
$$
Его можно переписать:
$$
C_n^k p^k(1-p)^{n-k} < C_n^{k+1} p^{k+1} (1-p)^{n-k-1}
$$
$$
\frac{n!}{k! (n-k)!} p^k (1-p)^{n-k} < \frac{n!}{(k+1)!(n-k-1)!} p^{k+1}(1-p)^{n-k-1}
$$
Полученное равносильно:
$$
(k+1)(1-p) < p(n-k)
$$
$$
k < (n+1)p - 1
$$

Если мы теперь посмотрим на обратное неравенство $P_n(k) > P_n(k+1)$, то увидим, что
оно равносильно $k > (n+1)p - 1$.

Рассмотрим два случая:
\begin{enumerate}
    \item $(n+1)p \notin \mb Z$. Обозначим за $k_n^\ast$ наиболее вероятное число
    успехов в $n$ испытаниях. Тогда $k_n^\ast=[(n+1)p]$
    \item $(n+1)p \in \mb Z$. В этому случае $k_n^{\ast_1} = (n+1)p - 1$ и
    $k_n^{\ast_2} = (n+1)p$ — наиболее вероятные числа успехов.
\end{enumerate}

\begin{example}
    Пусть $p = \frac{1}{2}$, $n = 10$. Тогда $k_{10}^\ast = 5$. Если же $n = 11$, то
    $k_{11}^{\ast_1} = 5$ и $k_{11}^{\ast_2} = 6$, так как $C_{11}^5 = C_{11}^6$.
\end{example}

\subsection{Предельные теоремы в схеме испытаний Бернулли}

Представим, что мы подбрасываем монету 10000 раз. Ясно, что наиболее вероятное число 
выпадений орла равна 5000. Чему же равна вероятность такого исхода?

$P_{10000}(5000) = C_{10000}^{5000}\big(\frac{1}{2}\big)^{10000}$. Мы хотели бы оценить это 
число.

Рассмотрим функции:
$$
\varphi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},
\quad \Phi(x) = \int\limits_{-\infty}^x \varphi(t)\mathrm dt
$$

Ясно, что $\varphi(x)$ — чётная, а $\Phi(-x) = 1 -\Phi(x)$.

Пусть $n$ — число испытаний, $k$ — число успехов, $p$ — вероятность успеха, $q=1-p$.
Введём обозначение: $x_{n,k} = \frac{k - np}{\sqrt{npq}}$.

\begin{theorem}[Локальная теорема Муавра-Лапласа]
    Справедливо следующее соотношение:
    $$
    \frac{P_n(k)}{\frac{1}{\sqrt{npq}}\cdot \frac{1}{\sqrt{2\pi}}\cdot
    e^{-\frac{x_{n,k}^2}{2}}} \underto{n \to \infty} 1\quad \text{равномерно по всем } k:\quad
    |x_{n,k}|\leqslant Cn^{\frac{1}{6}-\varepsilon}\quad \forall C>0,\, \varepsilon > 0
    $$
    
    Таким образом, $P_n(k) \approx \frac{1}{\sqrt{npq}}\varphi(x_{n,k})$.
\end{theorem}
\begin{lemma}[Формула Стирлинга]
    $$
    n! = n^n e^{-n} \sqrt{2\pi n}\cdot(1 + o(1))\, (n \to \infty)
    $$
\end{lemma}
\begin{lemma}\label{laplacetheoremlemma2}
    $$
    \ln(1+x) = x - \frac{x^2}{2} + \theta x^3\text{, где } |\theta| \leqslant 3
    \quad\forall |x| < \frac{1}{2}
    $$
\end{lemma}
\begin{proof}[Доказательство теоремы]
    По формуле Бернулли:
    $$
    P_n(k) = \frac{n!}{k!(n-k)!}p^k q^{n-k} =
    $$
    $$
    =
    \frac{n^n e^{-n}\sqrt{2\pi n} (1 + o(1))}{k^k e^{-k} \sqrt{2\pi k} (1 + o(1))
    (n-k)^{n-k} e^{-n+k}\sqrt{2\pi (n-k)}(1 + o(1))} p^kq^{n-k}
    $$
    Но это верно при $n \to \infty$, $k \to \infty$, $n-k \to \infty$.
    $$
    k = np + x_{n,k}\sqrt{npq} \underto{n \to \infty} \infty
    $$
    $$
    n-k = nq - x_{n,k}\sqrt{npq} \underto{n \to \infty} \infty
    $$
    Продолжая вычисления:
    $$
    P_n(k) = \bigg(\frac{k}{np}\bigg)^{-k-\frac{1}{2}}\bigg(\frac{n-k}{nq}\bigg)^{-n+k-\frac{1}{2}}
    \frac{1}{\sqrt{npq}}\frac{1}{\sqrt{2\pi}}(1 + o(1))
    $$
    Пусть $\sqrt{npq}P_n(k) = T_{n,k}$. Тогда:
    $$
    T_{n,k} = \bigg(\frac{k}{np}\bigg)^{-k-\frac{1}{2}}
    \bigg(\frac{n-k}{nq}\bigg)^{-n+k-\frac{1}{2}}
    \frac{1}{\sqrt{2\pi}}(1 + o(1))
    $$
    $$
    \ln T_{n,k} = \bigg(-k - \frac{1}{2}\bigg)\ln \frac{k}{np} + \bigg(-n+k-\frac{1}{2}\bigg)
    \ln \frac{n-k}{nq} + \ln \frac{1}{\sqrt{2\pi}} + o(1)
    $$
    С учётом
    $$
    \frac{k}{np} = 1 + x_{n,k}\sqrt{\frac{q}{np}}
    $$
    $$
    \frac{n-k}{nq} = 1 - x_{n,k}\sqrt{\frac{p}{nq}}
    $$
    Применив лемму \ref{laplacetheoremlemma2}, получим:
    $$
    \ln T_{n,k} = \bigg(-np - x_{n,k}\sqrt{npq} -
    \frac{1}{2}\bigg)\bigg(x_{n,k}\sqrt{\frac{q}{np}} -
    \frac{x_{n,k}^2}{2}\frac{q}{np} + \theta_1 \frac{x_{n,k}^3 q\sqrt{q}}{np\sqrt{np}}\bigg) +
    $$
    $$
    + \bigg(-np + x_{n,k}\sqrt{npq} - \frac{1}{2}\bigg)\bigg(-x_{n,k}\sqrt{\frac{p}{nq}} -
    \frac{x_{n,k}^2p}{2nq} + \theta_2x_{n,k}^3\frac{p}{nq}\frac{\sqrt{p}}{\sqrt{nq}}\bigg) +
    \ln \frac{1}{\sqrt{2\pi}} + o(1) =
    $$
    $$
    = -x_{n,k}\sqrt{npq} + \frac{x_{n,k}^2}{2}q + o(1) - x_{n,k}^2q +
    x_{n,k}\sqrt{npq} + \frac{x_{n,k}^2}{2}p
    - x_{n,k}^2p + \ln \frac{1}{\sqrt{2\pi}} =
    $$
    $$
    =-\frac{x_{n,k}^2}{2} + \ln \frac{1}{\sqrt{2\pi}} + o(1) 
    $$
    Тогда само $T_{n,k}$ равно:
    $$
    T_{n,k} = e^{-\frac{x_{n,k}^2}{2}} \frac{1}{\sqrt{2\pi}} e^{o(1)}
    $$
    Отсюда:
    $$
    P_n(k) = \frac{1}{\sqrt{npq}}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2_{n,k}}{2}}
    e^{o(1)}
    $$
    Теорема доказана.
\end{proof}

Попробуем ответить на вопрос: как найти $P_n(a < \text{число успехов} < b)$
при $a<b$ и больших $n$? Применение локальной теоремы Муавра-Лапласа может давать
слишком высокие погрешности, поэтому необходимо использовать иное решение.
\begin{theorem}[Интегральная теорема Муавра-Лапласа]\label{integral_theorem}
    Пусть $p$ — вероятность успеха, $0 < p < 1$. Тогда:
    $$
    \sup_{a<b} \Bigg| P_n(a < \text{число успехов} < b) -
    \int\limits_{\frac{a - np}{\sqrt{npq}}}^{\frac{b-np}{\sqrt{npq}}}
    \varphi(t) \dif t \Bigg| \underto{n \to \infty} 0
    $$
\end{theorem}

Мы докажем эту теорему позже — как частный случай более общей теоремы.

\begin{corollary}
    $$
    \int\limits_{-\infty}^{+\infty}
    \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}\dif t = 1
    $$
\end{corollary}
\begin{proof}
    Примем $a = -\infty$, $b = +\infty$.
\end{proof}

Если мы возьмём функцию $\Phi(x) = \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^x
e^{-\frac{t^2}{2}}\dif t$,
то теорему можно сформулировать так:
$$
P_n(a < \text{число успехов} < b) \approx
\Phi\bigg(\frac{b-np}{\sqrt{npq}}\bigg) -
\Phi\bigg(\frac{a - np}{\sqrt{npq}}\bigg)
$$

\begin{example}
    Подсчитаем вероятность того, что при подбрасывании монетки 10000 раз <<орёл>>
    выпадет 5000 раз. Для этого применим локальную теорему Муавра-Лапласа.
    $$
    P_{10000}(5000) \approx
    \frac{1}{\sqrt{10000 \cdot \frac{1}{2} \cdot \frac{1}{2}}}\varphi(0) =
    \frac{1}{50} \cdot 0.39894 \approx 0,00798
    $$
    Если мы захотим подсчитать вероятность того, что <<орёл>> выпадет как минимум 
    4900 раз и как максимум 5100, то необходимо будет применить интегральную
    теорему:
    $$
    P_{10000}(4900 < \text{число успехов} < 5100) \approx \Phi(2) - \Phi(-2) = 
    2\Phi(2) - 1 \approx 0.9544
    $$
\end{example}

\begin{theorem}[Пуассона]
    Будем рассматривать схему серий испытаний Бернулли. Допустим, первая серия 
    состоит из одного испытания такого, что $p_1 = P_1(У)$. Вторая серия
    состоит из двух испытаний и $p_2 = P_2(У)$.
    $n$-я серия испытаний состоит из $n$ испытаний и $p_n = P_n(У)$.
    (Здесь $P_i(У)$ — вероятность успеха в одном испытании для каждой серии 
    соответственно.)
    Пусть также $np_n = \const = \lambda > 0$. Тогда:
    $$
    P_n(k) \underto{n \to \infty} \frac{\lambda^k}{k!}e^{-\lambda}
    $$
\end{theorem}
\begin{proof}
    $$
    P_n(k) = C_n^k p_n^k(1 - p_n)^{n-k} = \frac{n!}{k!(n-k)!}p_n^k(1-p_n)^{n-k} =
    $$
    $$
    = \frac{1}{k!} \frac{n(n-1)\ldots(n-k+1)}{n^k} n^k
    p_n^k(1-p_n)^n(1-p_n)^{-k} =
    $$
    $$
    = \frac{\lambda^k}{k!}
    \underbrace{\bigg(1 - \frac{1}{n}\bigg)\ldots\bigg(1 - \frac{k-1}{n}\bigg)}_{\to 1} \cdot
    \underbrace{\bigg(1 - \frac{\lambda}{n}\bigg)^n}_{\to e^{-\lambda}}\cdot
    \underbrace{\bigg(1 - \frac{\lambda}{n}\bigg)^{-k}}_{\to 1}    
    $$
\end{proof}

\begin{theorem}[Закон больших чисел Бернулли]\label{large_numbers_bernoulli}
    $$
    \forall \varepsilon > 0\quad P_n\bigg(\bigg|\frac{k_n}{n} -
    p\bigg| > \varepsilon\bigg)
    \underto{n \to \infty} 0
    $$
    Здесь $\frac{k_n}{n}$ называется частотой успеха.
\end{theorem}
\begin{proof}
    Рассмотрим вероятность:
    $$
    P_n\bigg(\bigg|\frac{k_n}{n} -
    p\bigg| > \varepsilon\bigg) = 1 - P_n\bigg(\bigg|\frac{k_n}{n} -
    p\bigg| \leqslant \varepsilon\bigg) =
    \frac{1}{\sqrt{2\pi}}
    \int\limits_{-\infty}^{+\infty}e^{-\frac{t^2}{2}} \dif t -
    P_n(np - n\varepsilon \leqslant k_n \leqslant n\varepsilon + np) =
    $$
    $$
    =\frac{1}{\sqrt{2\pi}}
    \int\limits_{|t|\leqslant \varepsilon \sqrt{\frac{n}{pq}}}
    e^{-\frac{t^2}{2}} \dif t + \frac{1}{\sqrt{2\pi}}
    \int\limits_{|t| > \varepsilon \sqrt{\frac{n}{pq}}}
    e^{-\frac{t^2}{2}} \dif t -
    P_n(np - n\varepsilon \leqslant k_n \leqslant n\varepsilon + np)
    $$
    Разность первого и третьего слагаемого стремится к нулю, второе слагаемое тоже
    стремится к нулю. Теорема доказана.
\end{proof}

\subsection{Случайная величина. Распределение случайных величин}

Будем рассматривать вероятностное пространство $(\Omega, \mathfrak{F}, P)$.

\begin{definition}
    Функция $\xi: \Omega \to \real$ такая, что
    $\forall B \in \mathfrak B$
    $\xi^{-1}(B) \in \mathfrak F$, называется \emph{случайной величиной}.
    (Здесь $\mathfrak B$ обозначает борелевскую сигма-алгебру.)
\end{definition}
\begin{examples}
    \begin{enumerate}
        \item Пусть мы подбрасываем игральный кубик.
        $\Omega = \{\omega_1,\ldots,\omega_6\}$. Пусть $\xi_1(\omega_i) = i$,
        $\xi_2(\omega_i) =
        \begin{cases}
            0,\quad i \neq 6\\
            1,\quad i = 6\\  
        \end{cases}
        $. $\xi_1$, $\xi_2$ — случайные величины.
        \item Пусть $\Omega = [0, 1]$,
        $\mathfrak F = \Big\{\big[0, \frac{1}{2}\big],
        \big(\frac{1}{2}, 1\big], [0, 1], \varnothing\Big\}$,
        $\eta(x) = x$. $\eta$ не будет случайной величиной, так как
        $\eta^{-1}\Big(\big[0, \frac{1}{3}\big]\Big) = \big[0, \frac{1}{3}\big] \notin \mathfrak F$.
    \end{enumerate}
\end{examples}

Рассмотрим функцию $\mathfrak P_\xi: \mathfrak B \to \real$ такую, что
$\forall B \in \mathfrak B$ $\mathfrak P_\xi(B) = P(\xi^{-1}(B))$.

\begin{definition}
    $\mathfrak P_\xi$ называется \emph{распределением случайной величины $\xi$}.
\end{definition}
\begin{theorem}\label{distribution_is_probability}
    $\mathfrak P_\xi$ является вероятностью.
\end{theorem}
\begin{proof}
    Проверим аксиомы вероятности.
    \begin{enumerate}
        \item $\distr(B) = P(\xi^{-1}(B)) \geqslant 0$
        \item $\distr (\real) =P(\xi^{-1}(\real)) = P(\Omega) = 1$
        \item Пусть $B_1, B_2, \ldots \in B$, $B_i \cap B_j = \varnothing$
        ($i \neq j$).
        $$
        \distr \bigg(\bigcup\limits_{i=1}^\infty B_i\bigg) =
        P\bigg(\xi^{-1}\bigg(\bigcup\limits_{i=1}^\infty B_i\bigg)\bigg) =
        $$
        $$
        = P\bigg(\bigcup\limits_{i=1}^\infty \underbrace{\xi^{-1}
        (B_i)}_{\substack{\text{попарно}\\\text{несовместны}}}\bigg) =
        \sum\limits_i P(\xi^{-1}(B_i)) = \sum\limits_i \distr (B_i)
        $$
    \end{enumerate}
\end{proof}

Теперь мы можем перейти к использованию вероятностного пространства
$(\real, \mathfrak B, \distr)$.

\begin{definition}
    Функция $\funcdistr: \real \to \real$ такая, что
    $\forall x\in \real$ $\funcdistr(x) = \distr((-\infty, x)) = 
    P(\xi^{-1}((-\infty, x))) = P(\omega:\, \xi(\omega) < x) = P(\xi < x)$,
    называется \emph{функцией распределения случайной величины $\xi$}.
\end{definition}

Рассмотрим значение $\funcdistr(b) = \distr((-\infty, b)) =
\distr((-\infty, a) \cup [a, b)) = \distr((-\infty,a)) + \distr([a, b)) =
\funcdistr(a) + \distr([a, b))$. Получается, что
$\distr([a, b)) = \funcdistr(b) - \funcdistr(a)$, то есть,
существует взаимно-однозначное соответствие между $\distr$ и $\funcdistr$.

\begin{proposition}[Свойства функции распределения]
    \begin{enumerate}
        \item $\forall x$ $0\leqslant \funcdistr(x) \leqslant 1$;
        \item $\funcdistr$ неубывает;
        \item\label{funcdistr_cont} $\funcdistr$ непрерывна слева во всех точках.
        \item\label{funcdistr_lim} $\funcdistr(x) \underto{x \to -\infty} 0$,
        $\funcdistr(x) \underto{x \to \infty} 1$
    \end{enumerate}
\end{proposition}
\begin{proof}
    Докажем свойство \ref{funcdistr_cont}.
    Пусть $x_1 < x_2 < \ldots$ и $x_n \underto{n \to \infty} x$.
    Мы хотим доказать, что $\funcdistr(x_n) \underto{n \to \infty} \funcdistr(x)$.
    $(-\infty, x_n) \subset (-\infty, x_{n+1})$ и $\bigcup\limits_n(-\infty, x_n)
    = (-\infty, x)$. Отсюда, по свойству вероятности,
    $\underbrace{\distr((-\infty, x_n))}_{\funcdistr(x_n)}
    \underto{n \to \infty} \underbrace{\distr((-\infty, x))}_{\funcdistr(x)}$.
    Всё доказано.
    
    Докажем свойство \ref{funcdistr_lim}.
    $\forall n \geqslant 1$ $[-n, n) \subset [-(n+1), n+1)$ и
    $\bigcup\limits_{n \geqslant 1}[-n, n) = \real$. Тогда:
    $$
    \underbrace{\distr([-n, n))}_{\funcdistr(n) - \funcdistr(-n)}
    \underto{n \to \infty} \distr(\real) = 1
    $$
    Отсюда получаем, что $\lim\limits_{n \to \infty} \funcdistr(n) = 1$
    и $\lim\limits_{n \to \infty} \funcdistr(-n) = 0$. Свойство \ref{funcdistr_lim}
    доказано.
\end{proof}

Мы доказали, что функция распределения удовлетворяет указанным свойствам, однако
верно и обратное.

\begin{proposition}
    Пусть $G: \real \to \real$ — функция, удовлетворяющая свойствам функции
    распределения. Тогда $G$ является функцией распределения некоторой случайной 
    величины.
\end{proposition}
\begin{proof}
    Упражнение.
\end{proof}

\subsection{Различные типы распределений случайных величин. Случайные величины
с дискретным распределением}

\begin{definition}
    Случайная величина $\xi$ называется случайной величиной с дискретным 
    распределением, если существует не более чем счётное подмножество
    вещественных чисел $A$ такое, что $P(\xi \in A) = 1$. Другими словами, у этой
    случайной величины конечное или счётное число значений.
\end{definition}

Занумеруем все элементы множества $A$: $A = \{a_1, a_2,\ldots\}$, обозначим
$p_i = P(\xi = a_i)$. Правило, сопоставляющее каждому значению $a_i$ случайной
величины $\xi$ вероятность $p_i$, называется \emph{законом распределения}.
\begin{examples}
    \begin{enumerate}
        \item $\xi = c$, $p = 1$ — случайная величина с вырожденым в точке
        $c$ распределением. График функции распределения $\funcdistr(x) =
        P(\xi < x)$ представляет собой ступенчатую функцию, принимающую
        значение $0$ при $x \leqslant c$ и $1$ при $x > c$.
        \item $(\xi, p): (0, 1-p), (1, p)$ — случайная величина с
        распределением Бернулли.
        \item $(\xi, p): (a_1, p_1), (a_2, p_2), \ldots, (a_n, p_n)$.
        \item $\xi: 0, 1, 2, \ldots, n$ и $p_k = P(\xi = k) = C_n^k p^k(1-p)^{n-k}$ 
        — случайная величина с биномиальным законом распределения и
        параметрами $n$ и $p$.
        \item $\xi: 0, 1, 2, \ldots$ и
        $P(\xi = k) = \frac{\lambda^k}{k!}e^{-\lambda}$ — случайная величина
        с распределением Пуассона с параметром $\lambda > 0$.
        \item $\xi: 0, 1, 2, \ldots$ и $P(\xi = k) = p^k(1-p)$ —
        случайная величина с геометрическим распределением с параметром
        $p$ ($0 \leqslant p \leqslant 1$).
    \end{enumerate}
\end{examples}
\begin{definition}
    Случайная величина $\xi$ называется случайной величиной с абсолютно 
    непрерывным распределением, если существует функция $p(x)$ такая, что
    для любого $y \in \real$ $F_\xi(y) = \int\limits_{-\infty}^y p(x) \dif x$.
    В этом случае $p(x) = p_\xi(x)$ называется \emph{плотностью распределения 
    случайной величины $\xi$}.
\end{definition}
\begin{proposition}[Свойства плотности распределения]
\mbox{}
    \begin{enumerate}
        \item $p(x) \geqslant 0$.
        \item $\int\limits_{-\infty}^{+\infty} p(x) \dif x = 1$.
    \end{enumerate}
\end{proposition}
\begin{examples}
    \begin{enumerate}
        \item $\xi$ — случайная величина с равномерным распределением на
        отрезке $[a,b]$. (Обозначается $\xi \in U_{[a,b]}$).
        $$
        p_\xi (x) =
        \begin{cases}
            \frac{1}{|b-a|},\quad x \in [a,b] \\
            0, \quad x \notin [a,b] \\
        \end{cases}
        $$
        \item $\xi$ — случайная величина с экспоненциальным распределением с 
        параметром $\alpha$.
        $$
        p_\xi(x) =
        \begin{cases}
            0,\quad x < 0 \\
            \alpha e^{-\alpha x},\quad x \geqslant 0 \\
        \end{cases}
        $$
        \item (Важный пример!) $\xi$ — случайная величина с нормальным 
        распределением с параметрами $a \in \real$ и $\sigma^2 > 0$.
        $$
        p_\xi(x) = \frac{1}{\sqrt{2\pi \sigma^2}}e^{- \frac{(x - a)^2}{2\sigma^2}}
        $$
        В случае, когда $a = 0$, $\sigma^2 = 1$, говорят о \emph{стандартном 
        нормальном распределении}.
        В этом случае $p_\xi = \varphi$ — функция Гаусса.
        \item $\xi$ — случайная величина с распределением Коши.
        $p_\xi(x) = \frac{1}{\pi (1 + x^2)}$.
    \end{enumerate}
\end{examples}

Заметим, что $P(a < \xi < b) = P(a \leqslant \xi < b) =
P(a \leqslant \xi \leqslant b) = F_\xi(b) - F_\xi(a)$. Почему строгость неравенства
не имеет значения? В силу непрерывности функции распределения
$P(a < \xi < b) = F_\xi(b) - F_\xi(a) \underto{b \to a} 0$.

\subsection{Случайные векторы. Распределение случайных векторов}

\begin{definition}
    Пусть $\xi_1, \ldots, \xi_d$ — случайные величины, определённые на одном
    вероятностном пространстве. Тогда $\xi = (\xi_1, \ldots, \xi_d)$ будет
    называться случайным $d$-мерным вектором, $\xi: \Omega \to \real^d$.
    Иначе говоря, $\forall B \in \mathfrak B^d$ $\xi^{-1}(B) \in \mathfrak F$.
\end{definition}

\begin{definition}
    Функция $F_{(\xi_1, \ldots, \xi_d)}: \real^d \to \real$ такая, что
    $\forall x_1, \ldots, x_d$ $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d) =
    P(\xi_1 < x_1, \ldots, \xi_d < x_d)$, называется функцией распределения
    случайного вектора.
\end{definition}

$\xi_1, \ldots, \xi_d$ — с дискретным распределением, если существует $A \subset 
\real^d$ такое, что $A$ не более чем счётно и
$P((\xi_1, \ldots, \xi_d) \in A) = 1$.

$\xi_1, \ldots, \xi_d$ — с абсолютно непрерывным распределением, если существует
$p(x_1, \ldots, x_d)$ такое, что $F_{(\xi_1, \ldots, \xi_d)}(y_1, \ldots, y_d) =
\int\limits_{-\infty}^{y_1} \ldots \int\limits_{-\infty}^{y_d}
p(x_1, \ldots, x_d) \dif x_1 \ldots \dif x_d$.

\begin{proposition}[Свойства функции распределения случайного вектора]
\mbox{}
    \begin{enumerate}
        \item  $0 \leqslant F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d)
        \leqslant 1$
        \item Пусть $x_1 \leqslant y_1, \ldots, x_d \leqslant y_d$.
        Тогда $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d) \leqslant
        F_{(\xi_1, \ldots, \xi_d)}(y_1, \ldots, y_d)$
        \item $F_{(\xi_1, \ldots, \xi_d)}$ непрерывна слева по каждой координате.
        \item $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d)
        \underto{x_k \to -\infty} 0$.
        \item $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d)
        \underto{x_k \to +\infty}
        F_{(\xi_1, \ldots, \xi_{k-1}, \xi_{k+1}, \ldots \xi_d)}
        (x_1, \ldots, x_{k-1}, x_{k+1}, \ldots x_d)$.
        \item $F_{(\xi_1, \ldots, \xi_d)}(x_1, \ldots, x_d)
        \underto{x_1 \to +\infty, \ldots, x_d \to +\infty} 1$
        \item Пусть $d = 2$. Тогда
        $P\big((\xi_1, \xi_2) \in [a_1, b_1) \times [a_2, b_2)\big) =
        F_{(\xi_1, \xi_2)}(b_1, b_2) - F_{(\xi_1, \xi_2)}(a_1, b_2) -
        F_{(\xi_1, \xi_2)}(b_1, a_2) + F_{(\xi_1, \xi_2)}(a_1, a_2)$. То есть, 
        вероятность попадания точки в произвольный параллелепипед
        однозначно выражается 
        через значения функции распределения в вершинах этого параллелепипеда.
    \end{enumerate}
\end{proposition}

\subsection{Независимость случайных величин}

Вспомним, что события $A, B \in \mathfrak F$ называются независимыми, если
$P(A \cap B) = P(A)P(B)$. Введём теперь понятие независимости для случайных
величин.

\begin{definition}\label{random_var_independence1}
    Пусть $\xi, \eta$ — случайные величины, определённые на одном вероятностном
    пространстве. Будем говорить, что $\xi$ и $\eta$ — независимые случайные 
    величины, если для любых вещественных $x, y$
    $F_{(\xi, \eta)}(x, y) = F_\xi(x)F_\eta(y)$.
\end{definition}
\begin{definition}\label{random_var_independence2}
    $\xi$ и $\eta$ — независимые случайные величины, если
    $\forall a_1 < b_1, a_2 < b_2$ $P\big((\xi, \eta) \in [a_1, b_1) \times
    [a_2, b_2)\big) = P(a_1 \leqslant \xi < b_1) \cdot P(a_2 \leqslant
    \eta < b_2)$.
\end{definition}

\begin{proposition}
    Определения \ref{random_var_independence1} и \ref{random_var_independence2} 
    эквивалентны.
\end{proposition}
\begin{proof}
\mbox{}
    \begin{enumerate}
        \item (\ref{random_var_independence2} $\Rightarrow$
        \ref{random_var_independence1})
        $$
        P\big((\xi, \eta) \in [a_1, b_1) \times
        [a_2, b_2)\big) = P(a_1 \leqslant \xi < b_1) \cdot P(a_2 \leqslant
        \eta < b_2)
        $$
        При $a_1, a_2 \to -\infty$ левая часть равенства стремится к
        $F_{(\xi, \eta)}(b_1, b_2)$, а правая часть — к
        $F_\xi(b_1)\cdot F_\eta(b_2)$.
        \item (\ref{random_var_independence1} $\Rightarrow$
        \ref{random_var_independence2})
        $$
        P\big((\xi, \eta) \in [a_1, b_1) \times
        [a_2, b_2)\big) = F_{(\xi, \eta)}(b_1, b_2) - F_{(\xi, \eta)}(a_1, b_2)
        - F_{(\xi, \eta)}(b_1, b_a) +
        $$
        $$
        + F_{(\xi, \eta)}(a_1, a_2) = F_\xi(b_1)F_\eta(b_2) - 
        F_\xi(a_1)F_\eta(b_2) - F_\xi(b_1)F_\eta(a_2) +
        $$
        $$
        +F_\xi(a_1)F_\eta(a_2) = \big(F_\xi(b_1) - F_\xi(a_1)\big)
        \big(F_\eta(b_2) - F_\eta(a_2)\big) = 
        $$
        $$
        P(a_1 \leqslant \xi < b_1) \cdot
        P(a_2 \leqslant \eta < b_2)
        $$
    \end{enumerate}
\end{proof}
\begin{definition}\label{random_var_independence3}
    Пусть для любых $B_1,\ldots, B_d \in \mathfrak B$ $P(\xi_1 \in B_1, \ldots,
    \xi_d \in B_d) = \prod\limits_{i=1}^d P(\xi_i \in B_i)$. Тогда $\xi_1, \ldots,
    \xi_d$ называются независимыми случайными величинами.
\end{definition}

Очевидно, что это определение также эквивалентно двум предыдущим.

\begin{example}
        Рассмотрим случайный вектор $\xi = (\xi_1, \xi_2)$ с плотностью 
        распределения
        $$
        p_\xi(x, y) =
        \begin{cases}
            1,\quad 0\leqslant x \leqslant 1,\, 0\leqslant y \leqslant 1 \\
            0\quad \text{иначе} \\
        \end{cases}
        $$
        Здесь:
        $$
        F_{\xi_1}(x) = P(\xi_1 < x) =
        \begin{cases}
            0,\quad x\leqslant 0 \\
            x,\quad 0 < x \leqslant 1 \\
            1,\quad x > 1 \\
        \end{cases}
        $$
        $$
        F_{\xi_2}(y) = P(\xi_2 < y) =
        \begin{cases}
            0,\quad y\leqslant 0 \\
            y,\quad 0 < y \leqslant 1 \\
            1,\quad y > 1 \\
        \end{cases}
        $$
        $$
        F_{(\xi_1, \xi_2)}(x, y) =
        \begin{cases}
            0,\quad x\leqslant 0 \text{ или } y \leqslant 0 \\
            xy,\quad 0 < x \leqslant 1 \text{ и } 0 < y \leqslant 1 \\
            x,\quad 0 < x \leqslant 1 \text{ и } y > 1 \\
            y,\quad 0 < y \leqslant 1 \text{ и } x > 1 \\
            1,\quad x > 1 \text{ и } y > 1 \\
        \end{cases}
        $$
        Видим, что $F_{(\xi_1, \xi_2)}(x, y) = F_{\xi_1}(x)F_{\xi_2}(y)$, значит,
        $\xi_1$ и $\xi_2$ независимы.
\end{example}

Заметим, что для того, чтобы доказать, что события не являются независимыми,
достаточно найти хотя бы одну точку, в которой определение нарушается.

Мы хотим найти более простой критерий независимости событий.

Рассмотрим для начала случай дискретного распределения, ограничимся двумерным 
случаем.

\begin{proposition}
    $\xi = (\xi_1, \xi_2)$ — случайный вектор с дискретным распределением
    тогда и только тогда, когда его компоненты — с дискретным распределением.
\end{proposition}
\begin{proof}
    Очевидно.
\end{proof}

\begin{theorem}
    Пусть $\xi = (\xi_1, \xi_2)$ — с дискретным распределением.
    Тогда независимость
    случайных величин $\xi_1$ и $\xi_2$ равносильна тому, что для любых 
    вещественных $a$ и $b$ выполняется $P(\xi_1 = a, \xi_2 = b) =
    P(\xi_1 = a)P(\xi_2 = b)$.
\end{theorem}
\begin{proof}
        Если $\xi_1$, $\xi_2$ независимы, то
        $\forall a,b \in \real$ и $\forall \varepsilon > 0$
        $P(a \leqslant \xi_1 \leqslant a + \varepsilon,
        b \leqslant \xi_2 < b + \varepsilon) =
        P(a \leqslant \xi_1 \leqslant a + \varepsilon)
        P(b \leqslant \xi_2 < b + \varepsilon)$ — по определению 
        \ref{random_var_independence2}. Устремим $\varepsilon$ к нулю.
        
        Обратно:
        $$
        F_{(\xi_1, \xi_2)}(y_1, y_2) =
        \sum\limits_{\substack{a_i < y_1\\b_j < y_2}} P(\xi_1 = a_i, \xi_2 = b_j) =
        \sum\limits_{\substack{a_i < y_1\\b_j < y_2}} P(\xi_1 = a_i)P(\xi_2 = b_j) =
        $$
        $$
        = \sum\limits_{b_j < y_2}\Bigg(\sum\limits_{a_i < y_1}
        P(\xi_1 = a_i)P(\xi_2 = b_j)\Bigg) =
        \sum\limits_{a_i < y_1} P(\xi_1 = a_i) \cdot
        \sum\limits_{b_j < y_2} P(\xi_2 = b_j) =
        F_{\xi_1}(y_1) F_{\xi_2}(y_2)
        $$
\end{proof}

Теперь рассмотрим случай непрерывного распределения.

\begin{proposition}
    Если $\xi = (\xi_1, \ldots, \xi_d)$ — с абсолютным непрерывным распределением,
    то $\forall k = 1, 2,\ldots, d$ $\xi_k$ с абсолютно непрерывным 
    распределением.
\end{proposition}

Заметим, что обратное, вообще говоря, неверно (доказательство этого факта можно
найти, например, в пособии
<<Контрпримеры в теории вероятностей>>, Й. Стоянов, гл. 2 §5.6).

Таким образом, $\xi$ — случайный вектор с абсолютно непрерывным 
распределением тогда и только тогда, когда для любых $y_1, \ldots, y_d$
$$
F_\xi(y_1, \ldots, y_d) = \int\limits_{-\infty}^{y_1}\ldots
\int\limits_{-\infty}^{y_d} p_\xi(x_1, \ldots, x_d) \dif x_1\ldots \dif x_d
$$
При $y_2, \ldots, y_d \to \infty$ левая часть стремится к $F_{\xi_1}(y_1)$,
а в правой будет
$$
\int\limits_{-\infty}^{y_1}\Bigg(\int\limits_{-\infty}^{+\infty}\ldots
\int\limits_{-\infty}^{+\infty} p_\xi(x_1, \ldots, x_d)
\dif x_2\ldots\dif x_d\Bigg)\dif x_1
$$
Выражение в скобках равно $p_{\xi_1}(x_1)$ — плотности распределения $\xi_1$.

Помимо этого, $\xi = (\xi_1, \ldots, \xi_d)$ —
с абсолютно непрерывным распределением
$\iff$ для любых $a_1 < b_1, \ldots, a_d < b_d$ выполнено
$$
P(a_1 \leqslant \xi_1 < b_1, \ldots, a_d \leqslant \xi_d < b_d) =
\int\limits_{a_1}^{b_1}\ldots\int\limits_{a_d}^{b_d} p_\xi(x_1, \ldots, x_d)
\dif x_1\ldots \dif x_d
$$
Что также равносильно
$$
\forall B \in \mathfrak B^d\quad P(\xi \in B) = \int\limits_B p_\xi(x_1, \ldots, x_d)
\dif x_1\ldots \dif x_d
$$

Как нам проверить, будут ли случайные величины, составляющие случайный вектор
с абсолютно непрерывным распределением, независимыми?

\begin{theorem}
    Пусть $\xi = (\xi_1, \xi_2)$ — с абсолютно непрерывным распределением и 
    плотностью $p_\xi(x, y)$. Тогда независимость $\xi_1, \xi_2$ равносильна
    тому, что $p_\xi(x, y) = p_{\xi_1}(x)p_{\xi_2}(y)$.
\end{theorem}
\begin{proof}
    Проверим необходимость. Рассмотрим функцию распределения в произвольной точке:
    $F_{(\xi_1, \xi_2)}(x, y) = F_{\xi_1}(x)F_{\xi_2}(y)$. Дважды 
    продифференцировав левую часть, получим
    $$
    \frac{\dif^2 F_{(\xi_1, \xi_2)}(x, y)}{\dif x \dif y} = p_\xi(x, y)
    $$
    Но $\frac{\dif^2 F_{\xi_1}(x)F_{\xi_2}(y)}{\dif x \dif y} =
    \frac{\dif F_{\xi_1}(x)}{\dif x}\frac{\dif F_{\xi_2}(y)}{\dif y} =
    p_{\xi_1}(x)p_{\xi_2}(y)$.
    
    Обратно. Для любых $x, y$
    $$
    \int\limits_{-\infty}^x\int\limits_{-\infty}^y p_\xi(s, t) \dif s \dif t =
    \int\limits_{-\infty}^x p_{\xi_1}(s) \dif s
    \int\limits_{-\infty}^y p_{\xi_2}(t) \dif t =
    F_{\xi_1}(x)F_{\xi_2}(y)
    $$
\end{proof}

\begin{theorem}\label{functions_of_independent_variables}
    Пусть $\xi, \eta$ — независимые случайные величины, $f, g$ — измеримые 
    функции. Тогда $f(\xi)$ и $g(\eta)$ — тоже независимые случайные величины.
\end{theorem}
\begin{proof}
    Рассмотрим $B_1, B_2 \in \mathfrak B$.
    $$
    P\big(f(\xi) \in B_1, g(\eta) \in B_2\big) =
    P\big(\xi \in f^{-1}(B_1), \eta \in g^{-1}(B_2)\big)
    $$
    Но так как функции измеримы, то прообразы измеримых подмножеств будут также 
    измеримыми. По определению \ref{random_var_independence3} это равно:
    $$
    P\big(\xi \in f^{-1}(B_1)\big) P\big(\eta \in g^{-1}(B_2)\big) = P\big(f(\xi) \in B_1\big)
    P\big(g(\eta) \in B_2\big)
    $$
\end{proof}

\subsection{Свёртка распределения}

Пусть имеется две независимые случайные величины $\xi, \eta$. Мы хотим узнать
распределение $\xi + \eta$, зная функции распределения $F_\xi$ и $F_\eta$.

Рассмотрим функцию распределения $\xi + \eta$ в произвольной точке $x$:
$$
F_{\xi + \eta}(x) = P(\xi + \eta < x) = P((\xi, \eta) \in A_x) = 
\iint\limits_{A_x} \dif F_{(\xi, \eta)}(s, t) =
\iint\limits_{A_x} \dif(F_\xi(s)F_\eta(t)) =
$$
$$
= \iint\limits_{A_x} \dif F_\xi(s)\dif F_\eta(t) =
\int\limits_{-\infty}^{+\infty}
\Bigg(\int\limits_{-\infty}^{x-s} \dif F_\eta(t)\Bigg) \dif F_\xi(s)
$$
Здесь $A_x = \{(s, t)\, |\, s+t<x\}$. Таким образом, доказана следующая теорема.
\begin{theorem}[О свёртке распределения]
    $$
    F_{\xi + \eta}(x) = \int\limits_{-\infty}^{+\infty} F_\eta(x-s)
    \dif F_\xi(s) =
    \int\limits_{-\infty}^{+\infty} F_\xi(x-t) \dif F_\eta(t)
    $$
\end{theorem}
\begin{corollary}
    Если $\xi, \eta$ — независимые случайные величины и $\xi$ — с абсолютно
    непрерывным распределением, то $\xi + \eta$ — тоже с абсолютно непрерывным
    распределением и, кроме того, $p_{\xi + \eta}(x) =
    \int\limits_{-\infty}^{+\infty} p_\xi(x - t) \dif F_\eta(t)$. Если же
    обе случайные величины $\xi, \eta$ с абсолютно непрерывным распределением,
    то $p_{\xi + \eta}(x) = \int\limits_{-\infty}^{+\infty}
    p_\xi(x-t)p_\eta(t)\dif t$.
\end{corollary}

\subsection{Числовые характеристики случайных величин. Математическое ожидание}

Как обычно, будем рассматривать вероятностное пространство
$(\Omega, \mathfrak{F}, P)$, случайная величина $\xi: \Omega \to \real$ с функцией
распределения $F_\xi(x) = P(\xi < x)$ и распределением $\mathfrak P_\xi(A) =
P(\xi^{-1}(A)) = P(\xi \in A)$.

\begin{definition}
    Математическим ожиданием случайной величины $\xi$ называется число
    $E\xi = \int\limits_\Omega \xi(\omega) \dif P(\omega)$, если таковое
    существует.
\end{definition}

Интеграл можно записать следующим образом:
$\int\limits_\Omega \xi^+ (\omega) \dif P(\omega) + \int\limits_\Omega
\xi^-(\omega)\dif P(\omega)$. Здесь:
$$
\xi^+ =
\begin{cases}
    \xi,\quad \xi \geqslant 0 \\
    0,\quad \xi < 0 \\
\end{cases}
\quad
\xi^- =
\begin{cases}
    0,\quad \xi \geqslant 0 \\
    \xi,\quad \xi < 0 \\
\end{cases}
$$

Тогда $|\xi| = \xi^+ - \xi^-$. Ясно, что $E\xi$ и $E|\xi|$ существуют 
одновременно.

\begin{remark}
    В литературе математическое ожидание также может обозначаться как $M\xi$.
\end{remark}

Для того, чтобы понять смысл этой величины, рассмотрим следующий пример.

\begin{example}
    Пусть $\xi$ — случайная величина с дискретным распределением и законом 
    распределения $(\xi, p): (a_1, p_1), (a_2, p_2), \ldots, (a_n, p_n)$.
    Для такой величины:
    $$
    E\xi = \int\limits_\Omega \xi(\omega) \dif P
    $$
    Рассмотрим события $A_k = (\xi = a_k)$, причём $A_k \cap A_l = \varnothing$ 
    при $k \neq l$ и $\bigcup\limits_k A_k = \Omega$. Тогда:
    $$
    E\xi = \sum\limits_k \int\limits_{A_k} \xi(\omega) \dif P =
    \sum\limits_k a_k \underbrace{\int\limits_{A_k} \dif P}_{P(\xi = a_k)} = \sum\limits_k a_kp_k
    $$
    То есть, математическое ожидание является средневзвешенным значением случайной
    величины.
    В частности, если $p_1 = p_2 = \ldots = p_n = \frac{1}{n}$, то
    $E\xi = \frac{a_1 + \ldots + a_n}{n}$.
\end{example}

\begin{proposition}[Свойства математического ожидания]
Пусть $\xi$, $\eta$ — случайные величины. Верно:
    \begin{enumerate}
        \item $P(\xi = c) = 1 \implies E\xi = c$
        \item $E(a\xi + n\eta) = aE\xi + bE\eta$, где $a,b \in \real$
        \item $P(\xi \geqslant 0) = 1 \implies E\xi \geqslant 0$
        \item $P(\xi \geqslant \eta) = 1 \implies E\xi \geqslant E\eta$
        \item $|E\xi| \leqslant E|\xi|$
    \end{enumerate}
\end{proposition}
\begin{proof}
\mbox{}
    \begin{enumerate}
        \item $E\xi = \int\limits_\Omega\xi(\omega)\dif P =
        c\int\limits_\Omega \dif P = c\cdot P(\Omega) = c$
        \item Очевидно из линейности интеграла.
        \item Очевидно.
        \item $P(\xi \geqslant \eta) = P(\xi - \eta \geqslant 0) = 1 \implies
        E(\xi - \eta) = E\xi - E\eta \geqslant 0$
        \item Так как $|\xi| \geqslant \xi$ и $|\xi| \geqslant - \xi$, то
        $E|\xi| \geqslant E\xi$ и $E|\xi| \geqslant - E\xi$.
    \end{enumerate}
\end{proof}

Следующая теорема даёт правило вычисления математического ожидания случайных
величин.

\begin{theorem}
    Пусть $\xi_1, \xi_2, \ldots, \xi_n$ — случайные величины, определённые на
    одном вероятностном пространстве. Обозначим через $\xi$ случайные вектор
    $\xi = (\xi_1, \ldots, \xi_n)$. Рассмотрим также измеримую функцию
    $f: \real^n \to \real$. Тогда:
    $$
    Ef(\xi) = \int\limits_{\real^n} f(x_1, \ldots, x_n)
    \mathfrak P_\xi (\dif x_1, \ldots, \dif x_n) =
    \int\limits_{\real^n} f(x_1, \ldots, x_n) \dif F_\xi(x_1, \ldots, x_n)
    $$
\end{theorem}
\begin{corollary}
    Если $\xi$ — случайная величина с функцией распределения $F_\xi$,
    $f: \real^n \to \real$ — измеримая функция, то
    $$
    Ef(\xi) = \int\limits_\real f(x) \dif F_\xi(x)
    $$
    $$
    E\xi = \int\limits_\real x \dif F_\xi(x)
    $$
    Если известна плотность $p_\xi$, то $E\xi = \int\limits_\real xp_\xi(x)\dif x$
\end{corollary}

\begin{proof}[Доказательство теоремы]
    Рассмотрим два случая.
    \begin{enumerate}
        \item Функция $f$ принимает не более чем счётное число значений. Обозначим
        через $f_1, f_2, \ldots$ все значения функции $f$. Введём множества:
        
        $B_k = f^{-1}(f_k) \subset \real^n$, при этом $B_k \cap B_l = \varnothing$
        ($k \neq l$), $\bigcup\limits_k B_k = \real^n$;
        
        $A_k = \xi^{-1}(B_k) \in \mathfrak F$, $A_k \cap A_l = \varnothing$
        ($k \neq l$), $\bigcup\limits_k A_k = \Omega$.
        
        Рассмотрим математическое ожидание $Ef(\xi)$. По определению:
        $$
        Ef(\xi) = \int\limits_\Omega f(\xi) \dif P =
        \sum\limits_k \int\limits_{A_k} f(\xi) \dif P =
        \sum\limits_k \int\limits_{A_k} f_k \dif P =
        \sum\limits_k f_k \int\limits_{A_k} \dif P = \sum\limits_k f_k P(A_k) =
        $$
        $$
        = \sum\limits_k f_k \mathfrak P_\xi (B_k) =
        \sum\limits_k f_k \int\limits_{B_k}
        \mathfrak P_\xi (\dif x_1, \ldots, \dif x_n) =
        \sum\limits_k \int\limits_{B_k} f_k P_\xi (\dif x_1, \ldots, \dif x_n) =
        $$
        $$
        = \sum\limits_k \int\limits_{B_k} f(x_1, \ldots, x_n)
        \mathfrak P_\xi (\dif x_1, \ldots, \dif x_n) =
        \int\limits_{\real^n} f(x_1, \ldots, x_n)
        \mathfrak P_\xi (\dif x_1, \ldots, \dif x_n)
        $$
        \item Функция $f$ — произвольная.
        Возьмём $m \geqslant 1$. Определим $f^m$ следующим образом:
        $$
        f^m(x_1, \ldots, x_n) = \frac{k}{m} \text{, если }
        \frac{k}{m} \leqslant f(x_1, \ldots, x_n) \leqslant \frac{k+1}{m}
        $$
        То есть, мы будем приближать функцию $f$ ступенчатой функцией:
        $$
        \big|f(x_1, \ldots, x_n) - f^m(x_1, \ldots, x_n)\big| < \frac{1}{m}
        $$
        Отсюда:
        $$
        \big|Ef(\xi) - Ef^m(\xi)\big| = \big|E(f(\xi) - f^m(\xi))\big| \leqslant
        E\big|f(\xi) - f^m(\xi)\big| \leqslant \frac{1}{m}
        $$
        $$
        \Bigg|\int\limits_{\real^n} f(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n) -
        \int\limits_{\real^n} f^m(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\Bigg| \leqslant
        $$
        $$
        \leqslant \int\limits_{\real^n}\big|f(x_1, \ldots, x_n)-f^m(x_1, \ldots, x_n)\big|
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n) \leqslant
        \frac{1}{m} \int\limits_{\real^n}\mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)
        \leqslant\frac{1}{m}
        $$
        В последнем неравенстве мы пользуемся тем, что интеграл на $\real^n$ от единицы
        по вероятностной мере равен единице (теорема \ref{distribution_is_probability}).
        Рассмотрим модуль:
        $$
        \Bigg| Ef(\xi) - \int\limits_{\real^n} f(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\Bigg|
        $$
        Прибавим и вычтем в выражении под знаком модуля
        $$
        \int\limits_{\real^n} f^m(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)
        $$
        Получим:
        $$
        \Bigg| Ef(\xi) - \int\limits_{\real^n} f(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\Bigg| \leqslant
        $$
        $$
        \leqslant\Bigg| Ef(\xi) - \int\limits_{\real^n} f^m(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\Bigg| +
        $$
        $$
        +\Bigg|\int\limits_{\real^n}f^m(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n) -
        \int\limits_{\real^n}f(x_1, \ldots, x_n)
        \mathfrak P_\xi(\dif x_1, \ldots, \dif x_n)\Bigg| \leqslant
        \frac{1}{m} + \frac{1}{m} = \frac{2}{m}
        $$
        Поскольку это выполнено для любого сколь угодно болььшого $m$,
        исходная разность равна нулю
        и теорема доказана.
    \end{enumerate}
\end{proof}

\begin{theorem}[Следствие для произведения независимых случайных величин]
    \label{expectation_for_independent_variables}
    Пусть $\xi, \eta$ — независимые случайные величины и, кроме того,
    существуют $E\xi$ и $E\eta$. Тогда $E(\xi\eta) = E\xi \cdot E\eta$.
\end{theorem}
\begin{proof}
    Рассмотрим случайные вектор $(\xi, \eta)$.
    $$
    F_{(\xi, \eta)}(x, y) = F_\xi(x)F_\eta(y)
    $$
    $$
    \mathfrak P_{(\xi, \eta)} = \mathfrak P_\xi \cdot \mathfrak P_\eta
    $$
    Возьмём функцию $f: \real^2 \to \real$: $f(x, y) = xy$.
    $$
    Ef(\xi, \eta) = E(\xi\eta) = \iint\limits_{\real^2} x_1x_2
    \mathfrak P_{(\xi, \eta)}(\dif x_1, \dif x_2) =
    \iint\limits_{\real^2} x_1x_2 \mathfrak P_\xi(\dif x_1)
    \mathfrak P_\eta(\dif x_2) =
    $$
    $$
    = \int\limits_{\real} x_1\mathfrak P_\xi(\dif x_1) \cdot
    \int\limits_{\real} x_2\mathfrak P_\eta(\dif x_2) =
    E\xi \cdot E\eta
    $$
\end{proof}

\subsection{Дисперсия}

\begin{definition}
    Дисперсией случайной величины $\xi$ называется число
    $D\xi = E(\xi - E\xi)^2$, если оно существует.
\end{definition}

Дисперсия представляет собой среднеквадратичное отклонение случайной величины
от своего среднего значения.

\begin{proposition}[Свойства дисперсии]
    \mbox{}
    \begin{enumerate}
        \item $D\xi \geqslant 0$. $D\xi = 0 \iff  \exists c:$ $P(\xi = c) = 1$
        \item Если $a, b \in \real$, то $D(a\xi + b) = a^2D\xi$
        \item Если $\xi, \eta$ — случайные величины, то $D(\xi + \eta) =
        D\xi + D\eta + 2E((\xi - E\xi)(\eta - E\eta))$.
        \item Если $\xi, \eta$ — независимые случайные величины, то
        $D(\xi+ \eta) = D\xi + D\eta$. Обратное, вообще говоря, неверно.
        \item $D\xi = E\xi^2 - (E\xi)^2$
    \end{enumerate}
\end{proposition}
\begin{definition}
    Величина $E((\xi - E\xi)(\eta - E\eta))$ называется \emph{ковариацией}
    случайных величин $\xi$ и $\eta$. Обозначение: $\cov(\xi, \eta)$.
\end{definition}

\begin{proof}[Доказательство предложения]
\mbox{}
    \begin{enumerate}
        \item Неотрицательность следует из свойств математического ожидания.
        Далее, $D\xi = \int\limits_\Omega(\xi(\omega) - E\xi)^2\dif P(\omega) =
        0$. Это означает, что $P(\xi - E\xi = 0) = 1$, возьмём $c = E\xi$.
        \item $D(a\xi + b) = E(a\xi + b - E(a\xi + b))^2 =
        E(a\xi + b - aE\xi - b)^2 = a^2E(\xi - E\xi)^2 = a^2D\xi$.
        \item $D(\xi + \eta) = E(\xi + \eta - E\xi - E\eta)^2 = E(\xi - E\xi)^2 +
        E(\eta - E\eta)^2 + 2E((\xi - E\xi)(\eta - E\eta))$
        \item По теореме \ref{functions_of_independent_variables} $\xi - E\xi$ и
        $\eta - E\eta$ будут независимыми, отсюда
        $E((\xi - E\xi)(\eta - E\eta)) = E(\xi - E\xi)\cdot E(\eta - E\eta) = 0$.
        Подставим полученный результат в предыдущее свойство.
        \item $D\xi = E(\xi - E\xi)^2 = E(\xi^2 - 2\xi E\xi + (E\xi)^2) =
        E\xi^2 - 2(E\xi)^2 + E(E\xi)^2 = E\xi^2 - (E\xi)^2$.
    \end{enumerate}
\end{proof}

\begin{examples}
\mbox{}
    \begin{enumerate}
        \item $(\xi, p): (c, 1)$ (вырожденное распределение). Тогда $E\xi = c$,
        $D\xi = 0$.
        \item Если $(\xi, p): (0, 1-p), (1, p)$, то $E\xi = p$, $D\xi = p(1-p)$.
        \item $\xi \in B(n, p)$ — случайная величина с биномиальным законом
        распределения, то есть $p(\xi = k) = C_n^kp^k(1-p)^{n-k}$. Для этой величины:
        $$
        E\xi = \sum\limits_{k=0}^n k\cdot C_n^kp^k(1-p)^{n-k} =
        \sum\limits_{k=1}^n k\cdot C_n^kp^k(1-p)^{n-k} =
        \sum\limits_{k=1}^n \frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k} =
        $$
        $$
        = np\sum\limits_{k=1}^n \frac{(n-1)!}{(k-1)!(n-1-(k-1))!}
        p^{k-1}(1-p)^{n-1-(k-1)}
        $$
        Взяв $l = k-1$, получим
        $E\xi = np\sum\limits_{l=0}^{n-1} C_{n-1}^l p^l(1-p)^{n-1 - l} =
        np(p + (1-p))^{n-1} = np$ (здесь мы использовали бином Ньютона).
        $D\xi = np(1-p)$.
        \item Рассмотрим $\xi \in \Pi(\lambda)$ — случайную величину с 
        пуассоновским законом распределения: $P(\xi = k) =
        \frac{\lambda^k}{k!}e^{-\lambda}$.
        $$E\xi = \sum\limits_{k=0}^\infty k\frac{\lambda^k}{k!}e^{-\lambda} =
        \sum\limits_{k=1}^\infty \frac{\lambda^k}{(k-1)!}e^{-\lambda} =
        \lambda e^{-\lambda}\sum\limits_{k=1}^\infty
        \frac{\lambda^{k-1}}{(k-1)!} =
        \lambda
        $$
        Для вычисления дисперсии вычислим сначала $E\xi^2$:
        $$
        E\xi^2 = \sum\limits_{k=0}^\infty k^2\frac{\lambda^k}{k!}e^{-\lambda} =
        \sum\limits_{k=1}^\infty k\frac{\lambda^k}{(k-1)!}e^{-\lambda} =
        $$
        $$
        = \sum\limits_{k=1}^\infty (k-1)\frac{\lambda^k}{(k-1)!}e^{-\lambda} +
        \sum\limits_{k=1}^\infty\frac{\lambda^k}{(k-1)!}e^{-\lambda} =
        \sum\limits_{k=2}^\infty \frac{\lambda^k}{(k-2)!}e^{-\lambda} +
        \lambda e^{-\lambda}
        \sum\limits_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!} =
        $$
        $$
        = \lambda^2 e^{-\lambda}
        \sum\limits_{k=2}^\infty\frac{\lambda^{k-2}}{(k-2)!} + \lambda = \lambda^2 + \lambda
        $$
        С учётом $(E\xi)^2 = \lambda^2$ получаем $D\xi = \lambda$.
        \item Для $\xi \in U_{[a,b]}$ математическое ожидание будет:
        $$
        E\xi =
        \int\limits_a^b x\frac{1}{b-a}\dif x = \frac{a + b}{2}
        $$
        $$
        E\xi^2 =
        \int\limits_a^b x^2\frac{1}{b-a}\dif x = \frac{b^3 - a^3}{3(b-a)} =
        \frac{a^2 + ab + b^2}{3}
        $$
        Тогда $D\xi = E\xi^2 - (E\xi)^2 = \frac{(b-a)^2}{12}$.
        \item $\xi \in N(a, \sigma^2)$ — случайная величина с нормальным
        законом распределения ($p_\xi(x) =
        \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-a)^2}{2\sigma^2}}$).
        $$
        E\xi = \int\limits_{-\infty}^{+\infty}x\frac{1}{\sqrt{2\pi\sigma^2}}
        e^{-\frac{(x-a)^2}{2\sigma^2}}\dif x \undereq{x = \sigma y + a}
        \int\limits_{-\infty}^{+\infty}
        (\sigma y + a)\frac{1}{\sqrt{2\pi\sigma^2}}
        e^{-\frac{y^2}{2}}\sigma\dif y =
        $$
        $$
        =\underbrace{\sigma \int\limits_{-\infty}^{+\infty}
        y\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}} \dif y}_0 +
        a\int\limits_{-\infty}^{+\infty}
        \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}} \dif y = a
        $$
        $$
        D\xi = \int\limits_{-\infty}^{+\infty}
        (x-a)^2\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-a)^2}{2\sigma^2}} \dif x
        \undereq{x = \sigma y + a} \int\limits_{-\infty}^{+\infty}
        \sigma^2 y^2 \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{y^2}{2}}\sigma \dif y =
        $$
        $$
        =\sigma^2 \int\limits_{-\infty}^{+\infty}
        y^2 \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}\dif y
        = \sigma^2 \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{+\infty}
        y\cdot ye^{-\frac{y^2}{2}}\dif y =
        $$
        $$
        = \sigma^2 \frac{1}{\sqrt{2\pi}}
        \bigg[ -ye^{-\frac{y^2}{2}}\bigg|_{-\infty}^{+\infty} +
        \int\limits_{-\infty}^{+\infty}e^{-\frac{y^2}{2}}\dif y\bigg] = \sigma^2
        $$
        \item $\xi$ — случайная величина с распределением Коши
        ($p_\xi(x) = \frac{1}{\pi(1 + x^2)}$).
        $\int\limits_{-\infty}^{+\infty} \frac{x}{\pi(1 + x^2)}\dif x$ — этот 
        интеграл расходится, значит, математического ожидания не существует.
    \end{enumerate}
\end{examples}

\subsection{Моменты случайных величин}

\begin{definition}
    Пусть $k$ — целое неотрицательное число, $\xi$ — случайная величина.
    Число $E\xi^k$, если оно существует, называется $k$-м моментом $\xi$.
    Число $E|\xi|^k$ — абсолютный $k$-й момент $\xi$.
    Число $E(\xi - E\xi)^k$ — центральный $k$-й момент $\xi$.
    Число $E|\xi - E\xi|^k$ — абсолютный центральный $k$-й момент $\xi$.
\end{definition}

Заметим, что все $k$-е моменты величины $\xi$ существуют или отсутствуют одновременно.

\begin{proposition}[Свойства моментов]
\mbox{}
    \begin{enumerate}
        \item Пусть $n > m$ и существует $E\xi^n$. Тогда существует и $E\xi^m$.
        \item (Неравенство Гёльдера) Если $p, q > 0$ и
        $\frac{1}{p} + \frac{1}{q} = 1$, то $E|\xi\eta| \leqslant
        (E|\xi|^p)^\frac{1}{p} \cdot (E|\eta|^q)^\frac{1}{q}$.
        \item (Неравенство Коши-Буняковского-Шварца)
        $E|\xi\eta| \leqslant \sqrt{E\xi^2} \cdot \sqrt{E\eta^2}$.
        \item (Неравенство Ляпунова) Пусть $0 < p < q$. Тогда
        $(E|\xi|^p)^\frac{1}{p} \leqslant (E|\xi|^q)^\frac{1}{q}$.
    \end{enumerate}
\end{proposition}
\begin{proof}
\mbox{}
    \begin{enumerate}
        \item Ясно, что $|x|^m \leqslant |x|^n + 1$ для любого $x$. То есть,
        $|\xi|^m \leqslant |\xi|^n + 1$ и
        $E|\xi|^m \leqslant E|\xi|^n + 1 < \infty$.
        \item Очевидно из неравенства Гёльдера для интегралов.
        \item Это свойство — частный случай предыдущего при $p = q = \frac{1}{2}$.
        \item Данное неравенство равносильно
        $E|\xi|^p \leqslant (E|\xi|^q)^\frac{p}{q}$. С учётом
        $\frac{p}{q} + \frac{q - p}{q} = 1$, применим неравенство Гёльдера:
        $$
        E(|\xi|^p \cdot 1) \leqslant (E|\xi|^{p \cdot \frac{q}{p}})^\frac{p}{q}\cdot
        (E (1^\frac{q}{q - p}))^\frac{q - p}{q} = (E|\xi|^q)^\frac{p}{q}
        $$
    \end{enumerate}
\end{proof}

\begin{definition}
    Пусть $m,n > 0$. Смешанным моментом величин $\xi$, $\eta$ порядка $m$ плюс $n$ 
    называется число $E\xi^m\eta^n$, если оно существует. Смешанным центральным 
    моментом $\xi$, $\eta$ порядка $m$ плюс $n$ называется число
    $E(\xi - E\xi)^m(\eta - E\eta)^n$.
\end{definition}
\begin{remark}
    Таким образом, $\cov (\xi, \eta)$ является смешанным центральным моментом
    величин порядка $1$ плюс $1$.
\end{remark}

\subsection{Ковариация. Коэффициент корреляции}

Рассматриваем вероятностное пространство $(\Omega, \mathfrak F, P)$.

Вспомним, что если $\xi, \eta$ — случайные величины, то их ковариацией $\cov (\xi, \eta)$
называется число $E(\xi - E\xi)(\eta - E\eta)$.
\begin{proposition}[Свойства ковариации]
\mbox{}
    \begin{enumerate}
        \item $\cov(\xi, \eta) = E\xi\eta - E\xi E\eta$
        \item Если $\alpha, \beta \in \real$, то $\cov(\alpha\xi + \beta, \eta) =
        \alpha\cov(\xi, \eta)$
        \item Если $\xi, \eta$ — независимые, то $\cov(\xi, \eta) = 0$. Обратное,
        вообще говоря, неверно.
    \end{enumerate}
\end{proposition}
\begin{definition}
    Пусть существуют $D\xi > 0$ и $D\eta > 0$. Тогда число $\rho(\xi, \eta) =
    \frac{\cov(\xi, \eta)}{\sqrt{D\xi\cdot D\eta}}$ называется \emph{коэффициентом
    корреляции} случайных величин $\xi, \eta$.
\end{definition}

Заметим:
$$
\rho(\xi, \eta) =
\frac{E(\xi - E\xi)(\eta - E\eta)}{\sqrt{D\xi\cdot D\eta}} =
E\frac{\xi - E\xi}{\sqrt{D\xi}}\cdot \frac{\eta - E\eta}{\sqrt{D\eta}}
$$

\begin{proposition}[Свойства коэффициента корреляции]
\mbox{}
    \begin{enumerate}
        \item $-1 \leqslant \rho(\xi, \eta) \leqslant 1$
        \item Если $\xi, \eta$ — независимые, то $\rho(\xi, \eta) = 0$. Обратное,
        вообще говоря, неверно.
        \item $|\rho(\xi, \eta)| = 1 \iff \exists a,b \in \real:$
        $P(\xi = a\eta + b) = 1$ и $a\rho(\xi, \eta) > 0$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    Ясно, что $E\frac{\xi - E\xi}{\sqrt{D\xi}} = 0$. Отсюда:
    $$
    D\frac{\xi - E\xi}{\sqrt{D\xi}} =
    E\bigg(\frac{\xi - E\xi}{\sqrt{D\xi}}\bigg)^2 =
    \frac{1}{D\xi}E(\xi - E\xi)^2 = 1
    $$
    
    Докажем свойства, пользуясь этими фактами.
    \begin{enumerate}
        \item
        $$
        |\rho(\xi, \eta)| = \bigg|E\frac{\xi - E\xi}{\sqrt{D\xi}}\cdot
        \frac{\eta - E\eta}{\sqrt{D\eta}}\bigg| \leqslant
        E\bigg|\frac{\xi - E\xi}{\sqrt{D\xi}}\bigg|\cdot
        \bigg|\frac{\eta - E\eta}{\sqrt{D\eta}}\bigg| \leqslant
        $$
        $$
        \leqslant
        \sqrt{E\bigg(\frac{\xi - E\xi}{\sqrt{D\xi}}\bigg)^2}
        \sqrt{E\bigg(\frac{\eta - E\eta}{\sqrt{D\eta}}\bigg)^2} = 1
        $$
        (Мы воспользовались неравенством Коши-Буняковского-Шварца.)
        \item Очевидно.
        \item Пусть $\rho(\xi, \eta) = 1$. Тогда:
        $$
        D\bigg(\frac{\xi - E\xi}{\sqrt{D\xi}} -
        \frac{\eta - E\eta}{\sqrt{D\eta}}\bigg) =
        E\bigg(\frac{\xi - E\xi}{\sqrt{D\xi}} -
        \frac{\eta - E\eta}{\sqrt{D\eta}}\bigg)^2 =
        $$
        $$
        =
        E\bigg(\frac{\xi - E\xi}{\sqrt{D\xi}}\bigg)^2 +
        E\bigg(\frac{\eta - E\eta}{\sqrt{D\eta}}\bigg)^2 -
        2E\frac{\xi - E\xi}{\sqrt{D\xi}} \cdot \frac{\eta - E\eta}{\sqrt{D\eta}} =
        2 - 2\rho(\xi, \eta) = 0
        $$
        Это означает, что распределение случайной величины вырожденное, то есть, существует такое
        $c \in \real$, что:
        $$
        P\bigg(\frac{\xi - E\xi}{\sqrt{D\xi}} -
        \frac{\eta - E\eta}{\sqrt{D\eta}} = c\bigg) = 1
        $$
        Откуда домножением на $\sqrt{D\xi}$ и переносом нужных слагаемых в правую часть получим:
        $$
        P\bigg(\xi = \frac{\sqrt{D\xi}}{\sqrt{D\eta}}\eta + b\bigg) = 1
        $$
        Аналогично при $\rho(\xi, \eta) = -1$ будет:
        $$
        D\bigg(\frac{\xi - E\xi}{\sqrt{D\xi}} +
        \frac{\eta - E\eta}{\sqrt{D\eta}}\bigg) = 0
        $$
        И получаем:
        $$
        P\bigg(\xi = -\frac{\sqrt{D\xi}}{\sqrt{D\eta}}\eta + b\bigg) = 1
        $$
    \end{enumerate}
\end{proof}

\begin{examples}
\mbox{}
    \begin{enumerate}
        \item $\xi, \eta, \tau$ — независимые случайные величины.
        $\xi \in U_{[-1, 0]}$,
        $\eta \in N(0, 2)$, $\tau \in B(4, \frac{1}{2})$.
        $$
        \rho(\xi + \tau, \eta + \tau) =
        \frac{E(\xi + \tau)(\eta + \tau) -
        E(\xi + \tau)E(\eta + \tau)}{\sqrt{D(\xi + \tau)D(\eta + \tau)}} =
        $$
        $$
        = \frac{E\xi E\eta + E\xi E\tau + E\tau E\eta + E\tau^2 - E\xi E\eta -
        E\xi E\tau - E\tau E\eta - (E\tau)^2}{\sqrt{(D\xi + D\tau)(D\eta + 
        D\tau)}} =
        $$
        $$
        = \frac{D\tau}{\sqrt{(D\xi + D\tau)(D\eta + D\tau)}} =
        \frac{1}{\sqrt{\big(\frac{1}{12} + 1\big)\big(2 + 1\big)}}
        \approx 0.555
        $$
        \item Рассмотрим случайный вектор $(\xi, \eta)$ с законом распределения:
        \begin{center}
            \begin{tabular}{| c | c | c |}
                \hline
                $\eta\backslash\xi$ & $0$ & $1$ \\ \hline
                $-1$ & $0.4$ & $0.1$ \\ \hline
                $1$ & $0.3$ & $0.2$ \\ \hline
            \end{tabular}
        \end{center}
        Найдём законы распределения каждой величины:
        \begin{center}
            \begin{tabular}{| c | c | c |}
                \hline
                $\xi$ & $0$ & $1$ \\ \hline
                $p_\xi$ & $0.7$ & $0.3$ \\ \hline
            \end{tabular}
            \begin{tabular}{| c | c | c |}
                \hline
                $\eta$ & $-1$ & $1$ \\ \hline
                $p_\eta$ & $0.5$ & $0.5$ \\ \hline
            \end{tabular}
        \end{center}
        
        $$
        E\xi\eta = -0.1 + 0.2 = 0.1
        $$
        $$
        \rho(\xi, \eta) = \frac{E\xi\eta - E\xi E\eta}{\sqrt{D\xi D\eta}} =
        \frac{0.1 - 0.3 \cdot 0}{\sqrt{0.21 \cdot 1}} = \frac{0.1}{\sqrt{0.21}}
        \approx 0.218
        $$
    \end{enumerate}
\end{examples}

\subsection{Неравенства Маркова и Чебышёва}

Пусть $\xi$ — неотрицательная случайная величина.
\begin{lemma}[Неравенство Маркова]
    Для любого $t > 0$ выполнено $P(\xi \geqslant t) \leqslant \frac{E\xi}{t}$.
\end{lemma}
\begin{proof}
    Рассмотрим $E\xi$ и распишем его по определению:
    $$
    E\xi = \int\limits_\Omega \xi(\omega) \dif P(\omega) =
    \int\limits_{\omega:\, \xi(\omega)\geqslant t} \xi(\omega)\dif P(\omega) +
    \underbrace{\int\limits_{\omega:\, \xi(\omega) < t}
    \underbrace{\xi(\omega)}_{\geqslant 0} \dif P(\omega)}_{\geqslant 0} \geqslant
    $$
    $$
    \geqslant \int\limits_{\xi(\omega) \geqslant t} \xi(\omega)\dif P(\omega)
    \geqslant \int\limits_{\xi(\omega) \geqslant t} t\dif P(\omega) =
    t\int\limits_{\xi(\omega) \geqslant t} \dif P(\omega) =
    tP(\xi \geqslant t)
    $$
\end{proof}
\begin{corollary}[Неравенство Чебышёва]\label{chebyshevs_inequality}
    Пусть существует $D\xi$. Тогда для любых $t > 0$ выполнено
    $P(|\xi - E\xi| \geqslant t) \leqslant \frac{D\xi}{t^2}$.
\end{corollary}
\begin{proof}
    $$
    P(|\xi - E\xi| \geqslant t) = P\big((\xi - E\xi)^2 \geqslant t^2\big) \leqslant
    \frac{E(\xi - E\xi)^2}{t^2} = \frac{D\xi}{t^2}
    $$
\end{proof}

\subsection{Виды сходимости случайных величин}

Пусть $\xi$ — случайная величина, а
$\xi_1, \xi_2, \ldots$ — последовательность случайных величин на 
вероятностном пространстве $(\Omega, \mathfrak F, P)$.

\begin{definition}[Сходимость по вероятности]\label{convergence_in_probability}
    Последовательность $\{\xi_i\}_{i = 1}^\infty$ называется 
    \emph{сходящейся по вероятности} (обозначается $\xi_n \overto{P} \xi$), если
    $\forall \varepsilon > 0$ $P(|\xi_n - \xi| \geqslant \varepsilon)
    \underto{n \to \infty} 0$.    
\end{definition}

\begin{definition}[Сходимость почти наверное]\label{almost_sure_convergence}
    Последовательность $\{\xi_i\}_{i = 1}^\infty$ называется 
    \emph{сходящейся почти наверное} (обозначается $\xi_n \to \xi$), если
    $P\Big(\lim\limits_{n \to \infty} \xi_n = \xi\Big) = 1$.
\end{definition}

\begin{definition}[Сходимость в среднем]\label{convergence_in_mean}
    Последовательность $\{\xi_i\}_{i = 1}^\infty$ называется 
    \emph{сходящейся в среднем порядка $r > 0$}
    (обозначается $\xi_n \overto{(r)} \xi$), если
    $E|\xi_n - \xi|^r \underto{n \to \infty} 0$.
\end{definition}

\begin{definition}[Сходимость по распределению]\label{convergence_in_dostribution}
    Последовательность $\{\xi_i\}_{i = 1}^\infty$ называется 
    \emph{сходящейся по распределению}, или \emph{слабо сходящейся}
    (обозначается $\xi_n \overto{d} \xi$), если
    для всех $x$ — точек непрерывности $F_\xi$ выполнено
    $F_{\xi_n}(x) \to F_\xi (x)$.
\end{definition}

\begin{theorem}
    Из сходимости почти наверное или в среднем следует сходимость по вероятности;
    из сходимость по вероятности следует сходимость по распределению.
\end{theorem}
$$
\begin{tikzcd}
    \xi_n \to \xi \arrow[Rightarrow]{rd} && \\
    & \xi_n \overto{P} \xi \arrow[Rightarrow]{r}  & \xi_n \overto{d} \xi \\
    \xi_n \overto{(r)} \xi \arrow[Rightarrow]{ru} &&
\end{tikzcd}
$$

\begin{proof}
\mbox{}
    \begin{enumerate}
        \item
        ($\xi_n \overto{(r)} \xi \implies \xi_n \overto{P} \xi$)
        Рассмотрим произвольное $\varepsilon > 0$.
        $$
        P(|\xi_n - \xi| \geqslant \varepsilon) =
        P(|\xi_n - \xi|^r \geqslant \varepsilon^r) \leqslant
        \frac{E|\xi_n - \xi|^r}{\varepsilon^r} \underto{n \to \infty} 0
        $$
        \item
        ($\xi_n \overto{P} \xi \implies \xi_n \overto{d} \xi$)
        Пусть $\tau, \eta$ — случайные величины. Заметим, что верно включение:
        $$
        (\eta < x) \subset \big(|\eta - \tau| \geqslant \varepsilon\big) \cup
        (\tau < x + \varepsilon )
        $$
        Отсюда:
        $$
        F_\eta(x) \leqslant P\big(|\eta - \tau| \geqslant \varepsilon\big) +
        F_\tau(x + \varepsilon)
        $$
        Пусть теперь $x$ — точка непрерывности $F_\xi$.
        $$
        F_{\xi_n}(x) \leqslant P\big(|\xi_n - \xi| \geqslant \varepsilon\big) +
        F_\xi(x + \varepsilon) \implies
        \lim\limits_{n \to \infty} \sup F_{\xi_n}(x) \leqslant
        F_\xi(x - \varepsilon)
        $$
        $$
        F_\xi(x - \varepsilon) \leqslant P\big(|\xi_n - \xi| \geqslant \varepsilon\big) +
        F_{\xi_n}(x) \implies
        F_\xi(x - \varepsilon) \leqslant
        \lim\limits_{n \to \infty} \inf F_{\xi_n}(x)
        $$
        То есть:
        $$
        F_\xi(x - \varepsilon) \leqslant
        \lim\limits_{n \to \infty} \inf F_{\xi_n}(x) \leqslant
        \lim\limits_{n \to \infty} \sup F_{\xi_n}(x) \leqslant
        F_\xi(x - \varepsilon)
        $$
        Устремив $\varepsilon$ к нулю, получим:
        $$
        F_\xi(x) \leqslant
        \lim\limits_{n \to \infty} \inf F_{\xi_n}(x) \leqslant
        \lim\limits_{n \to \infty} \sup F_{\xi_n}(x) \leqslant
        F_\xi(x)
        $$
        
        Значит, существует $\lim\limits_{n \to \infty} F_{\xi_n}(x) = F_\xi(x)$.
        \item
        ($\xi_n \to \xi \implies \xi_n \overto{P} \xi$)
        Сходимость почти наверное равносильна тому, что
        $$
        P\bigg(\underbrace{\bigcap_{r=1}^\infty\bigcup_{n=1}^\infty
        \bigcap_{m=n}^\infty
        \Big(|\xi_m - \xi| < \frac{1}{r}\Big)}_{\xi_m \to \xi}\bigg) = 1
        $$
        Или, что то же самое:
        $$
        P\bigg(\bigcup_{r=1}^\infty\bigcap_{n=1}^\infty\bigcup_{m=n}^\infty
        \Big(|\xi_m - \xi| \geqslant \frac{1}{r}\Big)\bigg) = 0
        $$
        $$
        \forall r\geqslant 1\quad
        P\bigg(\bigcap_{n=1}^\infty\underbrace{\bigcup_{m=n}^\infty\Big(|\xi_m - \xi| 
        \geqslant \frac{1}{r}\Big)}_{B_n}\bigg) = 0
        $$
        Заметим, что $B_n \supset B_{n+1}$. Поэтому:
        $$
        \forall r\geqslant1\quad
        \lim\limits_{n \to \infty}P\bigg(\underbrace{\bigcup_{m=n}^\infty
        \Big(|\xi_m - \xi| 
        \geqslant \frac{1}{r}\Big)}_{\sup\limits_{m \geqslant n} |\xi_m - \xi| \geqslant
        \frac{1}{r}}\bigg) = 0
        $$
        Если супремум последовательности превосходит некоторое число, то и любой её элемент
        превосходит это число, именно это означает следующее включение:
        $$
        \Big(|\xi_n - \xi| \geqslant \frac{1}{r}\Big) \subset
        \Big(\sup\limits_{m\geqslant n} |\xi_m - \xi| \geqslant \frac{1}{r}\Big)
        $$
        $$
        \forall r\geqslant 1\quad
        P\Big(|\xi_n - \xi| \geqslant \frac{1}{r}\Big) \underto{n \to \infty} 0
        $$
    \end{enumerate}
\end{proof}

\subsection{Законы больших чисел}

\begin{definition}[<<Слабый>> закон больших чисел]
    Пусть $\xi_1, \xi_2,\ldots$ — последовательность случайных величин, у каждой
    из которых существует математическое ожидание $E\xi_k = a_k$. Говорят,
    что для этой последовательности выполняется закон больших чисел, если:
    $$
    \forall \varepsilon > 0\quad
    P\bigg(\bigg|\frac{1}{n}\sum\limits_{k=1}^n \xi_k -
    \frac{1}{n}\sum\limits_{k=1}^na_k\bigg| \geqslant \varepsilon \bigg)
    \underto{n \to \infty} 0
    $$
    Или, что то же самое:
    $$
    \forall \varepsilon > 0\quad
    P\bigg(\bigg|\frac{1}{n}\sum\limits_{k=1}^n(\xi_k - E\xi_k)\bigg|\geqslant 
    \varepsilon\bigg) \underto{n \to \infty} 0
    $$
    Или:
    $$
    \frac{1}{n}\sum\limits_{k=1}^n(\xi_k - E\xi_k) \overto{P} 0
    $$
\end{definition}

\begin{theorem}[Закон больших чисел Маркова]
    Пусть $\xi_1, \xi_2,\ldots$ — последовательность случайных величин и пусть
    для любого $n \geqslant 1$ существуют $D\sum\limits_{k=1}^n \xi_k$,
    $\frac{1}{n^2}D\sum\limits_{k=1}^n \xi_k \underto{n \to \infty}~0$.
    Тогда выполняется закон больших чисел.
\end{theorem}
\begin{proof}
    Возьмём произвольное $\varepsilon > 0$. Тогда из неравенства Чебышёва
    (следствие \ref{chebyshevs_inequality}):
    $$
    P\bigg(\bigg|\frac{1}{n}\sum\limits_{k=1}^n\xi_k -
    \underbrace{\frac{1}{n}\sum\limits_{k=1}^na_k}_
    {E\frac{1}{n}\sum\limits_{k=1}^n \xi_k}
    \bigg| \geqslant \varepsilon\bigg) \leqslant
    \frac{D\frac{1}{n}\sum\limits_{k=1}^n \xi_k}{\varepsilon^2} =
    \frac{D\sum\limits_{k=1}^n \xi_k}{n^2 \varepsilon^2}
    \underto{n \to \infty} 0
    $$
\end{proof}
\begin{theorem}[Закон большых чисел Чебышёва]
    Пусть $\xi_1, \xi_2,\ldots$ — последовательность независимых случайных величин
    и существует $c > 0$ такое, что $\forall n$ $D\xi_n \leqslant c$. Тогда
    выполняется закон больших чисел.
\end{theorem}
\begin{proof}
    $$
    P\bigg(\bigg|\frac{1}{n}\sum\limits_{k=1}^n\xi_k -
    \frac{1}{n}\sum\limits_{k=1}^na_k\bigg| \geqslant \varepsilon\bigg) \leqslant
    \frac{D\sum\limits_{k=1}^n\xi_k}{n^2\varepsilon^2} =
    \frac{\sum\limits_{k=1}^n D\xi_k}{n^2\varepsilon^2} \leqslant
    \frac{nc}{n^2\varepsilon^2} \underto{n \to \infty} 0
    $$
\end{proof}
\begin{remark}
    Теорема \ref{large_numbers_bernoulli} (Бернулли) является частным случаем
    теоремы Чебышёва.
    Пусть $\eta_1, \eta_2,\ldots$ — последовательность независимых одинаково
    распределённых случайных величин с законом распределения:
    \begin{center}
        \begin{tabular}{| c | c | c |}
            \hline
            $\eta_k$ & $0$ & $1$ \\ \hline
            $p_{\eta_k}$ & $1-p$ & $p$ \\ \hline
        \end{tabular}
    \end{center}
    $$
    E\eta_k = p,\quad D\eta_k = p(1-p) \leqslant 1
    $$
    $$
    P\bigg(\bigg|\frac{1}{n}\sum\limits_{i=1}^n \eta_i - p\bigg| \geqslant 
    \varepsilon\bigg) \to 0
    $$
\end{remark}

\begin{theorem}[Закон больших чисел Хинчина]\label{hinchins_lln}
    Пусть $\xi_1, \xi_2,\ldots$ — последовательность независимых одинаково
    распределённых случайных величин, у которых существует математическое ожидание
    $E_{\xi_k} = a$. Тогда выполняется закон больших чисел.
\end{theorem}

Мы докажем эту теорему позже.

\subsection{Характеристические функции}

\begin{definition}
    Пусть $\xi, \eta$ — случайные величины, определённые на вероятностном
    пространстве $(\Omega, \mathfrak F, P)$. Рассмотрим отображение
    $\xi + i\eta: \Omega \to \complex$ такое, что $\forall \omega$
    $(\xi + i\eta)(\omega) = \xi(\omega) + i\eta(\omega)$. $\xi + i\eta$ 
    называется \emph{комплекснозначной случайной величиной}.
\end{definition}
\begin{definition}
    Математическим ожиданием комплекснозначной величины называется число
    $E\xi + iE\eta$.
\end{definition}
\begin{definition}
    Случайные величины $\xi_1 + i\eta_1, \xi_2 + i\eta_2$ называются независимыми,
    если $\forall C_1, C_2 \subset \complex$ $(\xi_1 + i\eta_1)^{-1}(C_1)$ и
    $(\xi_2 + i\eta_2)^{-1}(C_2)$ — независимые события, или, что то же самое,
    $(\xi_1, \eta_1)$ и $(\xi_2, \eta_2)$ — независимые случайные векторы.
\end{definition}
\begin{definition}
    Рассмотрим функцию $f_\xi: \real \to \complex$ такую, что
    $\forall t$ $f_\xi(t) = Ee^{it\xi}$. Такая функция называется
    \emph{характеристической функцией} случайной величины $\xi$.
\end{definition}

Рассмотрим случай дискретного распределения с законом:
\begin{center}
    \begin{tabular}{| c | c | c | c |}
        \hline
        $\xi$ & $a_1$ & $a_2$ & $\ldots$ \\ \hline
        $p_\xi$ & $p_1$ & $p_2$ & $\ldots$ \\ \hline
    \end{tabular}
\end{center}
Тогда $f_\xi(t) = \sum\limits_k e^{ita_k}p_k$.

В случае абсолютно непрерывного распределения $f_\xi(t) =
\int\limits_{-\infty}^{+\infty} e^{itx}p_\xi(x)\dif x$.

\begin{proposition}[Свойства характеристической функции]
\mbox{}
    \begin{enumerate}
        \item\label{charfunc_limit} $|f_\xi(t)| \leqslant 1$.
        \item $f_\xi(0) = 1$.
        \item $f_{a\xi + \beta}(t) = e^{it\beta}\cdot f_\xi(\alpha\cdot t)$
        ($\alpha, \beta \in \real$).
        \item $f_\xi(-t) = \ol{f_\xi(t)}$.
        \item\label{charfunc_uniformly_continuous}
        $f_\xi(t)$ равномерно непрерывна на $\real$.
        \item\label{charfunc_independent_variables}
        Если $\xi, \eta$ — независимые, то
        $f_{\xi + \eta}(t) = f_\xi(t)f_\eta(t)$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    Свойство \ref{charfunc_limit}:
    $$
    |Ee^{it\eta}| \leqslant E|e^{it\xi}| = E1 = 1
    $$
    Свойство \ref{charfunc_uniformly_continuous}:
    $$
    |f_\xi(t + h) - f_\xi(t)| = |Ee^{i(t+h)\xi} - Ee^{it\xi}| =
    |Ee^{it\xi}(e^{ih\xi} - 1)| \leqslant
    E|\underbrace{e^{ih\xi} - 1}_{\leqslant 2}| \underto{h \to 0} 0
    $$
    Свойство \ref{charfunc_independent_variables}:
    $$
    f_{\xi + \eta}(t) = Ee^{it(\xi + \eta)} = Ee^{it\xi}e^{it\eta} =
    Ee^{it\xi}Ee^{it\eta} = f_\xi(t) + f_\eta(t)
    $$
    Здесь мы воспользовались теоремой \ref{expectation_for_independent_variables}.
\end{proof}

\begin{examples}
\mbox{}
    \begin{enumerate}
        \item $P(\xi = c) = 1$. $f_\xi(t) = e^{itc}$.
        \item $\xi$ имеет распределение Бернулли:
        \begin{center}
            \begin{tabular}{| c | c | c |}
                \hline
                $\xi$ & $0$ & $1$ \\ \hline
                $p_\xi$ & $1-p$ & $p$ \\ \hline
            \end{tabular}
        \end{center}
        $f_\xi(t) = pe^{it} + 1 - p$.
        \item $\xi \in B(n, p)$.
        $f_\xi(t) = \sum\limits_{k=0}^n e^{itk}C_n^kp^k(1-p)^{n-k} =
        (pe^{it} + 1 - p)^n$.
        \item $\xi \in \Pi(\lambda)$.
        $f_\xi(t) = \sum\limits_{k=0}^\infty
        e^{itk} \frac{\lambda^k}{k!}e^{-\lambda} =
        e^{-\lambda}\sum\limits_{k=0}^\infty\frac{(\lambda e^{it})^k}{k!} =
        e^{-\lambda}e^{\lambda e^{it}} = e^{\lambda(e^{it} - 1)}$.
        \item Случайная величина $\xi$ имеет закон распределения:
        \begin{center}
            \begin{tabular}{| c | c | c |}
                \hline
                $\xi$ & $-1$ & $1$ \\ \hline
                $p_\xi$ & $\frac{1}{2}$ & $\frac{1}{2}$ \\ \hline
            \end{tabular}
        \end{center}
        $f_\xi(t) = \cos t$.
        \item $\xi \in U_{[a, b]}$. $f_\xi(t) =
        \int\limits_a^b e^{itx} \frac{1}{b-a}\dif x =
        \frac{e^{itb} - e^{ita}}{it(b-a)}$.
        Если $\xi \in U_{[-a, a]}$, то $f_\xi(t) = \frac{\sin at}{at}$.
        \item $\xi \in N(0, 1)$. $p(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$.
        $$
        f_\xi(t) = \int\limits_{-\infty}^{+\infty}
        e^{itx}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \dif x
        $$.
        $$
        f'_\xi(t) = i\int\limits_{-\infty}^{+\infty}
        xe^{itx}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \dif x =
        \underbrace{-ie^{itx}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
        \bigg|_{-\infty}^{+\infty}}_{0} -
        t\int\limits_{-\infty}^{+\infty}
        e^{itx}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \dif x = -tf_\xi(t)
        $$
        Отсюда $\ln f_\xi(t) = -\frac{t^2}{2} + C$, $f_\xi(t) = e^{-\frac{t^2}{2}}
        \cdot e^C$, $C = 0$. Таким образом, $f_\xi(t) = e^{-\frac{t^2}{2}}$
        \item $\xi \in N(a, \sigma^2)$.
        Тогда $\frac{\xi - a}{\sigma} \in N(0, 1)$.
        $f_{\frac{\xi - a}{\sigma}}(t) = e^{-\frac{t^2}{2}}$ и $f_\xi(t) =
        e^{ita}e^{-\frac{t^2\sigma^2}{2}}$.
    \end{enumerate}
\end{examples}

Мы выяснили, что каждой функции распределения соответствует единственная функция
распределения. Покажем теперь, что верно и обратное.

\subsection{Формула обращения и теорема единственности}

Рассмотрим функцию $f: \real \to \real$, $f \in L_1$ , т. е. суммируемую.
\emph{Преобразованием Фурье} функции $f$ называется функция
%$\check{f}$

$\ldots$

\begin{theorem}[Формула обращения]
    Пусть $\xi$ — случайная величина с функцией распределения $F_\xi$ и
    характеристической функцией $f_\xi(t)$. Пусть также $x \in \real$ — 
    произвольное число, $l > 0$ и $x + l$, $x - l$ — точки непрерывности функции
    $F_\xi$. Тогда:
    $$
    F_\xi(x+l) - F_\xi(x-l) =
    \lim\limits_{\sigma \to 0} \frac{1}{\pi} \int\limits_{-\infty}^{+\infty}
    e^{itx} f_\xi(t) \frac{\sin tl}{t}e^{-\frac{t^2\sigma^2}{2}} \dif t
    $$
\end{theorem}

\begin{theorem}[Единственности]
    Разным функциям распределения $F$ и $G$ соответствуют разные 
    характеристические функции $f$ и $g$.
\end{theorem}
\begin{proof}
    Докажем теорему от противного. Пусть $F \to f$, $G \to f$, то есть двум 
    различным функция распределения соответствует одна и та же характеристическая
    функция. Рассмотрим $D_F$ — множество точек непрерывности функции $F$ и
    $D_G$ — множество точек непрерывности функции $G$. Заметим, что $D_F$ и $D_G$ 
    — всюду плотные, то есть $\ol{D_F} = \ol{D_G} = \real$. Тогда и $D_G \cap D_F$
    — всюду плотное множество на $\real$. Чтобы воспользоваться формулой 
    обращения, возьмём $a$, $b$ ($a < b$) — точки непрерывности $F$. Тогда:
    $$
    F(b) - F(a) = \lim\limits_{\sigma \to 0} \frac{1}{\pi}
    \int\limits_{-\infty}^{\infty} e^{-it\frac{a+b}{2}}f(t)
    \frac{\sin t\frac{a + b}{2}}{t}e^{-\frac{t^2\sigma^2}{2}}\dif t
    $$
    Выбрав такие $a, b \in D_G \cap D_F$, получим:
    $F(b) - F(a) = G(b) - G(a)$. Устремив $a$ к $-\infty$, получим $F(b) = G(b)$,
    это выполняется для всех точек непрерывности.
\end{proof}

\subsection{Связь характеристических функций и моментов случайных величин}

\begin{theorem}[О связи моментов и характеристических функций]
    Пусть $\xi$ — случайная величина, у которой существует момент порядка $n$.
    Тогда существует $f_\xi^{(n)}$ и $f_\xi^{(n)}(0) = i^n E\xi^n$.
\end{theorem}
\begin{proof}
    Будем доказывать по индукции.
    Пусть $n \geqslant 1$ и существует момент порядка $n$. Докажем сначала,
    что существует первая производная. Рассмотрим функцию
    $\frac{f_\xi(t + h) - f_\xi(t)}{h}$ и покажем, что у неё существует предел
    при $h \to 0$.
    $$
    \frac{f_\xi(t + h) - f_\xi(t)}{h} = E\frac{1}{h}(e^{i(t+h)\xi} - e^{}it\xi) =
    E\frac{1}{h}(e^{ih\xi} - 1)e^{it\xi}
    $$
    $$
    \bigg|\frac{1}{h}(e^{ig\xi} - 1)\bigg| \leqslant \frac{2}{h}
    $$
    $$
    \frac{1}{h}(e^{ih\xi} - 1) \underto{h \to 0} i\xi
    $$
    Отсюда по теореме Лебега:
    $$
    E\frac{1}{h}(e^{ih\xi} - 1)e^{it\xi} \underto{h \to 0} iE\xi e^{it\xi}
    $$
    То есть производная существует и равна:
    $$
    f'_\xi(t) = iE\xi e^{it\xi}
    $$
    
    Пусть теперь для любого $k \leqslant n - 1$ существует
    $f_\xi^{(k)}(t) = i^k E\xi^k e^{it\xi}$.
    $$
    \frac{f_\xi^{(n-1)}(t + h) - f_\xi^{(n-1)}(t)}{h} =
    i^{n-1}E\frac{1}{h}\xi^{n-1}(e^{i(t+h)\xi} - e^{it\xi}) =
    $$
    $$
    = i^{n-1}E\frac{e^{ih\xi} - 1}{h} \xi^{n-1} e^{it\xi} \underto{h \to 0}
    i^nE\xi^n e^{it\xi}
    $$
\end{proof}
\begin{corollary}
    Пусть существует момент порядка $n$ случайной величины $\xi$, который мы
    обозначим $E\xi^n = m_n$. Тогда:
    $$
    f_\xi(t) = \sum\limits_{k=0}^n \frac{(it)^k}{k!}m_k + o(t^n)
    $$
\end{corollary}

\subsection{Непрерывность соответствия между характеристической функцией и функцией
распределения}

Рассмотрим множество $\mathfrak F$ всех функций распределения и множество
$\mathfrak f$ всех характеристических функций. Мы установили, что между этими
множествами существует биекция. При этом последовательность функций распределения,
вообще говоря, не обязательно сходится к функции, являющейся функцией 
распределения. Аналогичное верно и для характеристических функций.

\begin{theorem}
    Пусть $F, F_1, F_2, \ldots$ — последовательность функций распределения,
    $f, f_1, f_2, \ldots$ — последовательность соответствующих им 
    характеристических функций. Пусть также для любой $x$ — точки непрерывности 
    $F$ выполнено $F_k(x) \underto{k \to \infty} F(x)$. Тогда для любого $t$
    $f_k(t) \underto{k \to \infty} f(t)$.
\end{theorem}
\begin{theorem}[Обратная]\label{reverse_char_dist}
    Пусть существует $\lim\limits_{n \to \infty} f_n(t) = f(t)$, непрерывный в 
    точке $0$. Тогда:
    \begin{enumerate}
        \item $f(t)$ — характеристическая функция.
        \item $F_n(x) \to F(x)$ для всех $x$ — точек непрерывности $x$, где
        $F$ — функция распределения, соответствующая $f$.
    \end{enumerate}
\end{theorem}

Пользуясь этими утверждениями, докажем ранее сформулированные теоремы.

\begin{proof}[Доказательство теоремы \ref{hinchins_lln}]
    Рассмотрим $E(\xi_k - a) = 0$. Тогда:
    $$
    f_{\xi_k - a}(t) = 1 + o(t)
    $$
    Рассмотрим характеристическую функцию:
    $$
    f_{\frac{1}{n}\sum\limits_{k=1}^n (\xi_k - a)}(t) =
    f_{\sum\limits_{k=1}^n (\xi_k - a)}\bigg(\frac{t}{n}\bigg) =
    \bigg(f_{\xi_1 - a}\bigg(\frac{t}{n}\bigg)\bigg)^n =
    \bigg(1 + o\bigg(\frac{t}{n}\bigg)\bigg)^n \underto{n \to \infty} 1
    $$
    Отсюда по теореме \ref{reverse_char_dist}:
    $$
    F_{\frac{1}{n}\sum\limits_{k=1}^n (\xi_k - a)}(x) \to
    \begin{cases}
        0,\quad x < 0 \\
        1,\quad x > 0 \\
    \end{cases}
    $$
    
    Мы показали сходимость по распределению, а нам требуется сходимость по 
    вероятности.
    $$
    \ldots
    $$
\end{proof}

\subsection{Центральная предельная теорема}

\begin{definition}
    Пусть $\xi_1, \xi_2, \ldots$ — последовательность случайных величин,
    $A_n = E\sum\limits_{i=1}^n\xi_i$, $B_n^2 = D\sum\limits_{i=1}^n\xi_n$, причём
    $B_n^2 \underto{n \to \infty} \infty$. Говорят, что для этой последовательности
    выполняется центральная предельная теорема, если для всех $x \in \real$:
    $$
    P\Bigg(\frac{\sum\limits_{i=1}^n \xi_i - A_n}{B_n} < x\Bigg) \underto{n \to \infty}
    \Phi(x) = \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^x e^{\frac{-u^2}{2}}\dif u
    $$
    Или, что то же самое:
    $$
    \frac{\sum\limits_{i=1}^n \xi_i - A_n}{B_n} \overto{\mathrm d} N(0, 1)
    $$
\end{definition}

\begin{theorem}[Центральная предельная теорема Леви]
    Пусть $\xi_1, \xi_2, \ldots$ — последовательность независимых одинаково распределённых 
    случайных величин, у которых существует математическое ожидание $E\xi_k = a$ и 
    дисперсия $D\xi_k = \sigma^2 < \infty$. Тогда для этой последовательности
    выполняется центральная предельная теорема.
\end{theorem}
\begin{proof}
    Нам нужно доказать, что
    $\frac{\sum\limits_{k=1}^n\xi_k - na}{\sqrt{n\sigma^2}} \overto{\mathrm d} N(0,1)$.
    Мы знаем, что это равносильно сходимости характеристических функций:
    $$
    f_{\frac{\sum\limits_{k=1}^n\xi_k - na}{\sqrt{n\sigma^2}}}(t) \underto{n \to \infty}
    e^{-\frac{t^2}{2}}
    $$
    Рассмотрим характеристическую функцию случайной величины $\frac{\xi_k - a}{\sigma}$. 
    Заметим, что $E\frac{\xi_k - a}{\sigma} = 0$, $D\frac{\xi_k - a}{\sigma} = 1 =
    E\Big(\frac{\xi_k - a}{\sigma}\Big)^2$. Тогда характеристическую функцию можно
    разложить в ряд Тейлора:
    $$
    f_{\frac{\xi_k - a}{\sigma}} = 1 + \frac{(it)^2}{2} + o(t^2) = 1 - \frac{t^2}{2} + o(1)
    $$
    Отсюда:
    $$
    f_{\frac{\sum\limits_{k=1}^n\xi_k - na}{\sqrt{n\sigma^2}}}(t) =
    f_{\frac{1}{\sqrt{n}} \sum\limits_{k=1}^n \frac{\xi_k - a}{\sigma}}(t) =
    f_{\sum\limits_{k=1}^n \frac{\xi_k - a}{\sigma}} \bigg(\frac{t}{\sqrt{n}}\bigg) =
    \prod\limits_{k=1}^n f_{\frac{\xi_k - a}{\sigma}}\bigg(\frac{t}{\sqrt{n}}\bigg) =
    \Bigg(f_{\frac{\xi_k - a}{\sigma}}\bigg(\frac{t}{\sqrt{n}}\bigg)\Bigg)^n =
    $$
    $$
    = \bigg(1 - \frac{t^2}{2n} + o\bigg(\frac{t^2}{n}\bigg)\bigg)^n \underto{n \to \infty}
    e^{-\frac{t^2}{2}}
    $$
\end{proof}

Докажем, что интегральная теорема Муарва-Лапласа (\ref{integral_theorem}) является частным
случаем теоремы Леви.

Пусть $\xi_1, \xi_2,\ldots$ — независимые одинаково распределённые случайные величины с
законом распределения:
\begin{center}
    \begin{tabular}{| c | c | c |}
        \hline
        $\xi_k$ & $0$ & $1$ \\ \hline
        $p_k$ & $1-p$ & $p$ \\ \hline
    \end{tabular}
\end{center}
    
$$
P\bigg(\sum\limits_{k=1}^n\xi_k < b) =
P\Bigg(\frac{\sum\limits_{k=1}^n \xi_k - np}{\sqrt{np(1-p)}} < \frac{b - np}{\sqrt{np(1-p)}}
\Bigg) \sim \Phi\bigg(\frac{b-np}{\sqrt{np(1-p)}}\bigg)
$$

\begin{theorem}[Центральная предельная теорема Ляпунова]
    Пусть $\xi_1, \xi_2, \ldots$ — последовательность независимых одинаково распределённых 
    случайных величин, у которых существуют математическое ожидание $E\xi_k = a$, 
    дисперсия $D\xi_k = \sigma^2 < \infty$ и третьи центральные абсолютные моменты
    $c_k^3 = E|\xi_k - a_k|^3 < \infty$. Возьмём $A_n = \sum\limits_{k=1}^n a_k$,
    $B_n^2 = \sum\limits_{k=1}^n \sigma_k^2$, $C_n^3 = \sum\limits_{k=1}^n c_k^3$.
    Пусть также $L_n = \frac{C_n}{B_n} \underto{n \to \infty} 0$. Тогда для
    последовательности выполняется центральная предельная теорема.
\end{theorem}

\subsection{Математическая статистика}

\begin{example}
    Допустим, у нас имеется ящик с $M$ занумерованными шариками. Мы хотим за конечное число
    экспериментов (<<доставаний>>) определить число $M$. Пусть $X_i$ — результат
    $i$-го эксперимента. Мы можем рассматривать $X_1, X_2, \ldots, X_n$ как независимые 
    одинаково распределённые случайные величины с некоторой функцией распределения $F(x)$.
\end{example}

В математической статистике набор $X_1, X_2, \ldots, X_n$ называется
\emph{случайной выборкой}, а функция распределения $F(x)$ —
\emph{генеральной функцией распределения}.

Другой тип задач заключается в нахождении некоторого параметра $\alpha$, от которого
может зависеть функция распределения $F(x, \alpha)$.

Пусть $G(X_1, X_2 \ldots, X_n) = g_n$. Такая величина называется \emph{статистикой}, или 
\emph{оценкой}.

$$
\widetilde{F}_n(t) = \frac{\text{число тех } X_i \text{, которые меньше } t}{n} =
\frac{1}{n}\sum\limits_{i=1}^n\mathbb I_{(-\infty, t)}(X_i)
$$
Здесь $\mathbb I_A(x) = \begin{cases}
    1,\quad x \in A \\
    0,\quad x \notin A \\
\end{cases}$.

\begin{enumerate}
    \item $\frac{1}{n}\sum\limits_{i=1}^n X_i^k$
    \item $\frac{1}{n}\sum\limits_{i=1}^nX_i = \ol X$
    \item $\frac{1}{n}\sum\limits_{i=1}^n(X_i - \ol X)^k$
    \item $\frac{1}{n}\sum\limits_{i=1}^n(X_i - \ol X)^2 = s^2 =
    \frac{1}{n}\sum\limits_{i=1}^n X_i^2 - {\ol X}^2$
    \item $s_0^2 = \frac{1}{n-1}\sum\limits_{i=1}^n(X_i - \ol X)^2$
\end{enumerate}

Рассмотрим $X_{(1)} = \min(X_1, \ldots, X_n)$, $X_{(2)}$ — следующее по величине значение,
и так далее, $X_{(n)} = \max(X_1, \ldots, X_n)$.

$X_{(k)}$ — $k$-я порядковая статистика.

$$
G(k(x) = P\big(X_{(k)} < x\big) = \sum\limits_{i=k}^nC_n^i(F(x))^i(1-F(x))^{n-i}
$$

Определим \emph{медиану} выборки как $m_{\frac{1}{2}} = \begin{cases}
    X_{([\frac{n}{2}] + 1)},\quad\text{если } n \text{ — нечётное} \\
    \frac{1}{2}(X_{(\frac{n}{2})} + X_{(\frac{n}{2} + 1)}),\quad
    \text{если } n \text{ — чётное}
\end{cases}$

Выборочная $\alpha$-квантиль: $\gamma_\alpha = X_{([\alpha n)] + 1}$. Здесь $0 < \alpha < 1$ 
и $\alpha \neq \frac{1}{2}$.

Рассмотрим независимые одинаково распределённые двумерные векторы
$(X_1, Y_1), \ldots, (X_n, Y_n)$ с генеральной функцией распределения $F(x, y)$.
Мы можем рассмотреть две одномерные выборки: $X_1, \ldots, X_n \sim G(x)$ и
$Y_1, \ldots, Y_n \sim H(y)$.
Тогда величина $\ol{\cov}(X, Y) = \frac{1}{n} \sum\limits_{i=1}^n X_iY_i - \ol X \ol Y =
\frac{1}{n}\sum\limits_{i=1}^n(X_i - \ol X)(Y_i - \ol Y)$ называется ковариацией выборки,
а $R(X, Y) = \frac{\ol{\cov}(X, Y)}{\sqrt{s_X^2s_Y^2}}$ — выборочным коэффициентом 
корреляции.

\begin{enumerate}
    \item Если $Eg_n = \alpha$, то $g_n$ называется
    \emph{несмещённой оценкой}. Если $Eg_n \underto{n \to \infty} \alpha$, то $g_n$ 
    называется \emph{асимптотической несмещённой} оценкой.
    \item Если $g_n \overto{P} \alpha$, то $g_n$ — \emph{состоятельная} 
    оценка.
    \item Если
    $\sqrt n \frac{g_n - \alpha}{\Delta} \overto{\mathrm d} N(0, 1)$, то $g_n$ — 
    \emph{асимптотическая нормальная} оценка с дисперсией $\Delta^2$
\end{enumerate}

\begin{theorem}[Гливенко]
    Выборочная (эмпирическая) функция распределения $\ol F(x)$ является
    несмещённой, состоятельной
    и асимптотически нормальной оценкой для генеральной функции распределения $F(x)$.
\end{theorem}
\begin{proof}
    Вспомни, что выборочная функция распределения может быть записана как
    $\ol F(x) = \frac{1}{n}\sum\limits_{i=1}^n \mb I_{(-\infty, x)} (X_i)$.
    Заметим, что $\mb I_{(-\infty, x)} (X_1), \mb I_{(-\infty, x)} (X_2), \ldots,
    \mb I_{(-\infty, x)} (X_n)$ — независимые одинаковы распределённые случайные величины.
    $$
    \mb I_{(-\infty, x)} (X_i) =
    \begin{cases}
        0,\quad X_i \geqslant x \\
        1,\quad X_i < x
    \end{cases}
    $$
    $$
    P\Big(\mb I_{(-\infty, x)} (X_i) = 1\Big) = P(X_i < x) = F(x)
    $$
    $$
    E\ol F(x) = E\frac{1}{n}\sum\limits_{i=1}^n\mb I_{(-\infty, x)} (X_i) =
    \frac{1}{n}\sum\limits_{i=1}^nE\mb I_{(-\infty, x)} (X_i) = F(x)
    $$
    Это означает несмещённость. По закону больших чисел Хинчина:
    $$
    \frac{1}{n}\sum\limits_{i=1}^n\mb I_{(-\infty, x)} (X_i) \overto{\mathrm P} F(x)
    $$
    Это означает состоятельность. По центральной предельной теореме Леви:
    $$
    \frac{\sum\limits_{i=1}^n \mb I_{(-\infty, x)} (X_i) - nF(x)}{\sqrt{(1 - F(x))F(x)n}}
    \overto{\mathrm d} N(0, 1)
    $$
    $$
    \frac{\sqrt n (\ol F(x) - F(x))}{\sqrt{F(x)(1 - F(x)}} \overto{\mathrm d} N(0, 1)
    $$
    Это означает асимптотическую нормальность.
\end{proof}
    
Пусть $a = EX_i$ для некоторой выборки $X_1, \ldots, X_n$ с генеральной функцией 
распределения $F(x)$. Мы хотим оценить $a$.

\begin{theorem}[Свойства выборочного среднего]
\mbox{}
    \begin{enumerate}
        \item Выборочное среднее $\ol X$ — несмещённая оценка для матожидания $a$.
        \item $\ol X$ — состоятельная оценка для $a$.
        \item Если существует второй момент $EX_i^2$, то $\ol X$ — асимптотическая оценка для
        $a$.
    \end{enumerate}
\end{theorem}
\begin{proof}
\mbox{}
    \begin{enumerate}
        \item $E\ol X = E\frac{1}{n}\sum\limits_{i=1}^n X_i =
        \frac{1}{n} \sum\limits_{i=1}^n EX_i = a$
        \item По закону больших чисел Хинчина
        $\frac{1}{n}\sum\limits_{i=1}^n X_i \overto{\mathrm P} a$
        \item По центральной предельной теореме Леви
        $\frac{\sum\limits_{i=1}^n X_i - na}{\sqrt{n (EX_i^2 - (EX_i)^2}}
        \overto{\mathrm d} N(0, 1)$
    \end{enumerate}
\end{proof}

Попробуем оценить дисперсию $\sigma^2 = DX_i$.
Вспомним, что $s^2 = \frac{1}{n} \sum\limits_{i=1}^n (X_i - \ol X)^2 =
\frac{1}{n}\sum\limits_{i=1}^n X_i^2 - \ol X^2$

\begin{theorem}[Свойства оценки $s^2$]
\mbox{}
    \begin{enumerate}
        \item $s^2$ — инвариантная относительно сдвига выборки оценка.
        \item $s^2$ — асимптотическая несмещённая оценка для $\sigma^2$ (которая смещённая).
        \item $s^2$ — состоятельная оценка для $\sigma^2$.
        \item Если существует $EX_i^4$, то $s^2$ — асимптотическая нормальная оценка с 
        дисперсией $EX_i^4 - (EX_i^2)^2$ для $\sigma^2$.
    \end{enumerate}
\end{theorem}
\begin{proof}
\mbox{}
    \begin{enumerate}
        \item Рассмотрим сдвиг $X_1 + c, \ldots, X_n + c$.
        $$
        s_c^2 = \frac{1}{n}\sum\limits_{i=1}^n(X_i + c - (\ol X + c))^2 =
        \frac{1}{n}\sum\limits_{i=1}^n(X_i - \ol X)^2 = s^2
        $$
        Поэтому далее разумно предположить, что $EX_i = 0$
        \item
        $$
        Es^2 = E\frac{1}{n}\sum\limits_{i=1}^n X_i^2 - E(\ol X)^2 =
        \frac{1}{n}\sum\limits_{i=1}^n EX_i^2 =
        E\bigg(\frac{1}{n}\sum\limits_{i=1}^n X_i\bigg)^2 =
        $$
        $$
        \sigma^2 - \frac{1}{n^2} \bigg(\underbrace{E\sum\limits_{i \neq j}X_iX_j}_
        {= \sum\limits_{i \neq j} EX_iEX_j = 0} +
        E\sum\limits_{i=1}^n X_i^2\bigg) = \sigma^2 - \frac{1}{n}\sigma^2 =
        \frac{n - 1}{n}\sigma ^2 \underto{n \to \infty} \sigma^2
        $$
        \item Возьмём $\varepsilon > 0$.
        $$
        P(|s^2 - \sigma^2| \geqslant \varepsilon) =
        P\bigg(\bigg|\frac{1}{n}\sum\limits_{i=1}^n X_i^2 - \ol X^2 - \sigma^2\bigg|
        \geqslant \varepsilon \bigg) \leqslant
        P\bigg(\bigg|\frac{1}{n}\sum\limits_{i=1}^n X_i^2 - \sigma^2\bigg|
        \geqslant \frac{\varepsilon}{2} \bigg) +
        P\bigg(\ol x^2 \geqslant \frac{\varepsilon}{2}\bigg)
        $$
        По закону больших чисел Хинчина первое слагаемое в правой части неравенства стремится
        к нулю при $n \to \infty$.
        $$
        P\bigg(\ol X^2 \geqslant \frac{\varepsilon}{2}\bigg) \leqslant
        \frac{E \ol X^2}{\frac{\varepsilon}{2}} =
        \frac{2E\bigg(\frac{1}{n}\sum\limits_{i=1}^n X_i\bigg)^2}{\varepsilon} =
        \frac{2n\sigma^2}{\varepsilon n^2} \underto{n \to \infty} 0
        $$
        Состоятельность доказана.
        \item
        $$
        \sqrt{n}(s^2 - \sigma^2) =
        \sqrt{n}\bigg(\frac{1}{n} \sum\limits_{i=1}^n X_i^2 - \ol X^2 - \sigma^2\bigg) =
        \frac{\sum\limits_{i=1}^n X_i^2 - n\sigma^2}{\sqrt n} - \sqrt{n}\ol X^2
        $$
        Первое слагаемое по центральной предельной теореме Леви стремится к нормальному
        закону, а второе — к нулю.
        $$
        P(\sqrt n \ol X^2 > \varepsilon) \leqslant
        \frac{n^{\frac{3}{2}}\sigma^2}{n^2\varepsilon} \underto{n \to \infty} 0
        $$
    \end{enumerate}
\end{proof}
\begin{lemma}
    Пусть $\Delta_n = \xi_n + \eta_n$, $\xi_n \overto{\mathrm d} \xi$,
    $\eta_n \overto{\mathrm P} 0$. Тогда $\Delta_n \overto{\mathrm d} \xi$.
\end{lemma}
\begin{proof}
    $$
    |f_{\Delta_n}(t) - f_\xi(t)| = |Ee^{it(\xi_n + \eta_n)} - Ee^{it\xi} \pm Ee^{it\xi^n}|
    \leqslant |Ee^{it(\xi_n + \eta_n)} - Ee^{it\xi_n}| +
    \underbrace{|Ee^{it\xi_n} - Ee^{it\xi}|}_{\to 0}
    $$
    $$
    |Ee^{it(\xi_n + \eta_n)} - Ee^{it\xi_n}| = |Ee^{it\xi_n}(e^{it\eta_n} - 1)| \leqslant
    E|e^{it\eta_n} \ldots
    $$
\end{proof}

Предположим, что у нас есть выборка $X_1, \ldots, X_n$ с генеральной функцией распределения
$F(x, \theta)$, где $\theta$ — какой-то параметр, который мы хотим оценить.

Рассмотрим понятие \emph{доверительного интервала}.
\begin{definition}
    Пусть $g_n$, $h_n$ — статистики, причём $P(g_n < h_n) = 1$. Пусть $0 < \alpha \ll 1$.
    Если $P(g_n < \theta < h_n) \geqslant 1 - \alpha$, то $(g_n, h_n)$ называется
    доверительным интервалом для параметра $\theta$ с доверительной вероятностью $\theta$.
\end{definition}

Геометрический смысл доверительного интервала заключается в том, что мы покрываем с заданной
вероятностью $\alpha$ точку $\theta$ этим интервалом.

Пусть $\xi_0, \xi_1, \ldots, \xi_n$ — независимые одинаково распределённые случайные 
величины, причём $\xi_i \in N(0, 1)$ для всех $i$.
Пусть $\chi_n^2 = \sum\limits_{i=1}^n \xi_i^2$. Такую случайную величину называют 
\emph{распределённой по закону хи-квадрат с $n$ степенями свободы}.

Рассмотрим $T_n = \frac{\xi_0}{\sqrt{\frac{1}{n}\sum\limits_{i=1}^n \xi_i^2}}$. Говорят, что
эта случайная величина распределена по закону Стьюдента с $n$ степенями свободы. При $n=1$
мы получим распределение Коши, а при $n \to \infty$ $T_n$ будет стремиться к стандартному
нормальному закону.

\begin{lemma}[Фишера]
    Пусть $X_1, \ldots, X_n \sim N(a, \sigma^2)$ — случайная выборка. Тогда:
    \begin{enumerate}
        \item $\sqrt{n}\frac{\ol X - a}{\sigma} \in N(0, 1)$
        \item $\ol X$, $s^2$ — независимые случайные величины.
        \item $\frac{ns^2}{\sigma^2}$ распределена по закону хи-квадрат с $n-1$ степенями 
        свободы.
        \item $\sqrt{n - 1}\frac{\ol X - a}{\sigma}$ имеет распределение Стьюдента с
        $n - 1$ степенями свободы
    \end{enumerate}
\end{lemma}

\begin{examples}
\mbox{}
    \begin{enumerate}
        \item Построение доверительного интервала для $a$, когда $\sigma^2$ известно.
        Воспользуемся первым утверждением вышеприведённой леммы. Возьмём число
        $\alpha$: $0 < \alpha \ll 1$. С помощью таблиц стандартного нормального закона
        возьмём $z_{1 - \frac{\alpha}{2}}$ такое, что функция распределения
        $\Phi(z_{1 - \frac{\alpha}{2}} = 1 - \frac{\alpha}{2}$.
        $$
        P\bigg(-z_{1 - \frac{\alpha}{2}} < \sqrt{n}\frac{\ol X - a}{\sigma} <
        z_{1 - \frac{\alpha}{2}}\bigg) = 1 - \alpha
        $$
        $$
        P\bigg(\ol X - \frac{\sigma z_{1 - \frac{\alpha}{2}}}{\sqrt n} < a <
        \ol X + \frac{\sigma z_{1 - \frac{\alpha}{2}}}{\sqrt n}\bigg) = 1 - \alpha
        $$
        То есть, доверительный интервал равен
        $\Big(\ol X - \frac{\sigma z_{1 - \frac{\alpha}{2}}}{\sqrt n},
        \ol X + \frac{\sigma z_{1 - \frac{\alpha}{2}}}{\sqrt n}\Big)$.
        \item Построение доверительного интервала для $a$, когда $\sigma^2$ неизвестно.
        С помощью таблиц распределения Стьюдента с $n-1$ степенями свободы найдём
        $z_{1 - \frac{\alpha}{2}}$ такое, что функция распределения
        $S_{n-1}(z_{1 - \frac{\alpha}{2}}) = 1 - \frac{\alpha}{2}$.
        $$
        P\bigg(-z_{1 - \frac{\alpha}{2}} < \sqrt{n-1}\frac{\ol X - a}{s} <
        z_{1 - \frac{\alpha}{2}}\bigg) = 1 - \alpha
        $$
        $$
        P\bigg(\ol X - \frac{s z_{1 - \frac{\alpha}{2}}}{\sqrt{n-1}} < a <
        \ol X + \frac{s z_{1 - \frac{\alpha}{2}}}{\sqrt{n-1}}\bigg) = 1 - \alpha
        $$
        Доверительный интервал: $\Big(\ol X - \frac{s z_{1 - \frac{\alpha}{2}}}{\sqrt{n-1}},
        \ol X + \frac{s z_{1 - \frac{\alpha}{2}}}{\sqrt{n-1}}\Big)$. Заметим, что
        длина этого интервала $\frac{2sz_{1 - \frac{\alpha}{2}}}{\sqrt{n-1}}
        \overto{\mathrm P} 0$ при $n \to \infty$.
    \end{enumerate}
\end{examples}

\subsection{Методы построения оценок для неизвестных параметров}

Мы рассматриваем статистики $X_1, \ldots, X_n$ с генеральной функцией распределения $F(x, \theta)$,
причём $\theta$ может быть вектором. Мы хотим оценить $\theta$.

\paragraph{Метод моментов}
    Обозначим через $a_i = \frac{1}{n}\sum\limits_{k=1}^n x_k^i$
    $i$-й выборочный момент, через $\alpha_i(\theta) = E_\theta X_1^i$.
    Составим систему и решим её:
    $$
    \begin{cases}
        \alpha_1(\theta) = a_1 \\
        \ldots \\
        \alpha_k(\theta) = a_k \\
    \end{cases}
    $$

\begin{examples}
\mbox{}
    \begin{enumerate}
        \item Пусть $X_1, \ldots, X_n \sim N(a, \sigma^2)$. Имеем систему:
        $$
        \begin{cases}
            a = \ol X \\
            \sigma^2 + a^2 = \frac{1}{n}\sum\limits_{i=1}^n X_i^2 \\
        \end{cases}
        $$
        Решив систему, поучим оценку $(\ol X, s^2)$.
        \item Пусть $X_1, \ldots, X_n \sim U_{[0, a]}$. $\frac{a}{2} = \ol X$, отсюда $a = 2\ol X$.
    \end{enumerate}
\end{examples}

\paragraph{Метод максимального правдоподобия}
        В случае абсолютно непрерывного распределения и $X_1, \ldots, X_n \sim F(x, \theta)$,
        $p(x, \theta)$ — плотности, будем рассматривать \emph{функцию правдоподобия}:
        $$
        L(X_1, \ldots, X_n, \theta) = \prod\limits_{i=1}^n p(X_i, \theta)
        $$
        Возьмём $\widetilde{\theta}:$ $L(X_1, \ldots, X_n, \widetilde\theta) \geqslant
        L(X_1, \ldots, X_n, \theta)\quad \forall \theta$. $\widetilde\theta$ будет оценкой для $\theta$,
        полученной методом максимального правдопободия.
        $$
        \frac{\dif \ln L(X_1, \ldots, X_n, \theta)}{\dif \theta} = 0
        $$
        
        В случае дискретного распределения функция правдоподобия:
        $$
        L(X_1, \ldots, X_n, \theta) = \prod\limits_{i=1}^n P_\theta(X = X_i)
        $$
        И будем брать $\widetilde\theta$ точкой максимума $L(X_1, \ldots, X_n, \theta)$.

\begin{example}
        Предположим $X_i \sim U_{[0, a]}$ и оценим $a$.
        $$
        p(x, a) = \frac{1}{a}\mb I_{[0, a]}(x)
        $$
        $$
        L(X_1, \ldots, X_n, a) = \frac{1}{a^n}\prod\limits_{i=1}^n\mb I_{[0, a]}(X_i)
        $$
        Построим вариционный ряд, то есть упорядочим $X_i$: $X_{(1)}, \ldots, X_{(n)}$.
        Тогда:
        $$
        L(X_1, \ldots, X_n, a) = \frac{1}{a^n}\prod\limits_{i=1}^n\mb I_{[0, a]}(X_{(i)})
        $$
        Таким образом, $L=0$ при $a < X_{(n)}$, в ней $L=X_{(n)}$, после
        чего c увеличением $a$ $L$ убывает, отсюда $\widetilde a = X_{(n)}$.
        
        Заметим, что полученная оценка отличается от той, что мы получили методом моментов.
\end{example}

Пусть $\widetilde g_1, \widetilde g_2$ — оценки параметра $\theta$. Пусть мы получили:
$$
E(\widetilde g_1 - \theta)^2 \geqslant E(\widetilde g_2 - \theta)^2\quad \forall \theta
$$
Это означает, что оценка $\widetilde g_2$ более эффективна.

\begin{exercise}
    Предположим $X_1, \ldots, X_n \sim p(x, \alpha) = \frac{1}{2} e^{-|x-\alpha|}$.
    Оценить $\alpha$ методами моментов и максимального правдободобия. Отметим, что сложность
    последнего в данном случае заключается в том, что приравнивание производной к нулю не работает,
    так как функция не дифференцируема в этой точке.
\end{exercise}

\subsection{Проверка статистических гипотез}

Как обычно, рассматриваем случайную выборку $X_1, \ldots, X_n$ с генеральной функцией распределения
$F$. Пусть $\mathfrak F$ — некоторое множество функций распределения, представимое в виде 
объединения двух непересекающихся подмножеств $\mathfrak F_0, \mathfrak F_1:$
$\mathfrak F_0 \cap \mathfrak F_1 = \mathfrak F$, $\mathfrak F_0 \cap \mathfrak F_1 = \varnothing$
Может возникнуть две гипотезы: $H_0 = \{F \in \mathfrak F_0\}$ или $H_1 = \{F \in \mathfrak F_1\}$.
Например, $H_0$ (<<нулевая гипотеза>>) могут быть такого вида:
\begin{itemize}
    \item $F \in N$;
    \item $F \in N(0, 4)$;
    \item $F$ таково, что математическое ожидание положительно.
\end{itemize}

Методология проверки гипотез такова: задаётся некоторое близкое к нулю положительное $\alpha$. На
практике это может быть $\alpha = 0.1, \alpha = 0.05, \alpha = 0.01, \ldots$

\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        & Верна гипотеза $H_0$ & Верна гипотеза $H_1$ \\ \hline
        Принята гипотеза $H_0$ & Верное решение & Ошибка II рода \\ \hline
        Принята гипотеза $H_1$ & Ошибка I рода & Верное решение \\ \hline
    \end{tabular}
\end{center}

В качестве основной (<<нулевой>>) гипотезы обычно берут ту, для которой вероятность ошибки
первого рода не превосходит $\alpha$, но при этом вероятность ошибки второго рода минимизируется.

Рассмотрим основные типы задач на проверку гипотез.
\begin{enumerate}
    \item Проверка гипотезы согласия. $X_1, \ldots, X_n \sim F$ и $F_0$ — некоторая известная 
    функция. Берётся $H_0 = \{F = F_0\}$, $H_1 = \{F \neq F_0\}$
    \item Проверка однородности. $X_1, \ldots, X_n \sim F$, $Y_1, \ldots, Y_n \sim G$. Берётся
    $H_0 = \{F = G\}$, $H_1 = \{F \neq G\}$.
    \item Проверка независимости. $(X_1, Y_1), \ldots, (X_n, Y_n) \sim F(x, y)$, где
    $X_1, \ldots, X_n \sim G(x)$, $Y_1, \ldots, Y_n \sim H(y)$. Берётся
    $H_0 = \{F(x, y) = G(x)H(y)\}$, $H_1 = \{\exists x_0, y_0:\, F(x_0, y_0) \neq G(x_0)H(y_0)\}$.
\end{enumerate}

Изучим некоторые инструменты, помогающие проверить гипотезу согласия.

\paragraph{Критерий Колмогорова}
$$
\ol F_n(t) = \frac{1}{n}\sum\limits_{i=1}^n \mb I_{(-\infty, t)}(X_i)
$$
$$
D_n = \sup\limits_{t \in \real} \Big| \ol F_n(t) - F_0(t) \Big|
$$

\begin{theorem}[Колмогорова]
    Если $H_0$ верна, то $P(\sqrt n D_n < x) \underto{n \to \infty} K(x)$. $K(x)$ — функция
    распределения Колмогорова.
\end{theorem}

Алгоритм:
\begin{enumerate}
    \item Вычисляем $D_n$.
    \item По таблицам $K(x)$ находим $z_{1-\alpha}$ такое, что $K(z_{1-\alpha}) = 1 - \alpha$.
    \item Сравниваем: если $\sqrt n D_n \geqslant z_{1-\alpha}$, то $H_0$ отвергаем; если
    $\sqrt n D_n < z_{1-\alpha}$, то принимаем.
\end{enumerate}

\paragraph{Критерий хи-квадрат для проверки гипотезы согласия}
Разбиваем вещественную прямую на $r$ зон $A_1, \ldots, A_r$ ($5 \leqslant r \leqslant 15$).
Считаем вероятности $p_k = P_{H_0}(X \in A_k)$.
Считаем $\nu_k = \sum\limits_{i=1}^n \mb I_{A_k} (X_i)$ — число элементов
выборки, попавших в зону $k$.
Построим статистику $\chi_n^2 = \sum\limits_{i=1}^r \frac{(\nu_i - np_i)^2}{np_i}$. Справедлива
следующая теорема.
\begin{theorem}[Пирсона]
    Если $H_0$ верна, то $P(\chi_n^2 < x) \underto{n \to \infty} K_{r-1}(x)$. $K_{r-1}(x)$ —
    функция закона хи-квадрат с $r-1$ степенью свободы.
\end{theorem}

Алгоритм:
\begin{enumerate}
    \item Вычисляем $\chi_n^2$.
    \item По таблицам $K_{r-1}(x)$ находим $z_{1-\alpha}$ такое,
    что $K(z_{1-\alpha}) = 1 - \alpha$.
    \item Сравниваем: если $\chi_n^2 \geqslant z_{1-\alpha}$, то $H_0$ отвергаем; если
    $\chi_n^2 < z_{1-\alpha}$, то принимаем.
\end{enumerate}

\begin{remark}
    В случае, когда несколько критериев дают принятие гипотезы согласия, но один не даёт,
    мы эту гипотезу отвергаем.
\end{remark}

\paragraph{Критерий хи-квадрат для проверки независимости}

\begin{theorem}
    $\ldots$
\end{theorem}

Пусть имеется случайная выборка $X_1, \ldots, X_n$ и вариационный ряд $X_{(1)}, \ldots, X_{(n)}$.
Пусть $R_i$ — место, которое занимает величина $X_i$ в вариационном ряду, называемое \emph{рангом}.
Итак по имеющейся выборке мы можем построить вариационный ряд и вектор рангов $(R_1, \ldots, R_n)$.
Очевидно, что мы также можем восстановить выборку по её вариационному ряду и вектору рангов.
Оказывается, для решения многих задач достаточно знать только вектор рангов.

\paragraph{Критерий Вилкоксона для проверки однородности}

$X_1, \ldots, X_n \sim F$, $Y_1, \ldots, Y_n \sim G$. Будем проверять гипотезы
$H_0 = \{F = G\}$, $H_1 = \{F \neq G\}$. Объединим выборки $X_i$ и $Y_j$ и построим ранги $R_i$
только для $X_i$. Возьмём $w_{n, m} = \sum\limits_{i=1}^n R_i$.

\begin{theorem}
    Если $H_0$ верна, то:
    \begin{enumerate}
        \item $Ew_{n,m} = \frac{n(n+m+1)}{2}$
        \item $Dw_{n,m} = \frac{n(n+m+1)}{12}$
        \item $P\bigg(\frac{w_{n,m} - Ew_{n,m}}{\sqrt{Dw_{n,m}}} < x\bigg)
        \underto{n \to \infty,\, m \to \infty} \Phi(x)$
    \end{enumerate}
\end{theorem}

Алгоритм:
\begin{enumerate}
    \item По $\alpha$ находим $z_{1-\frac{z}{2}}$ такое, что $\Phi(z_{1-\frac{z}{2}}) 
    \frac{\alpha}{2}$ $\ldots$ %пропущено%
\end{enumerate}

\paragraph{Критерий Спирмена для проверки независимости}
Пусть есть выборка $(X_1, Y_1), \ldots, (X_n, Y_n)$, причём $R_i$ — ранг $X_i$, $S_i$ — ранг
$Y_i$.
$$
d_n^2 = \sum\limits_{i=1}^n(R_i - S_i)^2
$$
$$
r_n = 1 - \frac{6d_n^2}{n(n-1)(n+1)}
$$
$r_n$ называется \emph{коэффициентом корреляции Спирмена}. Он действительно является коэффициентом
корреляции.

\begin{theorem}
    Если $H_0$ верна, то $P(\sqrt{n-1} r_n < x) \underto{n \to \infty} \Phi(x)$;
\end{theorem}

Если $|\sqrt{n-1}r_n| \geqslant z_{1-\frac{\alpha}{2}}$, то отвергаем $H_0$.

\end{document}
